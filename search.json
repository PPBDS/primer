[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Preceptor’s Primer for Bayesian Data Science: Using the Cardinal Virtues for Inference",
    "section": "",
    "text": "Welcome\n\n\nThe Primer is being re-written. Consider it all to be a draft, at least for now. There are three major changes in progress. First, I am switching from brms to tidymodels and marginaleffects. (If you disagree with this change, please let me know!) Second, I am standardizing the usage of the Cardinal Virtues. Third, I am using AI tools like ChatGPT and Claude to help with both writing and coding.\n\n\n\n\n\n\n\n\n \nThis isn’t the book you’re looking for.\n\nFirst, the book is for students in my classes. Everything about the book is designed to make the experience of those students better. I hope that some of the material here may be useful to people outside of this class.\nSecond, the book changes all the time. It is as up-to-date as possible.\nThird, I am highly opinionated about what matters and what does not. You might not share my views.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "preamble.html",
    "href": "preamble.html",
    "title": "Preamble",
    "section": "",
    "text": "Dedication\nAnd what is romantic, Kay —\nAnd what is love?\nNeed we ask anyone to tell us these things?",
    "crumbs": [
      "Preamble"
    ]
  },
  {
    "objectID": "preamble.html#acknowledgements",
    "href": "preamble.html#acknowledgements",
    "title": "Preamble",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis work builds on and extends the contributions of many people in the R and Open Source communities. In particular, I would like to acknowledge extensive material taken from @isrs2014, @rds2016, @dapa2019, @mdrt2019, @bryan2019, @isrs2014, @bsms2012, @rds2016, @tmwr2020, @timbers, and @glmlm2019.\nAlboukadel Kassambara, Andrew Tran, Thomas Mock and others kindly allowed for the re-use and/or modification of their work.\n\n\n\n\n\n\n\n\nThanks to contributions from students and colleagues at Harvard and elsewhere, as well as from random people I met on the internet: Albert Rivero, Nicholas Dow, Celine Vendler, Sophia Zheng, Maria Burzillo, Robert McKenzie, Deborah Gonzalez, Beau Meche, Evelyn Cai, Miro Bergam, Jessica Edwards, Emma Freeman, Cassidy Bargell, Yao Yu, Vivian Zhang, Ishan Bhatt, Mak Famulari, Tahmid Ahmed, Eliot Min, Hannah Valencia, Asmer Safi, Erin Guetzloe, Shea Jenkins, Thomas Weiss, Diego Martinez, Andy Wang, Tyler Simko, Jake Berg, Connor Rust, Liam Rust, Alla Baranovsky, Carine Hajjar, Diego Arias, and Stephanie Yao.\n\n\n\n\n\n\n\n\nAlso, Becca Gill, Ajay Malik, Heather Li, Nosa Lawani, Stephanie Saab, Nuo Wen Lei, Anmay Gupta and Dario Anaya.\n\n\n\n\n\n\n\n\nAlso, Kevin Xu, Anmay Gupta, Sophia Zhu, Arghayan Jeiyasarangkan, Yuhan Wu, Ryan Southward, George Pentchev, Ahmet Atilla Colak, Mahima Malhotra, and Shreeram Patkar.\n\n\n\n\n\n\n\n\nAlso, Tejas Mundhe, Jackson Roe, Varun Dommeti, Soham Gunturu and Felix Cai.\n\n\n\n\n\n\n\n\nAlso Melissa Ban, Srihith Garlapati, Miriam Heiss, Matthew Ru, Mann Talati, Alex Kuai, Anish Bellamkonda, Krish Saluja, Aryan Kancherla, Zayan Farooq, Rajarshi Mandal, Pranav Chivukula, and Pratham Kancherla.\n\n\n\n\n\n\n\n\nAlso Tongyan Ban, Anish Bellamkonda, Tejas Mundhe, Vaangmaya Rebba, Luit Deka, Aadhira Satheesh, Ethan Xiao, Nosa Lawani, Alan Cai, Adi Chaudhary, Atharva Bir Dutta, Monish Malla, Navya Joshi, Mihir Kaushal, Sanaka Dash, Gia Khang, and Tanay Janmanchi.\n\n\n\n\n\n\n\n\nAlso Navya Joshi, Aadhira Satheesh, Luit Deka, Aashna Patel, Jamie Dongwan Seoh, Ivy Spratt, Anish Talla, Amogh Patil, Sophia Yao, Thomas Seoh, Lowell Ethan, Sruthi Gandhi, Aaditya Gupta, Gia Khang, Luke Li, and John Stepher Sabio.\nI would like to gratefully acknowledge funding from The Derek Bok Center for Teaching and Learning at Harvard University, via its Digital Teaching Fellows and Learning Lab Undergraduate Fellows programs.\n\n\n\n\n\n\n\n\nDavid Kane",
    "crumbs": [
      "Preamble"
    ]
  },
  {
    "objectID": "preamble.html#license",
    "href": "preamble.html#license",
    "title": "Preamble",
    "section": "License",
    "text": "License\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.",
    "crumbs": [
      "Preamble"
    ]
  },
  {
    "objectID": "getting-started.html",
    "href": "getting-started.html",
    "title": "Getting Started",
    "section": "",
    "text": "Installing R and Positron\nYou can never look at the data too much. – Mark Engerman\nThe world confronts us. Make decisions we must.\nWe use R via Positron. R is to Positron as a car’s engine is to its dashboard.\nMore precisely, R is a programming languages that runs computations, while Positron is an integrated development environment (IDE) that provides an interface with many convenient features. Just as having access to a speedometer and navigation system makes driving much easier, using Positron’s interface makes working with R and Python much easier.\nDownload and install R and Positron on your computer.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "getting-started.html#summary",
    "href": "getting-started.html#summary",
    "title": "Getting Started",
    "section": "Summary",
    "text": "Summary\nYou should have done the following:\n\nInstalled the latest versions of R and Positron.\nIf using Windows, installed Git for Windows and RTools.\nInstalled, from CRAN, these packages:\n\n\n# Only run this options() line if NOT on Cloud platform\n# options(\"pkgType\" = \"binary\"). \ninstall.packages(\"pak\")\npak::pak(c(\"tidyverse\", \"tutorial.helpers\"))\n\n\nInstalled, from Github, the positron.tutorials and primer.tutorials packages:\n\n\npak::pak(c(\"PPBDS/positron.tutorials\", \"PPBDS/primer.tutorials\"))\n\n\nRun tutorial.helpers::set_positron_settings() at the Console in order to make some sensible changes to your Positron and Rprofile settings.\n\n\ntutorial.helpers::set_positron_settings()\n\n\nCompleted the “Getting Started” tutorial from the tutorial.helpers package:\n\n\nlearnr::run_tutorial(\"getting-started\", package = \"tutorial.helpers\")\n\nLet’s get started!",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "getting-started.html#python-appendix",
    "href": "getting-started.html#python-appendix",
    "title": "Getting Started",
    "section": "Python Appendix",
    "text": "Python Appendix\nYou also have the option of downloading and installing Python. Python is a flexible and high level object oriented programming language.\nInstalling Python\nThere are many ways to install Python but we will install it through uv, an extremely fast Python package and project manager written in Rust.\nThis is the installation section of the documentation of uv. Access the link.\nThere are different installation instructions for Mac/Linux and Windows, so choose the correct tab.\nMac/Linux\nThere are 2 different installation prompts specified in the documentation, we will go with the first one. All Macs have curl pre-installed. In Terminal, run:\n\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\nWindows\n\n\nThe installation prompt for Windows uses PowerShell, but it works on Git Bash regardless, so do not worry. In Terminal, run:\n\npowershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n\nuv Python\nAfter uv has been successfully installed on your device, run this command in the Terminal.\n\nuv python install\n\nStarting up the Python Console\nMac\nPython has been installed through uv, and all you need to do is go to the search bar and search &gt;Python: Select Interpreter. Make sure to include the greater than sign &gt; in your search.\n\n\n\n\n\n\n\n\nPython 3.14 or whatever version of Python you have installed will show up as an option, click it and your brand new Python console will be started. Like so:\n\n\n\n\n\n\n\n\nWindows\n\nIn the search bar type “&gt;Python: Select Interpreter”\n\n\n\n\n\n\n\n\nThis is what you will see\n\n\n\n\n\n\n\n\nClick on the “Enter interpreter path” option and press “Find…”. This will bring you to a version of your File Explorer. Almost always the path that your Python will be installed in is OS(C:) and Users/YOUR NAME/.local/bin. You will be able to find a version of Python in that path like so:\n\n\n\n\n\n\n\n\nSo when it asks you to find, go to ‘Users’ then click on your name, find and go into ‘.local’ and then ‘bin’ and within bin click whatever version of Python that was installed.\nAfter following these steps, a console of Python will automatically open. It may say unsupported, but this is just because Positron may not yet have been updated to support the newest version of Python. Like this:",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "getting-started.html#ai-appendix",
    "href": "getting-started.html#ai-appendix",
    "title": "Getting Started",
    "section": "AI Appendix",
    "text": "AI Appendix\nGenerative AI is the future of data science, and everything else. Unfortunately, it is rapidly changing, so these instructions may be out of date. Highlights:\nFirst, we encourage you to experiment with a wide variety of AIs, especially the free versions, generally via their default chat interfaces. Ask the AI for code which solves your problem. Copy provided code over to Positron. Try it. If the initial code does not do what you want, tell the AI what happened — often by copy/pasting from Positron back to the AI, especially if there was an error — and then ask for another try.\nSecond, see here for the latest information on using Positron Assistant, our recommended approach.\nThird, to use Positron Assistant, you need an API account from Athropic.\n\n\n\n\n\n\n\n\nGo here and follow the instructions. We recommend buying $5 worth of credits. This is plenty to get started, and also prevents you from inadvertantly spending too much money. Once you are more experienced, you can adjust your account settings.\n\n\n\n\n\n\n\n\nYou want to use the API. Click the button to create an API key.\n\n\n\n\n\n\n\n\nNote that the interface for Claude, and other generative AIs, is always changing. So, these images may not match what you see. The goal is to get an API key.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOnce you get the key, copy it somewhere temporarily, in a draft email is fine. We won’t be making use of advanced Claude tools like workspaces.\nNote that these instructions largely mimic the instructions which Positron provides.\nThe last step is to click on Positron Assistant button and provide the API key. You need to click “Sign In” after providing your API key. Be careful in pasting in your API key since something as simple as an extra space at the end will cause an error.\n\n\n\n\n\n\n\n\nYou should now have access to Positron Assistant!",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "01-probability.html",
    "href": "01-probability.html",
    "title": "1  Probability",
    "section": "",
    "text": "1.1 Distributions\nThe central tension, and opportunity, in data science is the interplay between the data and the science, between our empirical observations and the models which we create to use them. Probability is the language we use to explore that interplay; it connects data to models, and models to inference.\nWhat does it mean that Donald Trump had a 30% chance of winning election in the fall of 2016? That there is a 90% probability of rain tomorrow? That the dice at the casino are fair?\nProbability quantifies uncertainty. Think of probability as a proportion. The probability of an event occurring is a number from 0 to 1, where 0 means that the event is impossible and 1 means that the event is 100% certain.\nBegin with the simplest events: coin flips and dice rolls. The set of all outcomes is the sample space. With fair coins and dice, we know that:\nIf the probability of an outcome is unknown, we will often refer to it as an unknown parameter, something which we might use data to estimate. We usually use Greek letters to refer to parameters. Whenever we are talking about a specific probability (represented by a single value), we will use \\(\\rho\\) (the Greek letter “rho” but spoken aloud as “p” by us) with a subscript which specifies the exact outcome of which it is the probability. For instance, \\(\\rho_h = 0.5\\) denotes the probability of getting heads on a coin toss when the coin is fair. \\(\\rho_t\\) — spoken as “PT” or “P sub T” or “P tails” — denotes the probability of getting tails on a coin toss. This notation can become annoying if the outcome whose probability we seek is less concise. For example, we might write the probability of rolling a 1, 2 or 3 using a fair six-sided dice as:\n\\[\n\\rho_{dice\\ roll\\ is\\ 1,\\ 2\\ or\\ 3} = 0.5\n\\]\nWe will rarely write out the full definition of an event along with the \\(\\rho\\) symbol. The syntax is just too ugly. Instead, we will define an event a as the case when one rolled dice equals 1, 2 or 3 and, then, write\n\\[\\rho_a = 0.5\\]\nA random variable is a function which produces a value from a sample set. A random variable can be either discrete — where the sample set has a limited number of members, like H or T for the result of a coin flip, or 2, 3, …, 12 for the sum of two dice — or continuous (any value within a range). Probability is a claim about the value of a random variable, i.e., that you have a 50% probability of getting a 1, 2 or 3 when you roll a fair dice.\nWe usually use capital letters for random variables. So, \\(C\\) might be our symbol for the random variable which is a coin toss and \\(D\\) might be our symbol for the random variable which is the sum of two dice. When discussing random variables in general, or when we grow tired of coming up with new symbols, we will use \\(Y\\).\nSmall letters refer to a single outcome or result from a random variable. \\(c\\) is the outcome from one coin toss. \\(d\\) is the result from one throw of the two dice. The value of the outcome must come from the sample space. So, \\(c\\) can only take on two possible values: heads or tails. When discussing random variables in general, we use \\(y\\) to refer to one outcome of the random variable \\(Y\\). If there are multiple outcomes — if we have, for example, flipped the coin multiple times — then we use subscripts to indicate the separate outcomes: \\(y_1\\), \\(y_2\\), and so on. The symbol for an arbitrary outcome is \\(y_i\\), where \\(i\\) ranges from 1 through \\(N\\), the total number of events or experiments for which an outcome \\(y\\) was produced.\nThe only package we need in this chapter is tidyverse.\nTo understand probability more fully, we first need to understand distributions.\nA variable in a tibble is a column, a vector of values. We sometimes refer to this vector as a “distribution.” This is somewhat sloppy in that a distribution can be many things, most commonly a mathematical formula. But, strictly speaking, a “frequency distribution” or an “empirical distribution” is a list of values, so this usage is not unreasonable.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "01-probability.html#sec-distributions",
    "href": "01-probability.html#sec-distributions",
    "title": "1  Probability",
    "section": "",
    "text": "1.1.1 Scaling distributions\nConsider the vector which is the result of rolling one dice 10 times.\n\nten_rolls &lt;- c(5, 5, 1, 5, 4, 2, 6, 2, 1, 5)\n\nThere are other ways of storing the data in this vector. Instead of reporting every observation, we could record the number of times each value appears or the percentage of the total which this number accounts for.\n\ntibble(outcome = ten_rolls) |&gt; \n  summarise(n = n(), .by = outcome) |&gt; \n  mutate(portion = n / sum(n)) |&gt; \n  arrange(outcome) |&gt; \n  gt() |&gt; \n    tab_header(title = \"Distribution of Ten Rolls of a Fair Dice\",\n               subtitle = \"Counts and percentages reflect the same information\") |&gt; \n    cols_label(outcome = \"Outcome\",\n               n = \"Count\",\n               portion = \"Percentage\") |&gt; \n    fmt_percent(columns = c(portion), decimals = 0)\n\n\n\n\n\n\nDistribution of Ten Rolls of a Fair Dice\n\n\nCounts and percentages reflect the same information\n\n\nOutcome\nCount\nPercentage\n\n\n\n\n1\n2\n20%\n\n\n2\n2\n20%\n\n\n4\n1\n10%\n\n\n5\n4\n40%\n\n\n6\n1\n10%\n\n\n\n\n\n\nIn this case, with only 10 values, it is actually less efficient to store the data like this. But what happens when we have 1,000 rolls?\n\nset.seed(89)\n\ntibble(outcome = sample(1:6, size = 1000, replace = TRUE)) |&gt; \n  summarise(n = n(), .by = outcome) |&gt; \n  mutate(portion = n / sum(n)) |&gt; \n  arrange(outcome) |&gt; \n  gt() |&gt; \n    tab_header(title = \"Distribution of One Thousand Rolls of a Fair Dice\",\n               subtitle = \"Counts and percentages reflect the same information\") |&gt; \n    cols_label(outcome = \"Outcome\",\n               n = \"Count\",\n               portion = \"Percentage\") |&gt; \n    fmt_percent(columns = c(portion), decimals = 0)\n\n\n\n\n\n\nDistribution of One Thousand Rolls of a Fair Dice\n\n\nCounts and percentages reflect the same information\n\n\nOutcome\nCount\nPercentage\n\n\n\n\n1\n190\n19%\n\n\n2\n138\n14%\n\n\n3\n160\n16%\n\n\n4\n173\n17%\n\n\n5\n169\n17%\n\n\n6\n170\n17%\n\n\n\n\n\n\nInstead of keeping around a vector of length 1,000, we can just keep 12 values — the 6 possible outcomes and their frequency — without losing any information.\nTwo distributions can be identical even if they are of very different lengths. Let’s compare our original distribution of 10 rolls of the dice with another distribution which just features 100 copies of those 10 rolls.\n\nmore_rolls &lt;- rep(ten_rolls, 100)\n\n\nrolls_p &lt;- tibble(value = ten_rolls) |&gt; \n  ggplot(aes(value)) +\n    geom_bar() +\n    labs(title = \"Distribution of 10 Rolls\",\n         y = \"Count\") \n\n\nmore_rolls_p &lt;- tibble(value = more_rolls) |&gt; \n  ggplot(aes(value)) +\n    geom_bar() +\n    labs(title = \"Distribution of 1,000 Rolls\",\n         y = NULL)\n\n\nrolls_p + more_rolls_p\n\n\n\n\n\n\n\nThe two graphs have the exact same shape because, even though the vectors are of different lengths, the relative proportions of the outcomes are identical. In some sense, both vectors are from the same distribution. Relative proportions, not the total counts, are what matter.\n\n1.1.2 Normalizing distributions\nIf two distributions have the same shape, then they only differ by the labels on the y-axis. There are various ways of “normalizing” distributions so as to place them all on the same scale. The most common scale is one in which the area under the distribution adds to 1, e.g., 100%. For example, we can transform the above plots:\n\nrolls_p &lt;- tibble(value = ten_rolls) |&gt; \n  ggplot(aes(x = value)) +\n    geom_bar(aes(y = after_stat(count/sum(count)))) +\n    labs(title = \"Distribution of 10 Rolls\",\n         y = \"Percentage\") +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1))\n\nmore_rolls_p &lt;- tibble(value = more_rolls) |&gt; \n  ggplot(aes(value)) +\n    geom_bar(aes(y = after_stat(count/sum(count)))) +\n    labs(title = \"Distribution of 1,000 Rolls\",\n         y = NULL) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1))\n\nrolls_p + more_rolls_p\n\n\n\n\n\n\n\nWe sometimes refer to a distribution as “unnormalized” if the area under the curve does not add up to 1.\n\n1.1.3 Simulating distributions\nThere are two distinct concepts: a distribution and a set values drawn from that distribution. But, in typical usage, we employ “distribution” for both. When given a distribution (meaning a vector of numbers), we often use geom_histogram() or geom_density() to display it. But, sometimes, we don’t want to look at the whole thing. We just want some summary measures which report the key aspects of the distribution. The two most important attributes of a distribution are its center and its variation around that center.\n\nWe use summarize() to calculate statistics for a variable, a column, a vector of values, or a distribution. Note the language sloppiness. For the purposes of this book, “variable,” “column,” “vector,” and “distribution” all mean the same thing. Other popular statistical functions include: mean(), median(), min(), max(), n() and sum(). Functions which may be new to you include three measures of the “spread” of a distribution: sd() (the standard deviation), mad() (the scaled median absolute deviation) and quantile(), which is used to calculate an interval which includes a specified proportion of the values.\n\nThink of the distribution of a variable as an urn from which we can pull out, at random, values for that variable. Drawing a thousand or so values from that urn, and then looking at a histogram, can show where the values are centered and how they vary. Because people are sloppy, they will use the word distribution to refer to at least three related entities:\n\nthe (imaginary!) urn from which we are drawing values.\nall the values in the urn\nall the values which we have drawn from the urn, whether that be 10 or 1,000\n\nSloppiness in the usage of the word distribution is universal. However, keep three distinct ideas separate:\n\nThe unknown true distribution which, in reality, generates the data which we see. Outside of stylized examples in which we assume that a distribution follows a simple mathematical formula, we will never have access to the unknown true distribution. We can only estimate it. This unknown true distribution is often referred to as the data generating mechanism, or DGM. It is a function or black box or urn which produces data. We can see the data. We can’t see the urn.\nThe estimated distribution which, we think, generates the data which we see. Again, we can never know the unknown true distribution. But, by making some assumptions and using the data we have, we can estimate a distribution. Our estimate may be very close to the true distribution. Or it may be far away. The main task of data science to to create and use these estimated distributions. Almost always, these distributions are instantiated in computer code. Just as there is a true data generating mechanism associated with the (unknown) true distribution, there is an estimated data generating mechanism associated with the estimated ditribution.\nA vector of numbers drawn from the estimated distribution. Both true and estimated distributions can be complex animals, difficult to describe accurately and in detail. But a vector of numbers drawn from a distribution is easy to understand and use. So, in general, we work with vectors of numbers. When someone — either a colleague or a piece of R code — creates a distribution which we want to use to answer a question, we don’t really want the distribution itself. Rather, we want a vector of “draws” from that distribution. Vectors are easy to work with! Complex computer code is not.\n\nAgain, people (including us!) will often be sloppy and use the same word, “distribution,” without making it clear whether they are talking about the true distribution, the estimated distribution, or a vector of draws from the estimated distribution. The same sloppiness applies to the use of the term data generating mechanism. Try not to be sloppy.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "01-probability.html#probability-distributions",
    "href": "01-probability.html#probability-distributions",
    "title": "1  Probability",
    "section": "\n1.2 Probability distributions",
    "text": "1.2 Probability distributions\n\nknitr::include_graphics(\"probability/images/de_finetti.jpg\")\n\n\n\nBruno de Finetti, an Italian statistician who wrote a famous treatise on the theory of probability that began with the statement “PROBABILITY DOES NOT EXIST.”\n\n\n\nFor the purposes of this Primer, a probability distribution is a mathematical object which maps a set of outcomes to probabilities, where each distinct outcome has a chance of occurring between 0 and 1 inclusive. The probabilities must sum to 1. The set of possible outcomes, i.e., the sample space — heads and tails for the coin, 1 through 6 for a single dice, 2 through 12 for the sum of a pair of dice — can be either discrete or continuous. Discrete data can only take on certain values. Continuous data, like height and weight, can take any value within a range. The set of outcomes is the domain of the probability distribution. The range is the associated probabilities.\nAssume that a probability distribution is created by a probability function, a set function which maps outcomes to probabilities. The concept of a “probability function” is often split into two categories: probability mass functions (for discrete random variables) and probability density functions (for continuous random variables). As usual, we will be a bit sloppy, using the term probability distribution for both the mapping itself and for the function which creates the mapping.\nWe discuss three types of probability distributions: empirical, mathematical, and posterior.\nThe key difference between a distribution, as we have explored them in Section 1.1, and a probability distribution is the requirement that the sum of the probabilities of the individual outcomes must be exactly 1. There is no such requirement for a distribution in general. But any distribution can be turned into a probability distribution by “normalizing” it. In this context, we will often refer to a distribution which is not (yet) a probability distribution as an “unnormalized” distribution.\n\nPay attention to notation. Recall that when we are talking about a specific probability (represented by a single value), we will use \\(\\rho\\) (the Greek letter “rho”) with a subscript which specifies the exact outcome of which it is the probability. For instance, \\(\\rho_h = 0.5\\) denotes the probability of getting heads on a coin toss when the coin is fair. \\(\\rho_t\\) — spoken as “PT” or “P sub T” or “P tails” — denotes the probability of getting tails on a coin toss. However, when we are referring to the entire probability distribution over a set of outcomes, we will use \\(P()\\). For example, the probability distribution of a coin toss is \\(P(\\text{coin})\\). That is, \\(P(\\text{coin})\\) is composed of the two specific probabilities (e.g., 60% and 40% for a biased coin) mapped from the two values in the domain (heads and tails). Similarly, \\(P(\\text{sum of two dice})\\) is the probability distribution over the set of 11 outcomes (2 through 12) which are possible when you take the sum of two dice. \\(P(\\text{sum of two dice})\\) is made up of 11 numbers — \\(\\rho_2\\), \\(\\rho_3\\), …, \\(\\rho_{12}\\) — each representing the unknown probability that the sum will equal their value. That is, \\(\\rho_2\\) is the probability of rolling a 2.\nA distribution is a function that shows the possible values of a variable and how often they occur.\n\n1.2.1 Flipping a coin\nData science problems start with a question. Example:\nWhat are the chances of getting three heads in a row when flipping a fair coin?\nQuestions are answered with the help of probability distributions.\nAn empirical distribution is based on data. Think of this as the probability distribution created by collecting data in the real world or by running a simulation on your computer. In theory, if we increase the number of coins we flip (either in reality or via simulation), the empirical distribution will look more and more similar to the mathematical distribution. The mathematical distribution is the Platonic form. The empirical distribution will often look like the mathematical probability distribution, but it will rarely be exactly the same.\nIn this simulation, there are 44 heads and 56 tails. The outcome will vary every time we run the simulation, but the proportion of heads to tails should not be too different if the coin is fair.\n\n# We are flipping one fair coin a hundreds times. We need to get the same result\n# each time we create this graphic because we want the results to match the\n# description in the text. Using set.seed() guarantees that the random results\n# are the same each time. We define 0 as tails and 1 as heads.\n\nset.seed(3)\n\ntibble(results = sample(c(0, 1), 100, replace = TRUE)) |&gt; \n  ggplot(aes(x = results)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))), \n                   binwidth = 0.5, \n                   color = \"white\") +\n    labs(title = \"Empirical Probability Distribution\",\n         subtitle = \"Flipping one coin a hundred times\",\n         x = \"Outcome\\nResult of Coin Flip\",\n         y = \"Probability\") +\n    scale_x_continuous(breaks = c(0, 1), \n                       labels = c(\"Heads\", \"Tails\")) +\n    scale_y_continuous(labels = \n                         scales::percent_format(accuracy = 1)) +\n    theme_classic()\n\n\n\n\n\n\n\nA mathematical distribution is based on a mathematical formula. Assuming that the coin is perfectly fair, we should, on average, get heads as often as we get tails.\n\n# The mathematical case with the expected 50/50 outcome hard-coded.\n\ntibble(results = c(rep(0, 50),\n                   rep(1, 50))) |&gt; \nggplot(aes(x = results)) +\n  geom_histogram(aes(y = after_stat(count/sum(count))), \n                 binwidth = 0.5, \n                 color = \"white\") +\n  labs(title = \"Mathematical Probability Distribution\",\n       subtitle = \"Expectations for flipping a fair coin\",\n       x = \"Outcome\\nResult of Coin Flip\",\n       y = \"Probability\") +\n  scale_x_continuous(breaks = c(0, 1), \n                     labels = c(\"Heads\", \"Tails\")) +\n  scale_y_continuous(labels =\n                      scales::percent_format(accuracy = 1)) +\n  theme_classic()\n\n\n\n\n\n\n\nThe distribution of a single observation is described by this formula.\n\\[ P(Y = y) = \\begin{cases} 1/2 &\\text{for }y= \\text{Heads}\\\\ 1/2 &\\text{for }y= \\text{Tails} \\end{cases}\\] We sometimes do not know that the probability of heads and the probability of tails both equal 50%. In that case, we might write:\n\\[ P(Y = y) = \\begin{cases} \\rho_H &\\text{for }y= \\text{Heads}\\\\ \\rho_T &\\text{for }y= \\text{Tails} \\end{cases}\\]\nYet, we know that, by definition, \\(\\rho_H + \\rho_T = 1\\), so we can rewrite the above as:\n\\[ P(Y = y) = \\begin{cases} \\rho_H &\\text{for }y= \\text{Heads}\\\\ 1- \\rho_H &\\text{for }y= \\text{Tails} \\end{cases}\\]\nCoin flipping (and related scenarios with only two possible outcomes) are such common problems that the notation is often simplified further, with \\(\\rho\\) understood, by convention, to be the probability of heads (or TRUE). In that case, we can write the mathematical distribution is two canonical forms:\n\\[P(Y) = Bernoulli(\\rho)\\] and\n\\[y_i \\sim Bernoulli(\\rho)\\] All five of these versions mean the same thing! The first four describe the mathematical probability distribution for a fair coin. The capital \\(Y\\) within the \\(P()\\) indicates a random variable. The fifth highlights one “draw” from that random variable, hence the lower case \\(y\\) and the subscript \\(i\\).\nMost probability distributions do not have special names, which is why we will use the generic symbol \\(P\\) to refer to them. But some common probability distributions do have names, like “Bernoulli” in this case.\nIf the mathematical assumptions are correct, then, as your sample size increases, the empirical probability distribution will look more and more like the mathematical distribution.\nA posterior distribution is based on beliefs and expectations. It displays your beliefs about things you can’t see right now. You may have posterior distributions for outcomes in the past, present, or future.\nIn the case of the coin toss, the posterior distribution changes depending on your beliefs. For instance, let’s say your friend brought a coin to school and asked to bet you. If the result is heads, you have to pay them $5. In that case, your posterior probability distribution might look like this:\n\n# Hard-code the creation of the posterior.\n\ntibble(results = c(rep(0, 95),\n                   rep(1, 5))) |&gt; \nggplot(aes(x = results)) +\n  geom_histogram(aes(y = after_stat(count/sum(count))), \n                 binwidth = 0.5, \n                 color = \"white\") +\n  labs(title = \"Posterior Probability Distribution\",\n       subtitle = \"Your beliefs about flipping one coin to bet your friend\",\n       x = \"Outcome\\nResult of Coin Flip\",\n       y = \"Probability\") +\n  scale_x_continuous(breaks = c(0, 1), \n                     labels = c(\"Heads\", \"Tails\")) +\n  scale_y_continuous(labels = \n                       scales::percent_format(accuracy = 1)) +\n  theme_classic()\n\n\n\n\n\n\n\nThe fact that your friend wants to bet on heads suggests to you that the coin is not fair. Does it prove that the coin is unfair? No! Much depends on the sort of person you think your friend is. Your posterior probability distribution is your opinion, based on your experiences and beliefs. My posterior probability distribution will often be (very) different from yours.\nThe full terminology is mathematical (or empirical or posterior) probability distribution. But we will often shorten this to just mathematical (or empirical or posterior) distribution. The word “probability” is understood, even if it is not present.\n\nRecall the question with which we started this section: What are the chances of getting three heads in a row when flipping a fair coin? To answer this question, we need to use a probability distribution as our data generating mechanism. Fortunately, the rbinom() function allows us to generate the results for coin flips. For example:\n\nrbinom(n = 10, size = 1, prob = 0.5)\n\n [1] 1 1 0 1 1 0 0 0 0 0\n\n\ngenerates the results of 10 coin flips, where a result of heads is presented as 1 and tails as 0. With this tool, we can generate 1,000 draws from our experiment:\n\ntibble(toss_1 = rbinom(n = 1000, size = 1, prob = 0.5),\n       toss_2 = rbinom(n = 1000, size = 1, prob = 0.5),\n       toss_3 = rbinom(n = 1000, size = 1, prob = 0.5))\n\n# A tibble: 1,000 × 3\n   toss_1 toss_2 toss_3\n    &lt;int&gt;  &lt;int&gt;  &lt;int&gt;\n 1      0      1      1\n 2      0      1      1\n 3      0      1      0\n 4      0      0      1\n 5      1      1      0\n 6      1      0      1\n 7      1      0      0\n 8      1      0      1\n 9      0      0      1\n10      0      1      1\n# ℹ 990 more rows\n\n\nBecause the flips are independent, we can consider each row to be a draw from the experiment. Then, we simply count up the proportion of experiments in which resulted in three heads.\n\ntibble(toss_1 = rbinom(n = 1000, size = 1, prob = 0.5),\n       toss_2 = rbinom(n = 1000, size = 1, prob = 0.5),\n       toss_3 = rbinom(n = 1000, size = 1, prob = 0.5)) |&gt; \n  mutate(three_heads = toss_1 + toss_2 + toss_3 == 3) |&gt; \n  summarize(chance = mean(three_heads))\n\n# A tibble: 1 × 1\n  chance\n   &lt;dbl&gt;\n1  0.104\n\n\nThis is close to the “correct” answer of \\(1/8\\)th. If we increase the number of draws, we will get closer to the “truth.” The reason for the quotation marks around “correct” and “truth” is that we are uncertain. We don’t know the true probability distribution for this coin. If this coin is a trick coin — like the one we expect our friend to have brought to school — then the odds of three heads in a row would be much higher:\n\ntibble(toss_1 = rbinom(n = 1000, size = 1, prob = 0.95),\n       toss_2 = rbinom(n = 1000, size = 1, prob = 0.95),\n       toss_3 = rbinom(n = 1000, size = 1, prob = 0.95)) |&gt; \n  mutate(three_heads = toss_1 + toss_2 + toss_3 == 3) |&gt; \n  summarize(chance = mean(three_heads))\n\n# A tibble: 1 × 1\n  chance\n   &lt;dbl&gt;\n1   0.87\n\n\nThis is our first example of using a data generating mechanism — meaning rbinom() — to answer a question. We will see many more in the chapters to come.\n\n1.2.2 Rolling two dice\nData science begins with a question:\nWhat is the probability of rolling a 7 or an 11 with a pair of dice?\nWe get an empirical distribution by rolling two dice a hundred times, either by hand or with a computer simulation. The result is not identical to the mathematical distribution because of the inherent randomness of the real world and/or of simulation.\n\n# In the coin example, we create the vector ahead of time, and then assigned\n# that vector to a tibble. There was nothing wrong with that approach. And we\n# could do the same thing here. But the use of map_* functions is more powerful,\n# although it requires creating the 100 rows of the tibble at the start and then\n# doing things \"row-by_row.\"\n\nset.seed(1)\n\nemp_dist_dice &lt;- tibble(ID = 1:100) |&gt; \n  mutate(die_1 = map_dbl(ID, ~ sample(c(1:6), size = 1))) |&gt; \n  mutate(die_2 = map_dbl(ID, ~ sample(c(1:6), size = 1))) |&gt; \n  mutate(sum = die_1 + die_2) |&gt; \n  ggplot(aes(x = sum)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))), \n                   binwidth = 1, \n                   color = \"white\") +\n    labs(title = \"Empirical Probability Distribution\",\n         subtitle = \"Sum from rolling two dice, replicated one hundred times\",\n         x = \"Outcome\\nSum of Two Dice\",\n         y = \"Probability\") +\n    scale_x_continuous(breaks = seq(2, 12, 1), labels = 2:12) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()\n\nemp_dist_dice\n\n\n\n\n\n\n\nWe might consider labeling the y-axis in plots of empirical distributions as “Proportion” rather than “Probability” since it is an actual proportion, calculated from real (or simulated) data. We will keep it as “Probability” since we want to emphasize the parallels between mathematical, empirical and posterior probability distributions.\nOur mathematical distribution tells us that, with a fair dice, the probability of getting 1, 2, 3, 4, 5, and 6 are equal: there is a 1/6 chance of each. When we roll two dice at the same time and sum the numbers, the values closest to the middle are more common than values at the edge because there are more combinations of numbers that add up to the middle values.\n\ntibble(sum = c(rep(c(2, 12), 1), \n               rep(c(3, 11), 2),\n               rep(c(4, 10), 3), \n               rep(c(5,  9), 4),\n               rep(c(6,  8), 5),\n               rep(c(7), 6))) |&gt; \n  ggplot(aes(x = sum)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))), \n                   binwidth = 1, \n                   color = \"white\") +\n    labs(title = \"Mathematical Probability Distribution\",\n         subtitle = \"Expectation for the sum from rolling two dice\",\n         x = \"Outcome\\nSum of Two Dice\",\n         y = \"Probability\") +\n    scale_x_continuous(breaks = seq(2, 12, 1), labels = 2:12) +\n    scale_y_continuous(labels = \n                         scales::percent_format(accuracy = 1)) +\n    theme_classic()\n\n\n\n\n\n\n\n\\[ P(Y = y) = \\begin{cases} \\dfrac{y-1}{36} &\\text{for }y=1,2,3,4,5,6 \\\\ \\dfrac{13-y}{36} &\\text{for }y=7,8,9,10,11,12 \\\\ 0 &\\text{otherwise} \\end{cases} \\]\nThe posterior distribution for rolling two dice depends on your beliefs. If you take the dice from your Monopoly set, you have reason to believe that the assumptions underlying the mathematical distribution are true. However, if you walk into a crooked casino and a host asks you to play craps, you might be suspicious, meaning that you suspect that the data generating mechanism for these dice is not like that for fair dice. For example, in craps, a “come-out” roll of 7 and 11 is a “natural,” resulting in a win for the “shooter” and, therefore, a loss for the casino. You might expect those numbers to occur less often than they would with fair dice. Meanwhile, a come-out roll of 2, 3 or 12 is a loss for the shooter. You might also expect values like 2, 3 and 12 to occur more frequently. Your posterior distribution might look like this:\n\n# Hard code our posterior about the crooked casino.\n\ntibble(results = c(rep(2, 15), rep(3, 15), rep(4, 7), \n                   rep(5, 7), rep(6, 7), rep(7, 4),\n                   rep(8, 7), rep(9, 7), rep(10, 7), \n                   rep(11, 3), rep(12, 15))) |&gt; \n  ggplot(aes(x = results)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))), \n                   binwidth = 1, \n                   color = \"white\") +\n    labs(title = \"Posterior Probability Distribution\",\n         subtitle = \"Your belief about the sum of two dice at a crooked casino\",\n         x = \"Outcome\\nSum of Two Dice\",\n         y = \"Probability\") +\n    scale_x_continuous(breaks = seq(2, 12, 1), labels = 2:12) +\n    scale_y_continuous(labels = \n                         scales::percent_format(accuracy = 1)) +\n    theme_classic()\n\n\n\n\n\n\n\nSomeone less suspicious of the casino would have a posterior distribution which looks more like the mathematical distribution.\nWe began this section with a question about the probability (or odds) of rolling a 7 or 11 — i.e., a “natural” — with a pair of dice. The answer to the question depends on whether or not we think the dice are fair. In other words, we need to know which distribution to use to answer the question.\nAssume that the dice are fair. In that case, we can create a data generating mechanism by hand. (Alas, there is not a built-in R function for dice like there is for coin flips with rbinom().)\n\nset.seed(7)\n\n# Creating a variable like rolls makes our code easier to read and modify. Of\n# course, we could just hard code the 4 into the size argument for each of the\n# two calls to sample, but that is much less convenient.\n\nrolls &lt;- 4\n\n# The details of the code matter. If we don't have replace = TRUE, sample will\n# only use each of the 6 possible values once. That might be OK if we are just\n# rolling the dice 4 times, but it won't work for thousands of rolls.\n\ntibble(dice_1 = sample(x = 1:6, size = rolls, replace = TRUE),\n       dice_2 = sample(x = 1:6, size = rolls, replace = TRUE)) |&gt; \n  mutate(result = dice_1 + dice_2) |&gt; \n  mutate(natural = ifelse(result %in% c(7, 11), TRUE, FALSE))\n\n# A tibble: 4 × 4\n  dice_1 dice_2 result natural\n   &lt;int&gt;  &lt;int&gt;  &lt;int&gt; &lt;lgl&gt;  \n1      2      2      4 FALSE  \n2      3      6      9 FALSE  \n3      4      3      7 TRUE   \n4      2      6      8 FALSE  \n\n\nThis code is another data generating mechanism or DGM. It allows us to simulate the distribution of the results from rolling a pair of fair dice. To answer our question, we simply increase the number of rolls and calculate the proportion of rolls which result in a 7 or 11.\n\nrolls &lt;- 100000\n\n# We probably don't need 100,000 rolls, but this code is so fast that it does\n# not matter. Generally 1,000 (or even 100) draws from the data generating\n# mechanism is enough for most practical purposes.\n\ntibble(dice_1 = sample(x = 1:6, size = rolls, replace = TRUE),\n       dice_2 = sample(x = 1:6, size = rolls, replace = TRUE)) |&gt; \n  mutate(result = dice_1 + dice_2) |&gt; \n  summarize(natural_perc = mean(result %in% c(7, 11)))\n\n# A tibble: 1 × 1\n  natural_perc\n         &lt;dbl&gt;\n1        0.221\n\n\nThe probability of rolling either a 7 or an 11 with a pair of fair dice is about 22%.\n\n1.2.3 Presidential elections\nData science begins with a question:\nWhat is the probability that the Democratic candidate will win the Presidential election?\nConsider the probability distribution for a political event, like a presidential election. We want to know the probability that Democratic candidate wins X electoral votes, where X comes from the range of possible outcomes: 0 to 538. (The total number of electoral votes in US elections since 1964 is 538.)\n\nThe empirical distribution in this case would involve counting the number of electoral votes that the Democratic candidate won in each of the Presidential elections in the last 50 years or so. Looking at elections since 1964, we can observe that the number of electoral votes that the Democratic candidate received in each election is different.\n\n# Votes are since 1964, for Democratic candidate\n\nmydata &lt;- tibble(electoral_votes = c(486, 191,  17, 297, 49, 13, 111, \n                                     370, 379, 266, 251, 365, 332, \n                                     227, 306, 226))\n\n# 226 is electoral votes for Joe Biden in 2024 presidential election.\n\nggplot(mydata, aes(x = electoral_votes)) +\n  geom_histogram(aes(y = after_stat(count/sum(count))), \n                 bins = 100, \n                 color = \"white\") +\n  labs(title = \"Empirical Probability Distribution\",\n       subtitle = \"Electoral votes from 16 previous elections\",\n       x = \"Outcome\\nElectoral Vote Total for Democratic Candidate\",\n       y = \"Probability\") +\n  scale_x_continuous(breaks = seq(0, 500, 100)) +\n  scale_y_continuous(labels = \n                       scales::percent_format(accuracy = 1)) +\n  theme_classic()\n\n\n\n\n\n\n\nGiven that we only have 15 observations, it is difficult to draw conclusions or make predictions based off of this empirical distribution. But “difficult” does not mean “impossible.” For example, if someone, more than a year before the election, offered to bet us 50/50 that the Democratic candidate was going to win more than 475 electoral votes, we would take the bet. After all, this outcome has only happened once in the last 15 elections, so a 50/50 bet seems like a great deal.\nWe can build a mathematical distribution for X which assumes that the chances of the Democratic candidate winning any given state’s electoral votes is 0.5 and that the results from each state are independent.\n\n# Code currently produces a warning about missing values. Not sure why? Maybe an\n# interaction with the hard-coded limits in the x-axis? \n\n# We want the hard-coded limits because we want to make it obvious that the\n# mathematical results suggest that outcomes &lt; 100 or &gt; 400 are essentially\n# impossible. Text could make that more clear.\n\n# rep(3,8) means there are 8 states that have 3 electoral votes, via the same\n# for the rest of the code chunck. We use c() to make the rep() functions\n# together, so that we can use it in tibble().\n\n# I think these electoral votes are for 2016. Probably should update them for post 2020 census.\n\nev &lt;- c(rep(3, 8), rep(4, 5), rep(5, 3), rep(6, 6), rep(7, 3), rep(8,2),\n        rep(9, 3), rep(10, 4), rep(11, 4), 12, 13, 14, 15, rep(16, 2), 18, 20 , 20, 19, 29, 38, 55)\n\nsims &lt;- 10000\n\nresult &lt;- tibble(total = (matrix(rbinom(sims*length(ev),\n                        size = 1, p = 0.5),\n                 ncol = length(ev)) %*% ev)[,1])\n\nggplot(data = result, aes(total)) +\n  geom_histogram(aes(y = after_stat(count/sum(count))), \n                 binwidth = 1) +\n  labs(title = \"Mathematical Probability Distribution\",\n       subtitle = \"Presidential elections with each state being 50/50 and independent\",\n       y = \"Probability\",\n       x = \"Outcome\\nElectoral Vote Total for Democratic Candidate\") +\n  scale_x_continuous(breaks = seq(0, 500, 100), limits = c(0, 540)) +\n  scale_y_continuous(labels = \n                       scales::percent_format()) +\n  theme_classic()\n\n\n\n\n\n\n\nIf our assumptions about this mathematical distribution are correct — they are not! — then, as the sample size increase, the empirical distribution should look more and more similar to our mathematical distribution.\nHowever, the data from past elections is more than enough to demonstrate that the assumptions of our mathematical probability distribution do not work for electoral votes. The model assumes that the Democrats have a 50% chance of receiving each of the total electoral votes from the 50 states and the District of Columbia. Just looking at the mathematical probability distribution, we can observe that receiving 13 or 17 or 486 votes out of 538 would be extreme and almost impossible if the mathematical model were accurate. However, our empirical distribution shows that such extreme outcomes are quite common. Presidential elections have resulted in much bigger victories or defeats than this mathematical distribution seems to allow for, thereby demonstrating that our assumptions are false.\nThe posterior distribution of electoral votes is a popular topic, and an area of strong disagreement, among data scientists. Consider this posterior from FiveThirtyEight.\n\n# This data was downloaded by hand from the 538 website.\n\n# Have to include these cols type, otherwise it will shows up this \"Column\n# specification\", thing on the primer, not pretty.\n\nread_csv(\"./probability/data/election-forecasts-2020/prez_1.csv\",\n         col_types = cols(cycle = col_double(),\n                          branch = col_character(),\n                          model = col_character(),\n                          modeldate = col_character(),\n                          candidate_inc = col_character(),\n                          candidate_chal = col_character(),\n                          candidate_3rd = col_logical(),\n                          evprob_inc = col_double(),\n                          evprob_chal = col_double(),\n                          evprob_3rd = col_logical(),\n                          total_ev = col_double(),\n                          timestamp = col_character(),\n                          simulations = col_double())) |&gt;\n  select(evprob_chal, total_ev)  |&gt; \n  rename(prob = evprob_chal, electoral_votes = total_ev) |&gt; \n  ggplot(aes(electoral_votes, prob)) +\n    geom_bar(stat = 'identity') +\n    labs(title = \"Posterior Probability Distribution\",\n         subtitle = \"Democratic electoral votes according to 538 forecast\",\n         y = \"Probability\",\n         x = \"Outcome\\nElectoral Vote Total for Democratic Candidate\",\n         caption = \"Data from August 13, 2020\") +\n  scale_y_continuous(breaks = c(0, 0.01, 0.02),\n                     labels = \n                       scales::percent_format(accuracy = 1)) +\n  theme_classic()\n\n\n\n\n\n\n\nBelow is a posterior probability distribution from the FiveThirtyEight website for August 13, 2020. This was created using the same data as the above distribution, but is displayed differently. For each electoral result, the height of the bar represents the probability that a given event will occur. However, there are no labels on the y-axis telling us what the specific probability of each outcome is. And that is OK! The specific values are not that useful. If we removed the labels on our own y-axes, would it matter? Probably not. Anytime there are many possible outcomes — over 500 in this case — we stop looking at specific outcomes and, instead, look at where most of the “mass” of the distribution lies.\n\nknitr::include_graphics(\"probability/images/fivethirtyeight.png\")\n\n\n\n\n\n\n\nBelow is the posterior probability distribution from The Economist, also from August 13, 2020. This looks confusing at first because they chose to combine the axes for Republican and Democratic electoral votes. The Economist was less optimistic, relative to FiveThirtyEight, about Trump’s chances in the election.\n\nknitr::include_graphics(\"probability/images/economist_aug13.png\")\n\n\n\n\n\n\n\nThese two models, built by smart people using similar data sources, have reached fairly different conclusions. Data science is difficult! There is not one “right” answer. Real life is not a problem set.\n\nknitr::include_graphics(\"probability/images/538_versus_Economist.png\")\n\n\n\nWatch the makers of these two models throw shade at each other on Twitter! Eliot Morris is one of the primary authors of the Economist model. Nate Silver is in charge of 538. They don’t seem to be too impressed with each other’s work! More smack talk here and here.\n\n\n\nThere are many questions you could explore with posterior distributions. They can relate to the past, present, or future.\n\nPast: How many electoral votes would Hilary Clinton have won if she had picked a different VP?\nPresent: What are the total campaign donations from New College faculty?\nFuture: How many electoral votes will the Democratic candidate for president win in 2036?\n\n\n1.2.4 Height\n\nQuestion: What is the height of the next adult male we will meet?\nThe three examples above are all discrete probability distributions, meaning that the outcome variable can only take on a limited set of values. A coin flip has two outcomes. The sum of a pair of dice has 11 outcomes. The total electoral votes for the Democratic candidate has 539 possible outcomes. In the limit, we can also create continuous probability distributions which have an infinite number of possible outcomes. For example, the average height for an American male could be any real number between 0 inches and 100 inches. (Of course, a value anywhere near 0 or 100 is absurd. The point is that the average could be 68.564, 68.5643, 68.56432 68.564327, or any real number.)\nAll the characteristics for discrete probability distributions which we reviewed above apply just as much to continuous probability distributions. For example, we can create mathematical, empirical and posterior probability distributions for continuous outcomes just as we did for discrete outcomes.\nThe empirical distribution involves using data from the National Health and Nutrition Examination Survey (NHANES).\n\n# Use nhanes data set for this exercise, filter for adult and male.\n\nnhanes |&gt;\n  filter(sex == \"Male\", age &gt;= 18) |&gt;\n  select(height)|&gt;\n  drop_na() |&gt;\n  ggplot(aes(x = height)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))), \n                 binwidth = 1, \n                 color = \"white\")+\n  labs(title = \"Empirical Probability Distribution\",\n       subtitle = \"Height for adult men\",\n       x = \"Height (cm)\",\n       y = \"Probability\",caption = \"Source:NHANES\") +\n  scale_y_continuous(labels =\n                      scales::percent_format(accuracy = 1)) +\n  theme_classic()\n\n\n\n\n\n\n\nMathematical distribution is completely based on mathematical formula and assumptions, as in the coin flip example. In the coin-flip example, we assumed that the coin was perfectly fair, meaning that the probability of landing on heads or tails was equal. In this case, we make three assumptions. First, a male height follows a Normal distribution. Second, the average height of men is 175 cm. Third, the standard deviation for male height is 9 cm. We can create a Normal distribution using the rnorm() function with these two parameter values.\n\n# Greater the n size, the smoother the graph. Shouldn't we use geom_density?\n\ntibble(height = rnorm(1000000, mean = 175, sd = 9)) |&gt; \n  ggplot(aes(x = height)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))), \n                 binwidth = 1, \n                 color = \"white\")+\n  labs(title = \"Mathematical Probability Distribution\",\n       subtitle = \"Male height assuming a normal distribution with a mean of 175 cm and sd of 9 cm\",\n       x = \"Height (cm)\",\n       y = \"Probability\") +\n  scale_y_continuous(labels =\n                      scales::percent_format(accuracy = 1)) +\n  theme_classic()\n\n\n\n\n\n\n\nAgain, the Normal distribution which is a probability distribution that is symmetric about the mean described by this formula.\n\\[y_i \\sim N(\\mu, \\sigma^2)\\]\n\nEach value \\(y_i\\) is drawn from a Normal distribution with parameters \\(\\mu\\) for the mean and \\(\\sigma\\) for the standard deviation. If the assumptions are correct, then, as our sample size increases, the empirical probability distribution will look more and more like the mathematical distribution.\nThe posterior distribution for heights depends on the context. Are we considering all the adult men in America? In that case, our posterior would probably look a lot like the empirical distribution using NHANES data. If we are being asked about the distribution of heights among players in the NBA, then our posterior might look like:\n\ntibble(height = rnorm(100000, 200, 6)) |&gt; \n    ggplot(aes(x = height)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))), \n                   binwidth = 1, \n                   color = \"white\")+\n    labs(title = \"Posterior Probability Distribution\",\n         subtitle = \"My belief about the heights of professional basketball players\",\n         x = \"Height (cm)\",\n         y = \"Probability\") +\n    scale_y_continuous(labels =\n                           scales::percent_format(accuracy = 1)) +\n    theme_classic()\n\n\n\n\n\n\n\nCaveats:\n\nContinuous variables are a myth. Nothing that can be represented on a computer is truly continuous. Even something which appears continuous, like height, actually can only take on a (very large) set of discrete variables.\nThe math of continuous probability distributions can be tricky. Read a book on mathematical probability for all the messy details. Little of that matters in applied work.\nThe most important difference is that, with discrete distributions, it makes sense to estimate the probability of a specific outcome. What is the probability of rolling a 9? With continuous distributions, this makes no sense because there are an infinite number of possible outcomes. With continuous variables, we only estimate intervals.\n\nDon’t worry about the distinctions between discrete and continuous outcomes, or between the discrete and continuous probability distributions which we will use to summarize our beliefs about those outcomes. The basic intuition is the same in both cases.\n\n1.2.5 Joint distributions\n\nRecall that \\(P(\\text{coin})\\) is the probability distribution for the result of a coin toss. It includes two parts, the probability of heads (\\(\\rho_h\\)) and the probability of tails (\\(\\rho_t\\)). This is a univariate distribution because there is only one outcome, which can be heads or tails. If there is more than one outcome, then we have a joint distribution.\n\nJoint distributions are also mathematical objects that cover a set of outcomes, where each distinct outcome has a chance of occurring between 0 and 1 and the sum of all chances must equal 1. The key to a joint distribution is that it measures the chance that both outcome \\(a\\) from the set of events A and outcome \\(b\\) from the set of events B will occur. The notation is \\(P(A, B)\\).\nLet’s say that you are rolling two six-sided dice simultaneously. Dice 1 is weighted so that there is a 50% chance of rolling a 6 and a 10% chance of each of the other values. Dice 2 is weighted so there is a 50% chance of rolling a 5 and a 10% chance of rolling each of the other values. Let’s roll both dice 1,000 times. In previous examples involving two dice, we cared about the sum of results and not the outcomes of the first versus the second dice of each simulation. With a joint distributions, the outcomes for individual dice matter; so instead of 11 possible outcomes on the x-axis of our distribution plot (ranging from 2 to 12), we have 36 outcomes. Furthermore, a 2D probability distribution is not sufficient to represent all of the variables involved, so the joint distribution for this example is displayed using a 3D plot.\n\n\n# Revisit and clean up this code.\n\n# Step 1: Create and organize the data.\n\nmydata &lt;- tibble(die.1 = sample(1:6, size = 1000, prob =\n                                  c(0.1, 0.1, 0.1, 0.1, 0.1, 0.5), replace = TRUE),\n                 die.2 = sample(1:6, size = 1000, prob =\n                                  c(0.1, 0.1, 0.1, 0.1, 0.5, 0.1), replace = TRUE)) |&gt;\n          summarize(total = n(),\n                    .by = c(die.1, die.2))\n\ndie1_factor &lt;- as.factor(mydata$die.1)\ndie2_factor &lt;- as.factor(mydata$die.2)\n\n\n# Step 2: Create a ggplot object.\n\nmtplot &lt;-  ggplot(mydata) +\n              geom_point(aes(x = die1_factor, y = die2_factor, color = total)) +\n              scale_color_continuous(limits = c(0, 100)) +\n              labs(x = \"Dice 1\", y = \"Dice 2\") +\n              theme(legend.position = \"none\")\n\n\n# Step 3: Turn it into a 3D plot. This takes a bit of time and\n# will open an interactive \"RGL Device\" window with the plot.\n\nplot_gg(mtplot,\n         width = 3.5, # Plot width\n         zoom = 0.65, # How close view on plot should be (1=normal, &lt;1=closer)\n         theta = 25, # From which direction to view plot (0-360°)\n         phi = 30, # How \"steep\" view on plot should be (0-90°)\n         sunangle = 225, # Angle of sunshine for shadows (0-360°)\n         soliddepth = -0.5, # Thickness of pane (always &lt;0, the smaller the thicker)\n         windowsize = c(2048,1536)) # Resolution\n\n# Save the plot as an image\nrgl::rgl.snapshot(\"probability/images/die.png\")\n\n# Close the RGL device\nrgl::close3d()\n\n\n\n\n\n\n\n\n\n\n1.2.6 Conditional distrubutions\nImagine that 60% of people in a community have a disease. A doctor develops a test to determine if a random person has the disease. However, this test isn’t 100% accurate. There is an 80% probability of correctly returning positive if the person has the disease and 90% probability of correctly returning negative if the person does not have the disease.\nThe probability of a random person having the disease is 0.6. Since each person either has the disease or doesn’t (those are the only two possibilities), the probability that a person does not have the disease is \\(1 - 0.6 =  0.4\\).\n\n\n\n\n\n\n\n\n\nIf a person has the disease, then we go up the top branch. The probability of an infected person testing positive is 0.8 because the test is 80% sure of correctly returning positive when the person has the disease.\nBy the same logic, if a person does not have the disease, we go down the bottom branch. The probability of the person incorrectly testing positive is 0.1.\n\nWe decide to go down the top branch if our random person has the disease. We go down the bottom branch if they do not. This is conditional probability. The probability of testing positive is dependent on whether the person has the disease.\nHow would you express this in statistical notation? \\(P(A|B)\\) is the same thing as the probability of A given B. \\(P(A|B)\\) means the probability of A if we know for sure the value of B. Note that \\(P(A|B)\\) is not the same thing as \\(P(B|A)\\).\nThere are three main categories of probability distributions: univariate, joint and condictional. \\(p(A)\\) is the probability distribution for event A. This is a univariate probability distribution because there is only one random variable. \\(p(A, B)\\) is the joint probability distribution of A and B. \\(p(A | B)\\) is the conditional probability distribution of A given that B has taken on a specific value. This is often written as \\(p(A | B = b)\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "01-probability.html#two-models",
    "href": "01-probability.html#two-models",
    "title": "1  Probability",
    "section": "\n1.3 Two models",
    "text": "1.3 Two models\n\n\n\nThe simplest possible setting for inference involves two models — meaning two possible states of the world — and two outcomes from an experiment. Imagine that there is a disease — Probophobia, an irrational fear of probability — which you either have or don’t have. We don’t know if you have the diseases, but we assume that these are the only two possibilities.\nWe also have a test which is 99% accurate when given to a person who has Probophobia. Unfortunately, the test is only 50% accurate for people who do not have Probophobia. In this experiment, there only two possible outcomes: a positive or a negative result on the test.\nQuestion: If you test positive, what is the probability that you have Probophobia?\nMore generally, we are estimating a conditional probability. Conditional on the outcome of a positive test, what is the probability that you have Probophobia? Mathematically, we want:\n\\[ P(\\text{Probophobia | Test = Postive} ) \\]\nTo answer this question, we need to use the tools of joint and conditional probability from earlier in the Chapter. We begin by building, by hand, the joint distribution of the possible models (you have the Probophobia or you do not) and of the possible outcomes (you test positive or negative). Building the joint distribution involves assuming that each model is true and then creating the distribution of outcomes which might occur if that assumption is true.\nFor example, assume you have Probophobia. There is then a 99% chance that you test positive and a 1% chance you test negative. Similarly, if we assume that the second model is true — i.e., that you don’t have Probophobia — then there is 50% chance you test positive and a 50% you chance negative. Of course, for you (or any individual) we do not know for sure what is happening. We do not know if you have the disease. We do not know what your test will show. But we can use these relationships to construct the joint distribution.\n\nset.seed(9)\n\n\n\n# Pipes generally start with tibbles, so we start with a tibble which just\n# includes an ID variable. We don't really use ID. It is just handy for getting\n# organized. We call this object `jd_disease`, where the `jd` stands for\n# joint distribution.\n\nsims &lt;- 10000\n\njd_disease &lt;- tibble(ID = 1:sims, have_disease = rep(c(TRUE, FALSE), 5000)) |&gt;\n  mutate(positive_test =\n           if_else(have_disease,\n                   map_int(have_disease, ~ rbinom(n = 1, size = 1, p = 0.99)),\n                   map_int(have_disease, ~ rbinom(n = 1, size = 1, p = 0.5))))\n\n\n\njd_disease\n\n# A tibble: 10,000 × 3\n      ID have_disease positive_test\n   &lt;int&gt; &lt;lgl&gt;                &lt;int&gt;\n 1     1 TRUE                     1\n 2     2 FALSE                    1\n 3     3 TRUE                     1\n 4     4 FALSE                    1\n 5     5 TRUE                     1\n 6     6 FALSE                    0\n 7     7 TRUE                     1\n 8     8 FALSE                    1\n 9     9 TRUE                     1\n10    10 FALSE                    0\n# ℹ 9,990 more rows\n\n\nThe first step is to simply create an tibble that consists of the simulated data we need to plot our distribution. Keep in mind that in the setting we have two different probabilities and they are completely separate from each other and we want to keep the two probabilities and the disease results in two and only two columns so that we can graph using the ggplot() function. And that’s why we used the rep and seq functions when creating the table, we used the seq function to set the sequence we want, which in this case is only two numbers, 0.01 (99% accuracy for testing negative if no disease, therefore 1% for testing positive if no disease) and 0.5 (50% accuracy for testing positive/negative if have disease), then we used the rep functions to repeat the process 10,000 times for each probability, in total 20,000 times. Note that this number “20,000” also represents the number of observations in our simulated data, we simulated 20,000 results from testing, where 10,000 results from the have-disease group and 10,000 for the no-disease group, we often use the capital N to represent the population, in this simulated data \\(N=20,000\\).\nPlot the joint distribution:\n\n# Use discrete for FALSE and TRUE\n\njd_disease |&gt; \n  ggplot(aes(x = as.factor(positive_test), \n             y = as.factor(have_disease))) +\n  geom_point() +\n  geom_jitter(alpha = .2) +\n  labs(title = \"Unnormalized Distribution of Test Results and Disease Status\",\n       subtitle = \"Many False results in have disease than no disease\",\n       x = \"Test Result\",\n       y = \"Disease Status\") +\n  scale_x_discrete(breaks = c(0, 1), \n                   labels = c(\"Negative\", \"Positive\")) +\n  scale_y_discrete(breaks = c(FALSE, TRUE), \n                   labels = c(\"No Disease\", \"Have Disease\")) +\n  theme_classic()\n\n\n\n\n\n\n\nBelow is a joint distribution displayed in 3D. Instead of using the “jitter” feature in R to unstack the dots, we are using a 3D plot to visualize the number of dots in each box. The number of people who correctly test negative is far greater than of the other categories. The 3D plot shows the total number of cases for each section (True positive, True negative, False positive, False negative),the 3D bar coming from those combinations. Now,pay attention to the two rows of the 3D graph, if you trying to add up the length of the 3D bar for the top two sections and the bottom two sections, they should be equal to each other, where each have 10,000 case. This is because we simulate the experience in two independent and separate world one in the have-disease world and one in the no-disease world.\n\n# Pile up the dots in one point.\n# Later use for rayshader\n\njd_disease_plot &lt;- jd_disease |&gt;\n  summarize(total = n(),\n            .by = c(have_disease, positive_test)) |&gt; \n  ggplot(aes(x = as.factor(positive_test),\n             y = as.factor(have_disease),\n             color = total)) +\n    geom_point(size = 5) +\n    scale_color_continuous(limits = c(0, 5000)) +\n    scale_x_discrete(breaks = c(0, 1),\n                     labels = c(\"Negative\", \"Positive\")) +\n    scale_y_discrete(breaks = c(FALSE, TRUE),\n                     labels = c(\"No Disease\", \"Have Disease\")) +\n    labs(x = \"Test Result\",\n         y = \"Disease Status\",\n         title = \"Unnormalized Distribution of Results and Status\",\n         subtitle = \"Many False results in have disease than no disease\",\n         color = \"Cases\") +\n    theme_classic() +\n    theme(legend.position = \"none\",\n          title = element_text(size = 7),\n          axis.text.x = element_text(size = 7),\n          axis.text.y = element_text(size = 7))\n\nplot_gg(jd_disease_plot,\n        width = 3.5,\n        zoom = 0.65,\n        theta = 25,\n        phi = 30,\n        sunangle = 225,\n        soliddepth = -0.5,\n        windowsize = c(2048, 1536))\n\n# Save the plot as an image\nrgl::rgl.snapshot(\"probability/images/disease.png\")\n\n# Close the RGL device\nrgl::close3d()\n\n\n\n\n\n\n\n\n\n\n\nThis Section is called “Two Models” because, for each person, there are two possible states of the world: have the disease or not have the disease. By assumption, there are no other outcomes. We call these two possible states of the world “models,” even though they are very simple models.\nIn addition to the two models, we have two possible results of our experiment on a given person: test positive or test negative. Again, this is an assumption. We do not allow for any other outcome. In coming sections, we will look at more complex situations where we consider more than two models and more than two possible results of the experiment. In the meantime, we have built the unnormalized joint distribution for models and results. This is a key point! Look back earlier in this Chapter for discussions about both unnormalized distributions and joint distributions.\nWe want to analyze these plots by looking at different slices. For instance, let’s say that you have tested positive for the disease. Since the test is not always accurate, you cannot be 100% certain that you have it. We isolate the slice where the test result equals 1 (meaning positive).\n\njd_disease |&gt; \n  filter(positive_test == 1)\n\n# A tibble: 7,484 × 3\n      ID have_disease positive_test\n   &lt;int&gt; &lt;lgl&gt;                &lt;int&gt;\n 1     1 TRUE                     1\n 2     2 FALSE                    1\n 3     3 TRUE                     1\n 4     4 FALSE                    1\n 5     5 TRUE                     1\n 6     7 TRUE                     1\n 7     8 FALSE                    1\n 8     9 TRUE                     1\n 9    11 TRUE                     1\n10    12 FALSE                    1\n# ℹ 7,474 more rows\n\n\nMost people who test positive are infected This is a result for common diseases like cold. We can easily create an unnormalized conditional distribution with:\n\n# geom_histogram won't work because we are using non-continuous value.\n# geom_col won't work because need two aesthetic both x and y\n# geom_bar work\n\njd_disease |&gt; \n  filter(positive_test == 1) |&gt; \n  ggplot(aes(have_disease)) +\n    geom_bar() +\n    labs(title = \"Disease Status Given Positive Test\",\n         subtitle = \"Almost all of the cases were infected\",\n         x = \"Disease Status\",\n         y = \"Count\") +\n    scale_x_discrete(breaks = c(FALSE, TRUE),\n                       labels = c(\"Healthy\", \"Infected\")) +\n  theme_classic()\n\n\n\n\n\n\n\nfilter() transforms a joint distribution into a conditional distribution.\nTurn this unnormalized distribution into a posterior probability distribution:\n\n# Use discrete for TRUE and FALSE\n\njd_disease |&gt; \n  filter(positive_test == 1) |&gt; \n  ggplot(aes(have_disease)) +\n    geom_bar(aes(y = after_stat(count/sum(count))), \n                   color = \"white\") +\n    labs(title = \"Posterior for Probophobia Conditional on Positive Test\",\n         x = \"Probophobia Status\",\n         y = \"Probability\") +\n    scale_x_discrete(breaks = c(FALSE, TRUE),\n                       labels = c(\"Healthy\", \"Infected\")) +\n    scale_y_continuous(labels = \n                         scales::percent_format(accuracy = 1)) +\n    theme_classic()  \n\n\n\n\n\n\n\nIf we zoom in on the plot, about 70% of people who tested positive have the disease and 30% who tested positive do not have the disease. In this case, we are focusing on one slice of the probability distribution where the test result was positive. There are two disease outcomes: positive or negative. By isolating a section, we are looking at a conditional distribution. Conditional on a positive test, you can visualize the likelihood of actually having the disease versus not.\nNow recalled the question we asked at the start of the session:\nIf you test positive, what is the probability that you have Probophobia?\nBy looking at the posterior graph we just create, we can answer this question easily:\nWith a positive test, you can be almost 70% sure that you have Probophobia, however there is a good chance of about 30% that you receive a false positive, so don’t worry too much, there is still about a third of hope that you get the wrong result\nNow let’s consider the manipulation of this posterior, here is another question. Question : 10 people walks up to testing center, 5 of them tested negative, 5 of them tested positive, what is the probability of at least 6 people are actually healthy? \n\ntibble(test = 1:100000) |&gt;\n  mutate(person1 =  map_int(test, ~ rbinom(n = 1, size = 1, p = 0.3))) |&gt;\n  mutate(person2 =  map_int(test, ~ rbinom(n = 1, size = 1, p = 0.3))) |&gt;\n  mutate(person3 =  map_int(test, ~ rbinom(n = 1, size = 1, p = 0.3))) |&gt;\n  mutate(person4 =  map_int(test, ~ rbinom(n = 1, size = 1, p = 0.3))) |&gt;\n  mutate(person5 =  map_int(test, ~ rbinom(n = 1, size = 1, p = 0.3))) |&gt;\n  mutate(person6 =  map_int(test, ~ rbinom(n = 1, size = 1, p = 0.7))) |&gt;\n  mutate(person7 =  map_int(test, ~ rbinom(n = 1, size = 1, p = 0.7))) |&gt;\n  mutate(person8 =  map_int(test, ~ rbinom(n = 1, size = 1, p = 0.7))) |&gt;\n  mutate(person9 =  map_int(test, ~ rbinom(n = 1, size = 1, p = 0.7))) |&gt;\n  mutate(person10 = map_int(test, ~ rbinom(n = 1, size = 1, p = 0.7))) |&gt;\n  select(!test) |&gt; \n  \n  # The tricky part of this code is that we want to sum the outcomes across the\n  # rows of the tibble. This is different from our usual approach of summing\n  # down the columns, as with summarize(). The way to do this is to, first, use\n  # rowwise() to tell R that we want to work with rows in the tibble and then,\n  # second, use c_across() to indicate which variables we want to work with.\n  \n  rowwise() |&gt; \n  mutate(total = sum(c_across(person1:person10))) |&gt;\n  \n  ggplot(aes(total)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   binwidth = 1,\n                   color = \"white\") +\n    scale_x_continuous(breaks = c(0:10)) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "01-probability.html#three-models",
    "href": "01-probability.html#three-models",
    "title": "1  Probability",
    "section": "\n1.4 Three models",
    "text": "1.4 Three models\n\n\n\n\n\n\n\n\n\nImagine that your friend gives you a bag with two marbles. There could either be two white marbles, two black marbles, or one of each color. Thus, the bag could contain 0% white marbles, 50% white marbles, or 100% white marbles. The proportion, \\(p\\), of white marbles could be, respectively, 0, 0.5, or 1.\nQuestion: What is the chance of the bag contains exactly two white marbles, given that when we selected the marbles three times, every time we select a white marble?\n\\[ P(\\text{2 White Marbles in bag | White Marbles Sampled = 3} ) \\] Just as during the Probophobia models, in order to answer this question, we need to start up with the simulated data and then graph out the joint distribution of this scenario because we need to consider all possible outcomes of this model, and then based on the joint distribution we can slice out the the part we want (the conditional distribution) in the end making a posterior graph as well as normalizing it to see the probability.\nStep 1: Simulate the data into an tibble\nLet’s say you take a marble out of the bag, record whether it’s black or white, then return it to the bag. You repeat this three times, observing the number of white marbles you see out of three trials. You could get three whites, two whites, one white, or zero whites as a result of this trial. We have three models (three different proportions of white marbles in the bag) and four possible experimental results. Let’s create 3,000 draws from this joint distribution:\n\n# Create the joint distribution of the number of white marbles in the bag\n# (in_bag) and the number of white marbles pulled out in the sample (in_sample),\n# one-by-one. in_bag takes three possible values: 0, 1 and 2, corresponding to\n# zero, one and two white marbles potentially in the bag.\n\nset.seed(3)\nsims &lt;- 10000\n\n# We also start off with a tibble. It just makes things easier\n\njd_marbles &lt;- tibble(ID = 1:sims) |&gt; \n  \n  # For each row, we (randomly!) determine the number of white marbles in the\n  # bag. We do not know why the `as.integer()` hack is necessary. Shouldn't\n  # `map_int()` automatically coerce the result of `sample()` into an integer?\n  \n  mutate(in_bag = map_int(ID, ~ as.integer(sample(c(0, 1, 2), \n                                                  size = 1)))) |&gt;\n  \n  # Depending on the number of white marbles in the bag, we randomly draw out 0,\n  # 1, 2, or 3 white marbles in our experiment. We need `p = ./2` to transform\n  # the number of white marbles into the probability of drawing out a white\n  # marble in a single draw. That probability is either 0%, 50% or 100%.\n  \n  mutate(in_sample = map_int(in_bag, ~ rbinom(n = 1, \n                                              size = 3, \n                                              p = ./2))) \n\njd_marbles\n\n# A tibble: 10,000 × 3\n      ID in_bag in_sample\n   &lt;int&gt;  &lt;int&gt;     &lt;int&gt;\n 1     1      0         0\n 2     2      1         3\n 3     3      2         3\n 4     4      1         1\n 5     5      2         3\n 6     6      2         3\n 7     7      1         0\n 8     8      2         3\n 9     9      0         0\n10    10      1         2\n# ℹ 9,990 more rows\n\n\nStep 2: Plot the joint distribution:\n\n# The distribution is unnormalized. All we see is the number of outcomes in each\n# \"bucket.\" Although it is never stated clearly, we are assuming that there is\n# an equal likelihood of 0, 1 or 2 white marbles in the bag.\n\njd_marbles |&gt;\n  ggplot(aes(x = in_sample, y = in_bag)) +\n    geom_jitter(alpha = 0.5) +\n    labs(title = \"Black and White Marbles\",\n         subtitle = \"More white marbles in bag mean more white marbles selected\",\n         x = \"White Marbles Selected\",\n         y = \"White Marbles in the Bag\") +\n    scale_y_continuous(breaks = c(0, 1, 2)) +\n  theme_classic()\n\n\n\n\n\n\n\nHere is the 3D visualization:\n\n# Same process, first condense point plot to a single plot, then use plot_gg make it 3d and \n# give height.\n\njd_marbles_plot &lt;- jd_marbles |&gt;\n  summarize(total = n(), \n            .by = c(in_bag, in_sample)) |&gt; \n  mutate(in_sample = as.factor(in_sample)) |&gt; \n  mutate(in_bag = as.factor(in_bag)) |&gt; \n  ggplot(aes(x = in_sample, y = in_bag, color = total)) +\n    geom_point() +\n    scale_color_continuous(limits = c(0, 3500)) +\n    labs(x = \"White Marbles Selected\",\n         y = \"White Marbles in the Bag\",\n         title = \"Black and White Marbles\",\n         subtitle = \"More white marbles in bag mean more are selected\",\n         color = \"Count\") +\n    theme(legend.position = \"none\",\n          title = element_text(size = 7),\n    axis.text.x = element_text(size = 6),\n    axis.text.y = element_text(size = 6))\n\nplot_gg(jd_marbles_plot,\n         width = 4,\n         zoom = 0.75,\n         theta = 25,\n         phi = 30,\n         sunangle = 225,\n         soliddepth = -0.5,\n         raytrace = FALSE,\n         windowsize = c(2048,1536))\n\n# Save the plot as an image\nrgl::rgl.snapshot(\"probability/images/marbles.png\")\n\n# Close the RGL device\nrgl::close3d()\n\n\n\n\n\n\n\n\n\nThe y-axes of both the scatterplot and the 3D visualization are labeled “Number of White Marbles in the Bag.” Each value on the y-axis is a model, a belief about the world. For instance, when the model is 0, we have no white marbles in the bag, meaning that none of the marbles we pull out in the sample will be white.\nNow recalls the question, we essentially only care about the fourth column in the joint distribution (x-axis=3) because the question is asking us to create a conditional distribution given that fact that 3 marbles were selected. Therefore, we could isolate the slice where the result of the simulation involves three white marbles and zero black ones. Here is the unnormalized probability distribution.\nStep 3: Plot the unnormalized conditional distribution.\n\n# The key step is the filter. Creating a conditional distribution from a joint\n# distribution is the same thing as filtering that joint distribution for a\n# specific value. A conditional distribution is a \"slice\" of the joint\n# distribution, and we take that slice with filter().\n\njd_marbles |&gt; \n  filter(in_sample == 3) |&gt; \n  ggplot(aes(in_bag)) +\n    geom_histogram(binwidth = 0.5, color = \"white\") +\n    labs(title = \"Unnormalized Conditional Distribution\",\n         subtitle = \"Number of white marbles in bag given that three were selected in the sample\",\n         x = \"Number of White Marbles in the Bag\",\n         y = \"Count\") +\n    coord_cartesian(xlim = c(0, 2)) +\n    scale_x_continuous(breaks = c(0, 1, 2)) +\n    theme_classic()\n\n\n\n\n\n\n\nStep 4: Plot the normalize posterior distribution. Next, let’s normalize the distribution.\n\njd_marbles |&gt; \n  filter(in_sample == 3) |&gt; \n  ggplot(aes(in_bag)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))), \n                   binwidth = 0.5, \n                   color = \"white\") +\n    labs(title = \"Posterior Probability Distribution\",\n         subtitle = \"Number of white marbles in bag given that three were selected in the sample\",\n         x = \"Number of White Marbles in the Bag\",\n         y = \"Probability\") +\n    coord_cartesian(xlim = c(0, 2)) +\n    scale_x_continuous(breaks = c(0, 1, 2)) +\n    scale_y_continuous(labels = \n                         scales::percent_format(accuracy = 1)) +\n    theme_classic()\n\n\n\n\n\n\n\nThis plot makes sense because when all three marbles you draw out of the bag are white, there is a pretty good chance that there are no black marbles in the bag. But you can’t be certain! It is possible to draw three white even if the bag contains one white and one black. However, it is impossible that there are zero white marbles in the bag.\nLastly let’s answer the question: What is the chance of the bag contains exactly two white marbles, given that when we selected the white marbles three times, everytime we select a white marble?\nAnswer: As the Posterior Probability Distribution shows (x-axis=2), the chance of the bag contains exactly two white marbles given that we select 3 white marbles out of three tries is about 85%.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "01-probability.html#sec-n-models",
    "href": "01-probability.html#sec-n-models",
    "title": "1  Probability",
    "section": "\n1.5 N models",
    "text": "1.5 N models\n\n\n\n\n\n\n\n\n\n\n\nAssume that there is a coin with \\(\\rho_h\\). We guarantee that there are only 11 possible values of \\(\\rho_h\\): \\(0, 0.1, 0.2, ..., 0.9, 1\\). In other words, there are 11 possible models, 11 things which might be true about the world. This is just like situations we have previously discussed, except that there are more models to consider.\nWe are going to run an experiment in which you flip the coin 20 times and record the number of heads. What does this result tell you about the value of \\(\\rho_h\\)? Ultimately, we will want to calculate a posterior distribution of \\(\\rho_h\\), which is written as p(\\(\\rho_h\\)).\nQuestion: What is the probability of getting exactly 8 heads out of 20 tosses?\nTo start, it is useful to consider all the things which might happen if, for example, \\(\\rho_h = 0.4\\). Fortunately, the R functions for simulating random variables makes this easy.\n\nset.seed(9)\n\nsims &lt;- 1000\n\nx &lt;- tibble(ID = 1: sims, heads = rbinom(n = 1000, size = 20, p = 0.4)) \n\nx |&gt; \n  ggplot(aes(heads)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 50) + \n    labs(title = \"Empirical Distribution of Number of Heads\",\n         subtitle = \"Based on 1,000 simulations with p = 0.4\",\n         x = \"Number of Heads out of 20 Tosses\",\n         y = \"Probability\") +\n    scale_y_continuous(labels = \n                         scales::percent_format(accuracy = 1)) +\n  theme_classic()\n\n\n\n\n\n\n\nFirst, notice that many different things can happen! Even if we know, for certain, that \\(\\rho_h = 0.4\\), many outcomes are possible. Life is remarkably random. Second, the most likely result of the experiment is 8 heads, as we would expect. Third, we have transformed the raw counts of how many times each total appeared into a probability distribution. Sometimes, however, it is convenient to just keep track of the raw counts. The shape of the figure is the same in both cases.\n\nx |&gt;\n  ggplot(aes(heads)) +\n     geom_histogram(bins = 50) + \n    labs(title = \"Total Count of the Number of Heads Out of 20 Tosses\",\n         subtitle = \"Based on 1,000 simulations with p = 0.4\",\n         x = \"Number of Heads out of 20 Tosses\",\n         y = \"Count\") +\n  theme_classic()\n\n\n\n\n\n\n\nEither way, the figures show what would have happened if that model — that \\(\\rho_h = 0.4\\) — were true.\nWe can do the same thing for all 11 possible models, calculating what would happen if each of them were true. This is somewhat counterfactual since only one of them can be true. Yet this assumption does allow us to create the joint distribution of models which might be true and of data which our experiment might generate. Let’s simplify this as p(models, data), although you should keep the precise meaning in mind.\n\n# sims size depends on your data size, or the number in rep().\n\nset.seed(10)\n\nsims &lt;- 11000\n\njd_coin &lt;- tibble(ID = 1:sims, p = rep(seq(0, 1, 0.1), 1000)) |&gt;\n  mutate(heads = map_int(p, ~ rbinom(n = 1, size = 20, p = .)))\n\njd_coin |&gt;\n  ggplot(aes(y = p, x = heads)) +\n    geom_jitter(alpha = 0.1) +\n    labs(title = \"Empirical Distribution of Number of Heads\",\n         subtitle = expression(paste(\"Based on simulations with various values of \", rho[h])),\n         x = \"Number of Heads out of 20 Tosses\",\n         y = expression(rho[h])) +\n  scale_y_continuous(breaks = seq(0, 1, 0.1)) +\n  theme_classic()\n\n\n\n\n\n\n\nHere is the 3D version of the same plot.\n\n# Condese point and make rayshader plot.\n\njd_coin_plot &lt;- jd_coin |&gt; \n  summarize(total = n(), \n            .by = c(p, heads)) |&gt; \n  mutate(p = as.factor(p)) |&gt; \n  mutate(heads = as.factor(heads)) |&gt; \n  ggplot() +\n    geom_point(aes(x = heads, y = p, color = total)) +\n    scale_color_continuous(limits = c(0, 1000)) +\n    theme(legend.position = \"none\") +\n    labs(x = \"Number of Heads out of 20 Tosses\",\n         y = expression(rho[h]),\n         title = \"Empirical Distribution of Number of Heads\",\n         subtitle = expression(paste(\"Based on simulations with various values of \", \n                                     rho[h]))) +\n    theme(title = element_text(size = 9),\n          axis.text.x = element_text(size = 7),\n          axis.title.y = element_text(size = 7),\n          legend.position = \"none\")\n\nplot_gg(jd_coin_plot,\n       width = 3.5,\n         zoom = 0.65,\n         theta = 25,\n         phi = 30,\n         sunangle = 225,\n         soliddepth = -0.5,\n         raytrace = FALSE,\n         windowsize = c(2048,1536))\n\n# Save the plot as an image\nrgl::rgl.snapshot(\"probability/images/coin.png\")\n\n# Close the RGL device\nrgl::close3d()\n\n\n\n\n\n\n\n\n\nIn both of these diagrams, we see 11 models and 21 outcomes. We don’t really care about the p(\\(models\\), \\(data\\)), the joint distribution of the models-which-might-be-true and the data-which-our-experiment-might-generate. Instead, we want to estimate \\(p\\), the unknown parameter which determines the probability that this coin will come up heads when tossed. The joint distribution alone can’t tell us that. We created the joint distribution before we had even conducted the experiment. It is our creation, a tool which we use to make inferences. Instead, we want the conditional distribution, p(\\(models\\) | \\(data = 8\\)). We have the results of the experiment. What do those results tell us about the probability distribution of \\(p\\)?\nTo answer this question, we simply take a vertical slice from the joint distribution at the point of the x-axis corresponding to the results of the experiment.\nThis animation shows what we want to do with joint distributions. We take a slice (the red one), isolate it, rotate it to look at the conditional distribution, normalize it (change the values along the current z-axis from counts to probabilities), then observe the resulting posterior.\n\nThis is the only part of the joint distribution that we care about. We aren’t interested in what the object looks like where, for example, the number of heads is 11. That portion is irrelevant because we observed 8 heads, not 11. By using the filter function on the simulation tibble we created, we can conclude that there are a total of 465 times in our simulation in which 8 heads were observed.\nAs we would expect, most of the time when 8 coin tosses came up heads, the value of \\(p\\) was 0.4. But, on numerous occasions, it was not. It is quite common for a value of \\(p\\) like 0.3 or 0.5 to generate 8 heads. Consider:\n\njd_coin |&gt; \n  filter(heads == 8) |&gt; \n  ggplot(aes(p)) +\n    geom_bar() +\n    labs(title = expression(paste(\"Values of \", rho[h], \" Associated with 8 Heads\")),\n         x = expression(paste(\"Assumed value of \", rho[h], \" in simulation\")),\n         y = \"Count\") +\n  theme_classic()\n\n\n\n\n\n\n\nYet this is a distribution of raw counts. It is an unnormalized density. To turn it into a proper probability density (i.e., one in which the sum of the probabilities across possible outcomes sums to one) we just divide everything by the total number of observations.\n\njd_coin |&gt; \n  filter(heads == 8) |&gt; \n  ggplot(aes(x = p)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 50) + \n    labs(title = expression(paste(\"Posterior Probability Distribution of \", rho[h])),\n         x = expression(paste(\"Possible values for \", rho[h])),\n         y = \"Probability\") +\n    scale_x_continuous(breaks = seq(0.2, 0.7, by = 0.1)) +\n    scale_y_continuous(labels = \n                        scales::percent_format(accuracy = 1)) +\n  theme_classic()\n\n\n\n\n\n\n\nSolution:\nThe most likely value of \\(\\rho_h\\) is 0.4, as before. But, it is much more likely that \\(p\\) is either 0.3 or 0.5. And there is about an 8% chance that \\(\\rho_h \\ge 0.6\\).\nYou might be wondering: what is the use of a model? Well, let’s say we toss the coin 20 times and get 8 heads again. Given this result, we can ask: What is the probability that future samples of 20 flips will result in 10 or more heads?\nThere are three main ways you could go about solving this problem with simulations.\nThe first wrong way to do this is assuming that \\(\\rho_h\\) is certain because we observed 8 heads after 20 tosses. We would conclude that 8/20 gives us 0.4. The big problem with this is that you are ignoring your uncertainty when estimating \\(\\rho_h\\). This would lead us to the following code.\n\nsims &lt;- 10000000\n\nodds &lt;- tibble(sim_ID = 1:sims) |&gt;\n  mutate(heads = map_int(sim_ID, ~ rbinom(n = 1, size = 20, p = .4))) |&gt; \n  mutate(above_ten = if_else(heads &gt;= 10, TRUE, FALSE))\n\nodds\n\n# A tibble: 10,000,000 × 3\n   sim_ID heads above_ten\n    &lt;int&gt; &lt;int&gt; &lt;lgl&gt;    \n 1      1    10 TRUE     \n 2      2     5 FALSE    \n 3      3     2 FALSE    \n 4      4    10 TRUE     \n 5      5     5 FALSE    \n 6      6    10 TRUE     \n 7      7     7 FALSE    \n 8      8    11 TRUE     \n 9      9     9 FALSE    \n10     10     9 FALSE    \n# ℹ 9,999,990 more rows\n\n\n\nodds |&gt;\n  ggplot(aes(x=heads,fill=above_ten))+\n           geom_histogram(aes(y = after_stat(count/sum(count))),bins = 50)+\n  scale_fill_manual(values = c('grey50', 'red'))+\n  labs(title = \"Posterior Probability Distribution (Wrong Way)\",\n         subtitle = \"Number of heads in 20 tosses\",\n         x = \"Number of heads\",\n         y = \"Probability\",\n         fill = \"Above ten heads\") + \n    scale_x_continuous(labels = scales::number_format(accuracy = 1)) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()\n\n\n\n\n\n\n\nUsing this Posterior distribution derived from the (wrong way) simulated data, the probability results with 10 or more heads is\n\nodds |&gt;\n  summarize(success = sum(above_ten)/sims)\n\n# A tibble: 1 × 1\n  success\n    &lt;dbl&gt;\n1   0.245\n\n\nabout 24.5%.\nThe second method involves sampling the whole posterior distribution vector we previously created. This would lead to the following correct code.\n\np_draws &lt;- tibble(p = rep(seq(0, 1, 0.1), 1000)) |&gt;\n  mutate(heads = map_int(p, ~ rbinom(n = 1, size = 20, p = .))) |&gt;\n  filter(heads == 8)\n  \nodds_2nd &lt;- tibble(p = sample(p_draws$p, size = sims, replace = TRUE)) |&gt;\n  mutate(heads = map_int(p, ~ rbinom(n = 1, size = 20, p = .))) |&gt; \n  mutate(above_ten = if_else(heads &gt;= 10, TRUE, FALSE)) \n\nodds_2nd\n\n# A tibble: 10,000,000 × 3\n       p heads above_ten\n   &lt;dbl&gt; &lt;int&gt; &lt;lgl&gt;    \n 1   0.4     7 FALSE    \n 2   0.3     8 FALSE    \n 3   0.5    13 TRUE     \n 4   0.4    10 TRUE     \n 5   0.4     6 FALSE    \n 6   0.5     8 FALSE    \n 7   0.5     9 FALSE    \n 8   0.5    12 TRUE     \n 9   0.5    10 TRUE     \n10   0.4     8 FALSE    \n# ℹ 9,999,990 more rows\n\n\n\nodds_2nd |&gt;\n  ggplot(aes(x = heads,fill = above_ten))+\n           geom_histogram(aes(y = after_stat(count/sum(count))),bins = 50)+\n  scale_fill_manual(values = c('grey50', 'red'))+\n  labs(title = \"Posterior Probability Distribution (Right Way)\",\n         subtitle = \"Number of heads in 20 tosses\",\n         x = \"Number of heads\",\n         y = \"Probability\",\n         fill = \"Above ten heads\") + \n    scale_x_continuous(labels = scales::number_format(accuracy = 1)) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()\n\n\n\n\n\n\n\nUsing this Posterior distribution derived from the (right way 1st) simulated data, the probability results in 10 or more head is\n\nodds_2nd |&gt;\n  summarize(success = sum(above_ten)/sims)\n\n# A tibble: 1 × 1\n  success\n    &lt;dbl&gt;\n1   0.351\n\n\nabout 32.8%\nAs you may have noticed, if you calculated the value using the first method, you would believe that getting 10 or more heads is less likely than it really is. If you were to run a casino based on these assumptions, you will lose all your money. It is very important to be careful about the assumptions you are making. We tossed a coin 20 times and got 8 heads. However, you would be wrong to assume that \\(\\rho_h\\) = 0.4 just based on this result.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "02-sampling.html",
    "href": "02-sampling.html",
    "title": "2  Sampling",
    "section": "",
    "text": "2.1 Real sampling activity\n“The most important aspect of a statistical analysis is not what you do with the data, it’s what data you use.” – Hal Stern\nThe Hunger Games is a dystopian novel in which children are chosen via lottery to fight to the death. Primrose Everdeen is selected from the urn. Why does she have the misfortune of being selected? Or, as we data scientists say, sampled?\nIn Chapter 1, we learned about probability, the framework for quantifying uncertainty. In this chapter, we will learn about sampling, the beginning of our journey toward inference. When we sample, we take some units from a population. With the data collected via sampling, we use inference to create statistical models. With such models, we can answer questions.\nWe always use the Cardinal Virtues. Wisdom helps us to clarify the questions with which we begin. We build the Preceptor Table which, if no data were missing, would allow us to answer the question. We check for validity. Justice creates the Population Table and examines the assumptions of stability, representativeness, and unconfoundedness. With Courage, we create a data generating mechanism. Temperance helps us to use that DGM to answer the question with which we began.\nThe urn below has a certain number of red and a certain number of white beads all of equal size, mixed well together. What proportion, \\(\\rho\\), of this urn’s beads are red?\nAn urn with red and white beads.\nOne way to answer this question would be to perform an exhaustive count: remove each bead, count the number of red beads, count the number of white beads, and divide the number of red beads by the total number of beads. Call that ratio \\(\\rho\\), the proportion of red beads in the urn. However, this would be a long and tedious process. Therefore, we will use sampling!\nTo begin this chapter, we will look at a real sampling activity: the urn. Then, we will simulate the urn example using R code. This will help us to understand the standard error and the ways in which uncertainty factors into our predictions.\nUse the tidyverse package.\nlibrary(tidyverse)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling</span>"
    ]
  },
  {
    "objectID": "02-sampling.html#sec-sampling-activity",
    "href": "02-sampling.html#sec-sampling-activity",
    "title": "2  Sampling",
    "section": "",
    "text": "2.1.1 Using the shovel method once\nInstead of performing an exhaustive count, let’s insert a shovel into the urn and remove \\(5 \\cdot 10 = 50\\) beads. We are taking a sample of the total population of beads.\n\n\n\n\nInserting a shovel into the urn.\n\n\n\n\n\n\n\nRemoving 50 beads from the urn.\n\n\n\nObserve that 17 of the 50 sampled beads are red and thus \\(17/50 = 0.34 = 34\\%\\) of the shovel’s beads are red. We can view the proportion of beads that are red in this shovel as a guess at the proportion of beads that are red in the entire urn. While not as exact as doing an exhaustive count of all the beads in the urn, our guess of 34% took much less time and energy to make.\nRecall that \\(\\rho\\) is the true value of the proportion of red beads. There is only one \\(\\rho\\). A guess at the proportion of red beads is known as \\(\\hat{\\rho}\\) (pronounced p hat), where \\(\\hat{\\rho}\\) is the estimated value of \\(\\rho\\). There are many ways to estimate \\(\\rho\\), each leading to a (potentially) different \\(\\hat{\\rho}\\). The 34% value for \\(\\hat{\\rho}\\) came from taking this sample. But, if we used the shovel again, we would probably come up with a different \\(\\hat{\\rho}\\). There are many possible \\(\\hat{\\rho}\\)’s. You and I will often differ in our estimates. We might each have a different \\(\\hat{\\rho}\\) even though we agree that there is only one \\(\\rho\\).\nStart this activity over again from the beginning, placing the 50 beads back into the urn. Would a second sample include exactly 17 red beads? Maybe, but probably not.\nWhat if we repeated this activity many times? Would our guess at the proportion of the urn’s beads that are red, \\(\\hat{\\rho}\\), be exactly 34% every time? Surely not.\nLet’s repeat this exercise with the help of 33 groups of friends to understand how the value of \\(\\hat{\\rho}\\) varies across 33 independent trials.\n\n2.1.2 Using the shovel 33 times\nEach of our 33 groups of friends will do the following:\n\nUse the shovel to remove 50 beads each.\nCount the number of red beads and compute the proportion of the 50 beads that are red.\nReturn the beads into the urn.\nMix the contents of the urn to not let a previous group’s results influence the next group’s.\n\nEach of our 33 groups of friends make note of their proportion of red beads from their sample collected. Each group then marks their proportion of their 50 beads that were red in the appropriate bin in a hand-drawn histogram as seen below.\n\n\n\n\nConstructing a histogram of proportions.\n\n\n\nHistograms allow us to visualize the distribution of a numerical variable. In particular, where the center of the values falls and how the values vary. A partially completed histogram of the first 10 out of 33 groups of friends’ results can be seen in the figure below.\n\n\n\n\nHand-drawn histogram of first 10 out of 33 proportions.\n\n\n\nObserve the following details in the histogram:\n\nAt the low end, one group removed 50 beads from the urn with proportion red between 0.20 and 0.25.\nAt the high end, another group removed 50 beads from the urn with proportion red between 0.45 and 0.5 red.\nHowever, the most commonly occurring proportions were around 0.4 red, right in the middle of the distribution.\nThe distribution is somewhat bell-shaped.\n\ntactile_sample_urn saves the results from our 33 groups of friends.\n\n\n# A tibble: 33 × 4\n   group         red_beads prop_red group_ID\n   &lt;chr&gt;             &lt;dbl&gt;    &lt;dbl&gt;    &lt;int&gt;\n 1 Sophie, Brian        21     0.42        1\n 2 Maeve, Josh          18     0.36        2\n 3 Mal, Francis         17     0.34        3\n 4 Frank, Clara         21     0.42        4\n 5 Mia, James           15     0.3         5\n 6 Yuki, Harry          21     0.42        6\n 7 Claire, Cloud        16     0.32        7\n 8 Yao, Kate            23     0.46        8\n 9 Mark, Ramses         21     0.42        9\n10 Aaron, Mike          11     0.22       10\n# ℹ 23 more rows\n\n\nFor each group, we are given their names, the number of red_beads they obtained, and the corresponding proportion out of 50 beads that were red, called prop_red. We also have a group_ID variable which gives each of the 33 groups a unique identifier. Each row can be viewed as one instance of a replicated activity: using the shovel to remove 50 beads and computing the proportion of those beads that were red.\nLet’s visualize the distribution of these 33 proportions using geom_histogram() with binwidth = 0.05. This is a computerized and complete version of the partially completed hand-drawn histogram you saw earlier.\n\ntactile_sample_urn |&gt;\n  ggplot(aes(x = prop_red)) +\n  \n  # Setting `boundary = 0.4` indicates that we want a binning scheme such that\n  # one of the bins' boundary is at 0.4. `color = \"white\"` modifies the color of\n  # the boundary for visual clarity.\n  \n  geom_histogram(binwidth = 0.05, \n                 boundary = 0.4, \n                 color = \"white\") +\n  \n  # Add scale_y_continuous with breaks by 1, as the default shows the y-axis\n  # from 1 to 10 with breaks at .5. Breaks by 1 is better for this plot, as all\n  # resulting values are integers.\n  \n  scale_y_continuous(breaks = seq(from = 0, to = 10, by = 1)) +\n  \n  # The call expression() is used to insert a mathematical expression, like\n  # p-hat. The paste after expression allows us to paste text prior to said\n  # expression.\n  \n  labs(x = expression(paste(\"Proportion, \", hat(rho), \", of 50 beads that were red\")),\n       y = \"Count\",\n       title = \"Proportions Red in 33 Samples\") \n\n\n\n\n\n\n\n\n2.1.3 What did we just do?\nWhat we just demonstrated in this activity is the statistical concept of sampling. We want to know the proportion of red beads in the urn, with the urn being our population. Performing an exhaustive count of the red and white beads would be too time consuming. Therefore, it is much more practical to extract a sample of 50 beads using the shovel. Using this sample of 50 beads, we estimated the proportion of the urn’s beads that are red to be about 34%.\nMoreover, because we mixed the beads before each use of the shovel, the samples were random and independent. Because each sample was drawn at random, the samples were different from each other. This is an example of sampling variation. For example, what if instead of selecting 17 beads in our first sample we had selected just 11? Does that mean that the population proportion of the beads is 11/50 or 22%? No! Because we performed 33 trials we can look to our histogram, and see that the peak of the distribution occurs when \\(.35 &lt; \\hat{\\rho} &lt; .4\\) , so it is likely that the proportion of red beads in the entire population will also fall in or near this range.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling</span>"
    ]
  },
  {
    "objectID": "02-sampling.html#sec-virtual-sampling",
    "href": "02-sampling.html#sec-virtual-sampling",
    "title": "2  Sampling",
    "section": "\n2.2 Virtual sampling",
    "text": "2.2 Virtual sampling\nWe just performed a tactile sampling activity. We used a physical urn of beads and a physical shovel. We did this by hand so that we could develop our intuition about the ideas behind sampling. In this section, we mimic this physical sampling with virtual sampling, using a computer.\n\n2.2.1 Using the virtual shovel once\nVirtual sampling requires a virtual urn and a virtual shovel. Create a tibble named urn. The rows of urn correspond exactly to the contents of the actual urn.\n\n# set.seed() ensures that the beads in our virtual urn are always in the same\n# order. This ensures that the figures in the book match their written\n# descriptions. We want 40% of the beads to be red.\n\nset.seed(8)\n\nurn &lt;- tibble(color = c(rep(\"red\", 400), \n                        rep(\"white\", 600))) |&gt;\n  \n  # sample_frac() keeps all the rows in the tibble but rearranges their order.\n  # We don't need to do this. A virtual urn does not care about the order of the\n  # beads. But we find it aesthetically pleasing to mix them up.\n  \n  sample_frac() |&gt; \n  mutate(bead_ID = 1:1000) |&gt; \n  select(bead_ID, color)\n\nurn  \n\n# A tibble: 1,000 × 2\n   bead_ID color\n     &lt;int&gt; &lt;chr&gt;\n 1       1 white\n 2       2 red  \n 3       3 red  \n 4       4 white\n 5       5 white\n 6       6 red  \n 7       7 white\n 8       8 white\n 9       9 red  \n10      10 red  \n# ℹ 990 more rows\n\n\nObserve that urn has 1,000 rows, meaning that the urn contains 1,000 beads. The first variable bead_ID is used as an identification variable. None of the beads in the actual urn are marked with numbers. The second variable color indicates whether a particular virtual bead is red or white.\nOur virtual urn needs a virtual shovel. We use slice_sample() and some list-column mapping wizardry to take a sample of 50 beads from our virtual urn.\n\n# Define trial_ID as one instance of us sampling 50 beads from the urn. When\n# trial_ID is called within map(), we are performing slice_sample() upon our urn\n# once, and taking a sample of 50 beads. \n\ntibble(trial_ID = 1) |&gt; \n  mutate(shovel = map(trial_ID, ~ slice_sample(urn, n = 50)))\n\n# A tibble: 1 × 2\n  trial_ID shovel           \n     &lt;dbl&gt; &lt;list&gt;           \n1        1 &lt;tibble [50 × 2]&gt;\n\n\nAs usual, map functions and list-columns are powerful but confusing. The str() function is a good way to explore a tibble with a list-column.\n\ntibble(trial_ID = 1) |&gt; \n  mutate(shovel = map(trial_ID, ~ slice_sample(urn, n = 50))) |&gt; \n  str()\n\ntibble [1 × 2] (S3: tbl_df/tbl/data.frame)\n $ trial_ID: num 1\n $ shovel  :List of 1\n  ..$ : tibble [50 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ bead_ID: int [1:50] 391 171 24 703 399 404 134 145 272 487 ...\n  .. ..$ color  : chr [1:50] \"white\" \"white\" \"white\" \"white\" ...\n\n\nThere are two levels. There is one row in the tibble for each sample. So far, we have only drawn one sample. Within each row, there is a second level, the tibble which is the sample. That tibble has two variables: trial_ID and color. This is the advantage to using slice_sample(), because it selects all columns of our urn, whereas sample() can only sample from a single column. While identifying each individual bead may be irrelevant in our urn scenario, with other problems it could be very useful to have additional data about each individual.\nNow let’s add a column which indicates the number of red beads in the sample taken from the shovel.\n\n\ntibble(trial_ID = 1) |&gt; \n  mutate(shovel = map(trial_ID, ~ slice_sample(urn, n = 50))) |&gt; \n  \n  # To count the number of red beads in each shovel, we can use a lesser \n  # known property of the sum() function: By passing in a comparison \n  # expression, sum() will count the number of occurrences within a vector. \n  # In this case, we count the total number occurrences of the word red\n  # in the color column of shovel.\n\n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\")))\n\n# A tibble: 1 × 3\n  trial_ID shovel            numb_red\n     &lt;dbl&gt; &lt;list&gt;               &lt;int&gt;\n1        1 &lt;tibble [50 × 2]&gt;       17\n\n\nHow does this work? R evaluates if color == red, and treats TRUE values like the number 1 and FALSE values like the number 0. So summing the number of TRUEs and FALSEs is equivalent to summing 1’s and 0’s. In the end, this operation counts the number of beads where color equals “red”.\nFinally, calculate the proportion red by dividing numb_red (the number of red beads observed in the shovel) by the shovel size (which is 50 in this example).\n\ntibble(trial_ID = 1) |&gt; \n  mutate(shovel = map(trial_ID, ~ slice_sample(urn, n = 50))) |&gt; \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) |&gt; \n  mutate(prop_red = numb_red / 50)\n\n# A tibble: 1 × 4\n  trial_ID shovel            numb_red prop_red\n     &lt;dbl&gt; &lt;list&gt;               &lt;int&gt;    &lt;dbl&gt;\n1        1 &lt;tibble [50 × 2]&gt;       17     0.34\n\n\nCareful readers will note that the numb_red is changing in each example above. The reason, of course, is that each block re-runs the shovel exercise, and slice_sample() will return a random number of red beads. If we wanted the same number in each block, we would need to use the set.seed() function and then set the same seed each time.\nLet’s now perform the virtual analog of having 33 groups of students use the sampling shovel.\n\n2.2.2 Using the virtual shovel 33 times\nIn our tactile sampling exercise in Section 2.1, we had 33 groups of students use the shovel, yielding 33 samples of size 50 beads. We then used these 33 samples to compute 33 proportions.\nLet’s use our virtual sampling to replicate the tactile sampling activity in a virtual format. We’ll save these results in a data frame called virtual_samples.\n\nset.seed(13)\n\n virtual_samples &lt;- tibble(trial_ID = 1:33) |&gt;\n    mutate(shovel = map(trial_ID, ~ slice_sample(urn, n = 50))) |&gt; \n    mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) |&gt;\n    mutate(prop_red = numb_red / 50) \n\nvirtual_samples\n\n# A tibble: 33 × 4\n   trial_ID shovel            numb_red prop_red\n      &lt;int&gt; &lt;list&gt;               &lt;int&gt;    &lt;dbl&gt;\n 1        1 &lt;tibble [50 × 2]&gt;       21     0.42\n 2        2 &lt;tibble [50 × 2]&gt;       21     0.42\n 3        3 &lt;tibble [50 × 2]&gt;       20     0.4 \n 4        4 &lt;tibble [50 × 2]&gt;       17     0.34\n 5        5 &lt;tibble [50 × 2]&gt;       20     0.4 \n 6        6 &lt;tibble [50 × 2]&gt;       18     0.36\n 7        7 &lt;tibble [50 × 2]&gt;       24     0.48\n 8        8 &lt;tibble [50 × 2]&gt;       21     0.42\n 9        9 &lt;tibble [50 × 2]&gt;       17     0.34\n10       10 &lt;tibble [50 × 2]&gt;       21     0.42\n# ℹ 23 more rows\n\n\nLet’s visualize this variation in a histogram:\n\nvirtual_samples |&gt; \nggplot(aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.05, \n                 boundary = 0.4, \n                 color = \"white\") +\n  \n  # To use mathematical symbols in titles and labels, use the expression()\n  # function, as here.\n  \n  labs(x = expression(paste(\"Proportion, \", hat(rho), \", of 50 beads that were red\")),\n       y = \"Count\",\n       title = \"Distribution of 33 proportions red\") +\n  \n  # Label the y-axis in an attractive fashion. Without this code, the axis\n  # labels would include 2.5, which makes no sense because all the values are\n  # integers.\n  \n  scale_y_continuous(breaks = seq(2, 10, 2))\n\n\n\n\n\n\n\nSince binwidth = 0.05, this will create bins with boundaries at 0.30, 0.35, 0.45, and so on. Recall that \\(\\hat{\\rho}\\) is equal to the proportion of beads which are red in each sample.\nObserve that we occasionally obtained proportions red that are less than 30%. On the other hand, we occasionally obtained proportions that are greater than 45%. However, the most frequently occurring proportions were between 35% and 45%. Why do we have these differences in proportions red? Because of sampling variation.\nNow we will compare our virtual results with our tactile results from the previous section. Observe that both histograms are somewhat similar in their center and variation, although not identical. These slight differences are again due to random sampling variation. Furthermore, observe that both distributions are somewhat bell-shaped.\n\n\n\n\nComparing 33 virtual and 33 tactile proportions red. Note that, though the figures differ slightly, both are centered around .35 to .45. This shows that, in both sampling distributions, the most frequently occuring proportion red is between 35% and 45%.\n\n\n\nThis visualization allows us to see how our results differed between our tactile and virtual urn results. As we can see, there is some variation between our results. This is not a cause for concern, as there is always sampling variation across results.\n\n2.2.3 Using the virtual shovel 10,000 times\n\n\n\n\nSo much sampling, so little time.\n\n\n\nAlthough we took 33 samples from the urn in the previous section, we should not use so few samples. Instead, in this section we’ll examine the effects of sampling from the urn 10,000 times.\nWe can reuse our code from above, making sure to replace 33 trials with 10,000.\n\nset.seed(9)\n\n virtual_samples &lt;- tibble(trial_ID = 1:10000) |&gt;\n    mutate(shovel = map(trial_ID, ~ slice_sample(urn, n = 50))) |&gt; \n    mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) |&gt;\n    mutate(numb_beads = map_int(shovel, ~ length(.$color))) |&gt; \n    mutate(prop_red = numb_red / numb_beads) \n\nNow we have 10,000 values for prop_red, each representing the proportion of 50 beads that are red in a sample. Using the same code as earlier, let’s visualize the distribution of these 10,000 replicates of prop_red in a histogram:\n\nvirtual_samples |&gt; \nggplot(aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.01, \n                 boundary = 0.4, \n                 color = \"white\") +\n  labs(x = expression(hat(rho)),\n       y = \"Count\",\n       title = \"Distribution of 10,000 proportions red\") \n\n\n\n\n\n\n\nWhy the empty spaces among the bars? Recall that, with only 50 beads, there are only 51 possible values for \\(\\hat{\\rho}\\): 0, 0.02, 0.04, …, 0.98, 1. A value of 0.31 or 0.47 is impossible, hence the gaps.\nThe most frequently occurring proportions of red beads occur, again, between 35% and 45%. Every now and then we observe proportions much higher or lower. This occurs because as we increase the number of trials, tails develop on our distribution as we are more likely to witness extreme \\(\\hat{\\rho}\\) values. The symmetric, bell-shaped distribution shown in the histogram is well approximated by the normal distribution.\nNow that we have a good understanding of virtual sampling, we can apply our knowledge to examine the effects of changing our virtual shovel size.\n\n2.2.4 The effect of different shovel sizes\n\n\n\n\nWhat happens if we use different sized shovels to sample?\n\n\n\nInstead of just one shovel, imagine we have three choices of shovels with which to extract a sample of beads: shovels of size 25, 50, and 100. Using our newly developed tools for virtual sampling, let’s unpack the effect of having different sample sizes. Start by creating a tibble with 10,000 rows, each row representing an instance of us sampling from the urn with our chosen shovel size. Then, compute the resulting 10,000 replicates of proportion red. Finally, plot the distribution using a histogram.\n\n# Within slice_sample(), n = 25 represents our shovel of size 25. We also divide\n# by 25 to compute the proportion red.\n\nvirtual_samples_25 &lt;- tibble(trial_ID = 1:10000) |&gt; \n  mutate(shovel = map(trial_ID, ~ slice_sample(urn, n = 25))) |&gt; \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) |&gt; \n  mutate(prop_red = numb_red / 25)\n\nvirtual_samples_25 |&gt;\n  ggplot(aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.04, \n                 boundary = 0.4, \n                 color = \"white\") +\n  labs(x = expression(paste(\"Proportion, \", hat(rho), \", of 25 beads that were red\")), \n       title = \"25\") \n\n\n\n\n\n\n\nWe will repeat this process with a shovel size of 50.\n\nvirtual_samples_50 &lt;- tibble(trial_ID = 1:10000) |&gt; \n  mutate(shovel = map(trial_ID, ~ slice_sample(urn, n = 50))) |&gt; \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) |&gt; \n  mutate(prop_red = numb_red / 50)\n\n\nvirtual_samples_50  |&gt;\n  ggplot(aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.04, \n                 boundary = 0.4, \n                 color = \"white\") +\n  labs(x = expression(paste(\"Proportion, \", hat(rho), \", of 50 beads that were red\")), \n       title = \"50\")  \n\n\n\n\n\n\n\nWe choose a bin width of .04 for all histograms to more easily compare different shovel sizes. Using a smaller bin size would result in gaps between the bars, as a shovel of size 50 has more possible \\(\\hat{\\rho}\\) values than a shovel of size 25.\nFinally, we will perform the same process with 10,000 replicates to map the histogram using a shovel size of 100.\n\nvirtual_samples_100 &lt;- tibble(trial_ID = 1:10000) |&gt; \n  mutate(shovel = map(trial_ID, ~ slice_sample(urn, n = 100))) |&gt; \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) |&gt; \n  mutate(prop_red = numb_red / 100)\n\n\nvirtual_samples_100 |&gt;\n  ggplot(aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.04, \n                 boundary = 0.4, \n                 color = \"white\") +\n  labs(x = expression(paste(\"Proportion, \", hat(rho), \", of 100 beads that were red\")), \n       title = \"100\") \n\n\n\n\n\n\n\nFor easy comparison, we present the three resulting histograms in a single row with matching x and y axes:\n\n# Use bind_rows to combine the data from our three saved virtual sampling\n# objects. Use mutate() in each to clarify the n as the necessary number\n# of samples taken. This makes our data easier to interpret and prevents\n# duplicate elements.\n\nvirtual_prop &lt;- bind_rows(virtual_samples_25 |&gt; \n                            mutate(n = 25), \n                          virtual_samples_50 |&gt; \n                            mutate(n = 50), \n                          virtual_samples_100 |&gt; \n                            mutate(n = 100))\n\n# Plot our new object with the x-axis showing prop_red. Add elements binwidth,\n# boundary, and color for stylistic clarity. Use labs() to add an x-axis label\n# and title. Facet_wrap() splits the graph into multiple plots by the variable\n# (~n).\n\ncomparing_sampling_distributions &lt;- ggplot(virtual_prop, aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.04, boundary = 0.4, color = \"white\") +\n  labs(x = expression(paste(\"Proportion, \", hat(rho), \", of the beads that were red\")), \n       title = \"Comparing distributions of proportions red for three different shovel sizes.\") +\n  facet_wrap(~ n) \n\n# Inspect our new faceted graph. \n\ncomparing_sampling_distributions\n\n\n\nComparing the distributions of proportion red for different sample sizes (25, 50, 100). The important takeaway is that our center becomes more concentrated as our sample size increases, indicating a smaller standard deviation between our guesses.\n\n\n\nObserve that as the sample size increases, the histogram becomes taller and narrower. This is because the variation of the proportion red for each sample decreases. Remember: A large variation means there are a wide range of values that might occur, while smaller variations are more concentrated around the central value.\nThe Central Limit Theorem states, more or less, that when sample means are based on larger and larger sample sizes, the sampling distribution of these sample means becomes both narrower and more bell-shaped. In other words, the sampling distribution increasingly follows a normal distribution and the variation of this sampling distribution gets smaller.\nWhy does variation decrease as sample size increases? If we use a large sample size like 100 or 500, our sample is much more representative of the population. As a result, the proportion red in our sample (\\(\\hat{\\rho}\\)) will be closer to the true population proportion (\\(\\rho\\)). On the other hand, smaller samples have much more variation because of chance. We are much more likely to have extreme estimates that are not representative of our population when the sample size is small.\nLet’s attempt to visualize the concept of variation a different way. For each sample size, let’s plot the proportion red for all 10,000 samples. With 3 different shovel sizes, we will have 30,000 total points, with each point representing an instance of sampling from the urn with a specific shovel size.\n\nvirtual_prop |&gt;\n  ggplot(aes(x = n, y = prop_red, color = as.factor(n))) +\n  geom_jitter(alpha = .15) + \n  labs(title = \"Results of 10,000 samples for 3 different shovel sizes.\",\n       subtitle = \"As shovel size increases, variation decreases.\",\n       y = \"Proportion red in sample\",\n       color = \"Shovel size\") +\n  \n  # We do not need an x axis, because the color of the points denotes the shovel size. \n   \n  theme(axis.title.x = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks.x = element_blank())\n\n\n\n\n\n\n\nThis graph illustrates the exact same concept as the histogram. With the smallest shovel size, there is significant variance from sample to sample, as samples feature, by chance, some extreme results. However, as we increase the sample size, the points become more concentrated, i.e., they demonstrate less variance.\nThere is also a third way to understand variation. We can be numerically explicit about the amount of variation in our three sets of 10,000 values of prop_red by using the standard deviation. A standard deviation is a summary statistic that measures the amount of variation within a numeric vector. For all three sample sizes, let’s compute the standard deviation of the 10,000 proportions red.\n\n\n\n\n\n\n\nComparing standard deviations of proportions red for three different shovels\n\n\nNumber of slots in shovel\nStandard deviation of proportions red\n\n\n\n\n25\n0.098\n\n\n50\n0.068\n\n\n100\n0.046\n\n\n\n\nComparing the number of slots in the shovel with the standard deviation of proportions red. Here, we see that standard deviation decreases with higher sample sizes. Larger sample sizes yield more precise estimates.\n\n\n \nAs the sample size increases, the sample-to-sample variation decreases, and our guesses at the true proportion of the urn’s beads that are red get more precise. The larger the shovel, the more precise the result.\n\n\n\n\nVariance appears everywhere in data science.\n\n\n\nLet’s take a step back from all the variance. The reality is that our code needs to be better optimized, as it is bad practice to make a separate tibble for each sample size. To make comparisons easier, let’s attempt to put all 3 shovel sizes in the same tibble using mapping.\n\ntibble(trial_ID = 1:10000) |&gt;\n  mutate(shovel_ID = map(trial_ID, ~c(25, 50, 100))) |&gt;\n  unnest(shovel_ID) |&gt;\n  mutate(samples = map(shovel_ID, ~slice_sample(urn, n = .))) |&gt;\n  mutate(num_red = map_int(samples, ~sum(.$color == \"red\"))) |&gt;\n  mutate(prop_red = num_red/shovel_ID)\n\n# A tibble: 30,000 × 5\n   trial_ID shovel_ID samples            num_red prop_red\n      &lt;int&gt;     &lt;dbl&gt; &lt;list&gt;               &lt;int&gt;    &lt;dbl&gt;\n 1        1        25 &lt;tibble [25 × 2]&gt;       13     0.52\n 2        1        50 &lt;tibble [50 × 2]&gt;       27     0.54\n 3        1       100 &lt;tibble [100 × 2]&gt;      49     0.49\n 4        2        25 &lt;tibble [25 × 2]&gt;       12     0.48\n 5        2        50 &lt;tibble [50 × 2]&gt;       22     0.44\n 6        2       100 &lt;tibble [100 × 2]&gt;      33     0.33\n 7        3        25 &lt;tibble [25 × 2]&gt;       11     0.44\n 8        3        50 &lt;tibble [50 × 2]&gt;       16     0.32\n 9        3       100 &lt;tibble [100 × 2]&gt;      33     0.33\n10        4        25 &lt;tibble [25 × 2]&gt;       13     0.52\n# ℹ 29,990 more rows\n\n\nTo those of us who do not completely understand mapping, do not fret! The tidyr package provides the expand_grid() function as a neat alternative. We can use expand_grid() and a new variable, shovel_size, to create a tibble which will organize our results. Instead of using 1,000 trials, let’s use 3 to get a feel for the function.\n\nexpand_grid(trial_ID = c(1:3), shovel_size = c(25, 50, 100))\n\n# A tibble: 9 × 2\n  trial_ID shovel_size\n     &lt;int&gt;       &lt;dbl&gt;\n1        1          25\n2        1          50\n3        1         100\n4        2          25\n5        2          50\n6        2         100\n7        3          25\n8        3          50\n9        3         100\n\n\nThe above sets the stage for simulating three samples for each of three different shovel sizes. Similar code as above can be used.\n\nexpand_grid(trial_ID = c(1:3), shovel_size = c(25, 50, 100)) |&gt; \n  mutate(shovel = map(shovel_size, ~ slice_sample(urn, n = .))) |&gt; \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) |&gt; \n  mutate(prop_red = numb_red/shovel_size) \n\n# A tibble: 9 × 5\n  trial_ID shovel_size shovel             numb_red prop_red\n     &lt;int&gt;       &lt;dbl&gt; &lt;list&gt;                &lt;int&gt;    &lt;dbl&gt;\n1        1          25 &lt;tibble [25 × 2]&gt;         9     0.36\n2        1          50 &lt;tibble [50 × 2]&gt;        20     0.4 \n3        1         100 &lt;tibble [100 × 2]&gt;       46     0.46\n4        2          25 &lt;tibble [25 × 2]&gt;         7     0.28\n5        2          50 &lt;tibble [50 × 2]&gt;        12     0.24\n6        2         100 &lt;tibble [100 × 2]&gt;       35     0.35\n7        3          25 &lt;tibble [25 × 2]&gt;        14     0.56\n8        3          50 &lt;tibble [50 × 2]&gt;        16     0.32\n9        3         100 &lt;tibble [100 × 2]&gt;       34     0.34\n\n\nAgain, we changed the second line to use shovel_size rather than trial_ID as the mapping variable since we can no longer hard code the shovel size into the call to slice_sample(). Expand to 1,000 simulations for each value of shovel_size and finish with a calculation of standard deviation.\n\nexpand_grid(trial_ID = c(1:10000), shovel_size = c(25, 50, 100)) |&gt; \n  mutate(shovel = map(shovel_size, ~ slice_sample(urn, n = .))) |&gt; \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) |&gt; \n  mutate(prop_red = numb_red/shovel_size) |&gt;\n  summarize(st_dev_p_hat = sd(prop_red),\n            .by = shovel_size)\n\n# A tibble: 3 × 2\n  shovel_size st_dev_p_hat\n        &lt;dbl&gt;        &lt;dbl&gt;\n1          25       0.0964\n2          50       0.0675\n3         100       0.0465\n\n\nThis is, approximately, the same result as we saw above, but with one re-factored tibble instead of three separate ones. Use functions like expand_grid() in the future to make your code more concise.\nNow that we have this framework, there’s no need to limit ourselves to the sizes 25, 50, and 100. Why not try all integers from 1 to 100? We can use the same code, except we’ll now set shovel_size = 1:100. (We also decrease the number of replications from 10,000 to 1,000 in order to save time.)\n\nshovels_100 &lt;- expand_grid(trial_ID = c(1:1000), shovel_size = c(1:100)) |&gt; \n  mutate(shovel = map(shovel_size, ~ slice_sample(urn, n = .))) |&gt; \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) |&gt; \n  mutate(prop_red = numb_red / shovel_size) |&gt; \n  summarize(st_dev_p_hat = sd(prop_red),\n            .by = shovel_size)\n\nglimpse(shovels_100)\n\nRows: 100\nColumns: 2\n$ shovel_size  &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17…\n$ st_dev_p_hat &lt;dbl&gt; 0.49207941, 0.33817764, 0.27815410, 0.24720459, 0.2187414…\n\n\nNow, we have the standard deviation of prop_red for all shovel sizes from 1 to 100. Let’s plot that value to see how it changes as the shovel gets larger:\n\n\n\n\nComparing standard deviations of proportions red for 100 different shovels. The standard deviation decreases at the same rate as the square root of shovel size. The red line shows the standard error.\n\n\n\nThe red line here represents an important statistical concept: standard error (SE). As the shovel size increases, and thus our sample size increases, we find that the standard error decreases.\n\n\n\n\nTo any poets and philosophers confused about this: don’t worry! It won’t be on a problem set.\n\n\n\nThis is the power of running many analyses at once using map functions and list columns: before, we could tell that the standard deviation was decreasing as the shovel size increased, but when only looking at shovel sizes of 25, 50, and 100, it wasn’t clear how quickly it was decreasing.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling</span>"
    ]
  },
  {
    "objectID": "02-sampling.html#sec-standard-errors",
    "href": "02-sampling.html#sec-standard-errors",
    "title": "2  Sampling",
    "section": "\n2.3 Standard error",
    "text": "2.3 Standard error\n\n\n\n\nStandard errors are just the way old people talk about confidence intervals.\n\n\n\nStandard errors (SE) quantify the effect of sampling variation on our estimates. In other words, they quantify how much we can expect the calculated proportions of a shovel’s beads that are red to vary from one sample to another sample to another sample. As sample size increases, the standard error decreases.\nThe standard error is the standard deviation of a sample statistic such as the estimated proportion. For example, the standard error of the mean refers to the standard deviation of the distribution of sample means taken from a population.\nThe relationship between the standard error and the standard deviation is that, for a given sample size, the standard error equals the standard deviation of the data divided by the square root of the sample size. Accordingly, the standard error is inversely proportional to the square root of the sample size. The larger the sample size, the smaller the standard error.\n\n2.3.1 Terminology and notation\n\n\n\n\nLet Yoda’s wisdom dull the pain of this terminology section.\n\n\n\nA population is the set of relevant units. The population’s size is upper-case \\(N\\). In our sampling activities, the population is the collection of \\(N\\) = 1,000 identically-sized red and white beads in the urn. This is about the simplest possible population. Other examples are all the adult men in the US, all the classrooms in a school, all the wheelbarrows in Massachusetts, all the values of your blood pressure, read at five minute intervals, for your entire life. Often, the population extends over time, as with your blood pressure readings and is, therefore, more amorphous. Consider all the people who have run for governor of a US state since 1900, or all the people who will run for governor through 2050. Those are also populations.\nA population parameter is a numeric statistic about the population that is unknown, but you wish you knew. For example, when this quantity is the mean, the population parameter of interest is the population mean. This is mathematically denoted with the Greek letter \\(\\mu\\) pronounced “mu.” In our earlier sampling-from-the-urn activity, however, since we were interested in the proportion of the urn’s beads that were red, the population parameter is the population proportion of interest, denoted by \\(\\rho\\). Different academic fields often use different Greek letters to denote the same population parameter.\nA census is an exhaustive enumeration or counting of all \\(N\\) units in the population in order to compute the population parameter’s value exactly. In our sampling activity, this would correspond to counting the number of red beads out of the \\(N\\) total in the urn and then computing the red population proportion, \\(\\rho\\), exactly. When the number \\(N\\) of individuals or observations in our population is large as was the case with our urn, a census can be quite expensive in terms of time, energy, and money. A census is impossible for any population which includes the future, like our blood pressure next year or candidates for governor in 2050. There is a truth, but we could not, even in theory, determine it.\nSampling is the act of collecting a sample from the population when we can not, or do not want to, perform a census. The sample size is lower case \\(n\\), as opposed to upper case \\(N\\) for the population’s size. Typically the sample size \\(n\\) is much smaller than the population size \\(N\\). In our sampling activities, we used shovels with varying slots to extract samples of size \\(n = 1\\) through \\(n = 100\\).\nA point estimate, also known as a sample statistic, is a measure computed from a sample that estimates an unknown population parameter. In our sampling activities, recall that the unknown population parameter was the proportion of red beads and that this is denoted by \\(\\rho\\). Our point estimate is the sample proportion: the proportion of the shovel’s beads that are red. In other words, it is our guess at the proportion of the urn’s beads that are red. The point estimate of the parameter \\(\\rho\\) is \\(\\hat{\\rho}\\). The “hat” on top of the \\(\\rho\\) indicates that it is an estimate of the unknown population proportion \\(\\rho\\).\nA sample is said to be representative if it roughly looks like the population. In other words, are the sample’s characteristics a good representation of the population’s characteristics? In our sampling activity, are the samples of \\(n\\) beads extracted using our shovels representative of the urn’s \\(N = 1,000\\) beads?\nA sample is generalizable if any results based on the sample can generalize to the population. In our sampling activity, can we generalize the sample proportion from our shovels to the entire urn? Using our mathematical notation, this is akin to asking if \\(\\hat{\\rho}\\) is a “good guess” of \\(\\rho\\).\nBiased sampling occurs if certain individuals or observations in a population have a higher chance of being included in a sample than others. We say a sampling procedure is unbiased if every observation in a population had an equal chance of being sampled. Had the red beads been much smaller than the white beads, and therefore more prone to falling out of the shovel, our sample would have been biased. In our sampling activities, since we mixed all \\(N = 1,000\\) beads prior to each group’s sampling and since each of the beads had an equal chance of being sampled, our samples were unbiased.\nA sampling procedure is random if we sample randomly from the population in an unbiased fashion. With random sampling, each unit has an equal chance of being selected into the sample. This condition entails, among other things, sufficiently mixing the urn before each use of the shovel.\n\n\n\n\nFear not if you look like Spongebob after reading this section. We will re-cap right now!\n\n\n\nIn general:\n\nIf the sampling of a sample of size \\(n\\) is done at random, then\nthe sample is unbiased and representative of the population of size \\(N\\), thus\nany result based on the sample can generalize to the population, thus\nthe point estimate is a “good guess” of the unknown population parameter, thus\ninstead of performing a census, we can draw inferences about the population using sampling.\n\nSpecific to our sampling activity:\n\nIf we extract a sample of \\(n=50\\) beads at random, in other words, we mix all of the equally-sized beads before using the shovel, then\nthe contents of the shovel are an unbiased representation of the contents of the urn’s 1,000 beads, thus\nany result based on the shovel’s beads can generalize to the urn, thus\nthe sample proportion \\(\\hat{\\rho}\\) of the \\(n=50\\) beads in the shovel that are red is a “good guess” of the population proportion \\(\\rho\\) of the \\(N=1,000\\) beads that are red, thus\ninstead of manually going over all 1,000 beads in the urn, we can make inferences about the urn by using the results from the shovel.\n\n2.3.2 Statistical definitions\nFor our 1,000 repeated/replicated virtual samples of size \\(n = 25\\), \\(n = 50\\), and \\(n = 100\\) in Section 2.2, let’s display our figure showing the difference in proportions red according to different shovel sizes.\n\n\n\n\nPreviously seen three distributions of the sample proportion \\(\\hat{\\rho}\\).\n\n\n\nThese types of distributions have a special name: sampling distributions. The visualization displays the effect of sampling variation on the distribution of a point estimate: the sample proportion \\(\\hat{\\rho}\\). Using these sampling distributions, for a given sample size \\(n\\), we can make statements about what range of values we typically expect.\nFor example, observe the centers of all three sampling distributions: all around \\(0.4 = 40\\%\\). Furthermore, observe that while we are somewhat likely to observe sample proportions of red beads of \\(0.2 = 20\\%\\) when using the shovel with 25 slots, we will almost never observe a proportion of 20% when using the shovel with 100 slots. Observe also the effect of sample size on the sampling variation. As the sample size \\(n\\) increases from 25 to 50 to 100, the variation of the sampling distributions decreases and thus the values cluster more and more tightly around the same center: about 40%.\n\n\n\n\nPreviously seen comparing standard deviations of proportions red from 100 different shovel sizes\n\n\n\nAs the sample size increases, the standard deviation of the proportion of red beads across the 1,000 replications decreases. This type of standard deviation has another special name: standard error\n\n2.3.3 What is a standard error?\nThe standard error (SE) is the standard deviation of a sample statistic (aka point estimate), such as the mean or median. For example, the “standard error of the mean” refers to the standard deviation of the distribution of sample means taken from a population.\n\nIn statistics, a sample mean deviates from the actual mean of a population; this deviation is the standard error of the mean.\n\nMany students struggle to differentiate the standard error from the standard deviation. The relationship between the standard error and the standard deviation is such that, for a given sample size, the standard error equals the standard deviation divided by the square root of the sample size.\n\nLarger sample size = smaller standard error = more representative of the truth.\nTo help reinforce these concepts, let’s re-display our previous figure but using our new sampling terminology, notation, and definitions:\n\n\n\n\nThree sampling distributions of the sample proportion \\(\\hat{\\rho}\\). Note the increased concentration on the bins around .4 as our sample size increases.\n\n\n\nFurthermore, let’s display the graph of standard errors for \\(n = 1\\) to \\(n = 100\\) using our new terminology, notation, and definitions relating to sampling.\n\n\n\n\nStandard errors of the sample proportion based on sample sizes of 1 to 100\n\n\n\nAs the sample size \\(n\\) goes up, the “typical” error of your point estimate will go down, as quantified by the standard error.\n\n2.3.4 The moral of the story\nStandard error is just a fancy term for your uncertainty about something you don’t know. Standard error is the precision of our (uncertain) beliefs.\n\n\n\n\nIf you are wondering how much you need to know, follow this helpful guide of the information we have learned this chapter!\n\n\n\nThis hierarchy represents the knowledge we need to understand standard errors. At the bottom, we have math. It’s the foundation for our understanding, but it doesn’t need to be what we take away from this lesson. As we go up, we simplify the topic. The top of the pyramid are the most basic levels of understanding.\nIf I know your estimate plus or minus two standard errors, I know your 95% confidence interval. This is valuable information. Standard error is really just a measure for how uncertain we are about something we do not know, the thing we are estimating. Speaking snarkily, standard error is the way old people talk about confidence intervals.\nRecall that \\(\\hat{\\rho}\\) is the estimated value of p which comes from taking a sample. There can be billions and billions of \\(\\hat{\\rho}\\)’s. We look at a large group of \\(\\hat{\\rho}\\)’s, create a distribution of results to represent the possible values of \\(\\rho\\) based on our findings, and then we compute a standard error to account for our own uncertainty about our estimates. Our 95% confidence interval for our prediction is our estimate plus or minus two standard errors.\nYou must understand what the standard error of \\(\\hat{\\rho}\\) means. You do not need to understand why.\nCentral lesson: Your posterior for (almost) any population parameter is normally distributed with a mean equal to the sample mean and a standard deviation equal to the standard error. And that means that your posterior has the same shape as the sampling distribution.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling</span>"
    ]
  },
  {
    "objectID": "03-rubin-causal-model.html",
    "href": "03-rubin-causal-model.html",
    "title": "3  Rubin Causal Model",
    "section": "",
    "text": "3.1 Predictive model\nHave you ever wondered what the world would be like without you?\nIn the classic film “It’s a Wonderful Life,” the protagonist, George Bailey, finds himself in a state of despair, convinced that his life has been meaningless. The movie takes the audience on a journey alongside George as he is granted a glimpse into an alternate reality—a world in which he had never existed. Through this experience, it becomes evident that George’s life had a significant and positive impact on the lives of countless individuals within his community. His selfless actions and kind-hearted nature had touched the lives of many, leaving an indelible mark that he had never fully appreciated. The film serves as a poignant reminder that our lives, no matter how seemingly ordinary, have the power to make a profound difference in the world around us.\nBy showing what the world would have been like without George, we get an idea of the causal effect of his life on his town and the people who live there. This chapter explains causation using the framework of potential outcomes and the Rubin Causal Model (RCM).\nWe begin by discussing predictive versus causal models in data science. In both cases, a Preceptor Table and a Population Table provide analytic tools for attacking the problem. Assumptions concerning validity, stability, representativeness, and unconfoundedness are critical.\nWe would not need data science if we (and our bosses, colleagues, and clients) did not have questions. Every data science project starts with a question. Examples:\nWhat is the average height of a student at New College of Florida?\nWhat are the chances that, out of the next four New College students we meet, one will be taller than 183 centimeters?\nA predictive model involves exactly one outcome which, in this case, is height. Of course, there are many other variables associated with college students: weight, sex, age, family income, GPA, et cetera. These questions, however, are about height, a variable which is fixed, meaning that each individual has only one possible value for height, at least for the purpose of these questions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Rubin Causal Model</span>"
    ]
  },
  {
    "objectID": "03-rubin-causal-model.html#predictive-model",
    "href": "03-rubin-causal-model.html#predictive-model",
    "title": "3  Rubin Causal Model",
    "section": "",
    "text": "3.1.1 Preceptor Table\nA Preceptor Table1 is the smallest possible table with rows and columns such that, if none of the data is missing, then the things we want to know are easy to calculate. Consider:\n\n\n\n\n\n\n\n\n\n\n\nPreceptor Table\n\n\nID\n\nOutcome\n\n\n\nHeight (cm)\n\n\n\n\nStudent 1\n190\n\n\nStudent 2\n160\n\n\n...\n...\n\n\nStudent 47\n172\n\n\nStudent 48\n176\n\n\n...\n...\n\n\nStudent 325\n180\n\n\nStudent 326\n162\n\n\n...\n...\n\n\nStudent 670\n185\n\n\n\n\n\n\nIf we had a table with the height of every New College student, then statistics like the average would be easy to calculate. It would also be straightforward, using simulation, to estimate the chances of various scenarios, as in our second question.\nConstructing the Preceptor Table is sometimes tricky. The question(s) we start with help us to identify the “units,” “outcome,” and “covariates” which describe the structure of the table. The “units” refer to the rows, which are the individual students at New College. Different question(s) would require different units. The “outcome” refers to height, the key variable which is the focus of the question(s).\nEven the simplest Preceptor Table will have two columns. The first is an ID column which serves to label each unit. The second column is the outcome of interest, the variable we are trying to predict/understand/influence. The rows of the Preceptor Table are the units, the objects on which the outcome is measured.\nHowever, if we had different questions, then we would need a different Preceptor Table. Consider:\nWhat is the average height of a female student at New College of Florida?\nWhat are the chances that, out of the next four male New College students we meet, one will be taller than 183 centimeters?\nThe Preceptor Table for the first set of questions did not require a column for sex because we did not need that information to answer those questions. For these new questions, we do:\n\n\n\n\n\n\n\n\n\n\n\n\nPreceptor Table\n\n\nID\n\nOutcome\n\n\nCovariate\n\n\n\nHeight (cm)\nSex\n\n\n\n\nStudent 1\n190\nMale\n\n\nStudent 2\n160\nFemale\n\n\n...\n...\n...\n\n\nStudent 47\n172\nFemale\n\n\nStudent 48\n176\nFemale\n\n\n...\n...\n...\n\n\nStudent 325\n180\nMale\n\n\nStudent 326\n162\nFemale\n\n\n...\n...\n...\n\n\nStudent 670\n185\nMale\n\n\n\n\n\n\nCovariates are the variables, other than the outcome, which we need to answer our question(s).\n\n3.1.2 Data\nSadly, in the real world, we almost never have a complete Preceptor Table with no missing data. After all, if our boss/client/colleague had that, they would not have needed to call us! Instead, we have some data. Consider:\n\n\n\n\n\n\n\n\n\n\n\nData for New College Students\n\n\nID\n\nOutcome\n\n\n\nHeight (cm)\n\n\n\n\nStudent 11\n160\n\n\nStudent 32\n189\n\n\nStudent 47\n172\n\n\nStudent 68\n170\n\n\nStudent 425\n175\n\n\nStudent 436\n162\n\n\nStudent 570\n188\n\n\n\n\n\n\nNote that, if this is the only data we have, then we can’t answer the second set of questions because our data does not include any information about sex. And that is OK! There are many situations in which the data we have access to is not enough to answer the questions which we have.\nThe notion of time is important, both in our Preceptor Table and in our data. To what moment in time do the questions refer? At what moment in time was our data collected? Those two moments are almost always different. Our data comes from some point in the past, even if it was collected yesterday. Our questions usually refer to now or to an indeterminate moment in the future. Implicit in both the Preceptor Table and the data is a “time” column, even if it is not actually present. The times almost always differ between Preceptor Table and data.\nIn order to use this data to answer our questions, we need to consider the concept of validity. Is the data we have valid for answering our questions? Height, as an outcome, seems to make this simple, but, even here, complications can arise. For example, was height measured with shoes on or off? Which measure do we want? Validity is the consistency, or lack there of, in the columns of our data set and the corresponding columns in our Preceptor Table. If validity holds — if “height” means close enough to the same thing in both sources — we can combine the Preceptor Table with our data to construct the Population Table.\n\n3.1.3 Population Table\nWe create the Population Table2 by combining the Preceptor Table and the data. The aim of the Population Table is to illustrate the broader population in which we are interested. This table has three sources of data: the data for units we want to have (the Preceptor Table), the data for units which we actually have (our data), and the data for units we do not care about (the rest of the population, not included in the data or the Preceptor Table).\nWe are trying to answer questions about the height of New College students in 2025, so the “Year” entries for these rows will read “2025.” In this case, the actual data comes from a survey of New College students in 2015.\nWe drop the ID column since we do not need it to answer our questions.\nThe “Source” column refers to the source of the rows in the Population Table. The rows with no “Source” are from the larger population from which, by assumption, both the Preceptor Table and the data are drawn. As such, all values, other than year, are missing.\n\n\n\n\n\n\n\nPopulation Table\n\n\nSource\nYear\nHeight\n\n\n\n\n…\n2010\n…\n\n\n…\n…\n…\n\n\nData\n2015\n180\n\n\nData\n2015\n163\n\n\n…\n…\n…\n\n\n…\n2018\n…\n\n\n…\n…\n…\n\n\nPreceptor Table\n2025\n?\n\n\nPreceptor Table\n2025\n?\n\n\n…\n…\n…\n\n\n…\n2030\n…\n\n\n…\n2030\n…\n\n\n\n\n\n\nThe question marks indicate the data which we do not know but which we need in order to answer the questions. The ellipses are data we don’t know and don’t need to know.\nBy definition, our population covers both the data we have and the data we want to have: the Preceptor Table. That is to say that, given our data is sourced from 2015 and our desired data is from 2025, our population must span at least those years. But, obviously, it seems reasonable that the population goes both earlier and later than these endpoints. After all, we would probably use the same model even if our data came from 2010 instead of 2015 or if we had wanted to answer questions about 2030 instead of 2025.\nNever forget that time is always a lie in the Population Table. Indeed, any time variable is suspect in data science more broadly.\n\nA moment in time is rarely measured accurately. In this example, we refer to the data being recorded in 2015. But that isn’t true! We didn’t record student heights across a year. We recorded them once, on a specific date like August 9, 2015 and at a specific time like 3:16 PM.\nIdentical values often aren’t truly identical. Even though the data measures for different students all refer to 2015, they almost certainly occurred at different moments in time, even if all measurements were taken during the year 2015. In fact, measurements can and do occur on different days or even months. Using 2015 a the value for year hides that variation.\nThe value for the time variable for rows corresponding to the Preceptor Table is always hazy. Does it refer to now? Tomorrow? Whenever we finish the analysis? In general, we don’t know the time period for which we are answering our question until we actually get to the point of answering it.\n\n3.1.4 Assumptions\nValidity, as we have already discussed, is the consistency, or lack there of, in the columns of our data set and the corresponding columns in our Preceptor Table. In order to consider the two data sets to be drawn from the same population, the columns from one must have a valid correspondence with the columns in the other. Validity, if true (or at least reasonable), allows us to construct the Population Table. Fortunately, the height variable from each source, while perhaps not identical in meaning, are similar enough that we can treat them as if they are the same thing.\nStability means that the relationship between the columns in the Population Table is the same for three categories of rows: the data, the Preceptor Table, and the larger population from which both are drawn.\nIn the case of height, we might worry that the population of students at New College is different today than it was in 2015. For example, maybe there are more students from Denmark and Slovenia, two countries where people are taller, on average, than in the US. If so, then the stability assumption would not hold and we could not use our data from 2015 to make inferences about students in 2015.\nRepresentativeness, or the lack thereof, concerns two relationships among the rows in the Population Table. The first is between the Preceptor Table and the other rows. The second is between our data and the other rows. Ideally, we would like both the Preceptor Table and our data to be random samples from the population. Sadly, this is almost never the case.\nWith height among New College students, we might worry about the process by which our data was collected in 2015. If the students in the data were a random sample of all students, then representativeness would not be a concern. Random samples are representative, at least in expectation. But if, instead, the 2015 data was collected from students walking out of the gym, then that sample of students might not be representative of the population of New College students from 2010 to 2030 as a whole.\nUnconfounded is an assumption which only applies to causal models. We will discuss it below.\nWe started with some questions about the height of New College students. We created a Preceptor Table which, if it existed with no missing data, would allow us to answer those questions",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Rubin Causal Model</span>"
    ]
  },
  {
    "objectID": "03-rubin-causal-model.html#causal-model",
    "href": "03-rubin-causal-model.html#causal-model",
    "title": "3  Rubin Causal Model",
    "section": "\n3.2 Causal model",
    "text": "3.2 Causal model\nPredictive models are flexible enough to cover a wide range of questions. For example, @enos2014 measured attitudes toward immigration among Boston commuters. Individuals were exposed to one of two possible conditions, and then their attitudes towards immigrants were recorded. One condition was waiting on a train platform near individuals speaking Spanish. The other was being on a train platform without Spanish-speakers.\n\n\n\n\nThis study, which shows the impact of exposure to Spanish-speaking individuals on attitudes towards immigration, was conducted by Ryan Enos.\n\n\n\nConsider two questions:\nWhat is the average attitude toward immigration of Boston commuters?\nWhat is the average attitude toward immigration of Boston commuters exposed to Spanish speakers on the train platform?\nTo answer these questions, we can use a Preceptor Table with the same format as the height/sex example from New College:\n\n\n\n\n\n\n\n\n\n\n\n\nPreceptor Table\n\n\nID\n\nOutcome\n\n\nCovariate\n\n\n\nAttitude toward Immigration\nTreatment\n\n\n\n\nCommuter 1\n10\nTreated\n\n\nCommuter 2\n15\nControl\n\n\n...\n...\n...\n\n\nCommuter 47\n7\nControl\n\n\nCommuter 48\n5\nControl\n\n\n...\n...\n...\n\n\nCommuter 325\n10\nTreated\n\n\nCommuter 326\n9\nControl\n\n\n...\n...\n...\n\n\nCommuter N\n6\nTreated\n\n\n\n\n\n\nThe two Preceptor Tables have the same structure because the structure of the two problems is the same. In this case, the units are individual Boston commuters, the outcome is their attitude toward immigration (measured on a 3 to 15 integer scale), and the covariate is whether or not they were exposed to Spanish speakers (“treated”) on the train platform or not (“control”). If we had access to a Preceptor Table with no missing data, we could easily answer our questions. (Side note: the last commuter has an ID number N because we are not sure how many commuters there are.)\nThis is a predictive model, like the height one, because we assume that attitude toward immigration is fixed, conditional on the covariate value for treatment. We want to know what Commuter 1’s attitude is (10) and, to estimate that, we can use the fact that they were treated, i.e., exposed to Spanish speakers on the train platform. To answer our questions, we do not need to consider any counter-factuals. We do not need to know what Commuter 1’s attitude would have been if they had not been exposed to the Spanish speakers. To answer questions concerning such counter-factuals, we need to construct a causal model.\n\n3.2.1 Rubin Causal Model (RCM)\nThe Rubin Causal Model (RCM) is based on the idea of potential outcomes. To calculate the causal effect of having Spanish-speakers nearby, we need to compare the outcome for an individual in one possible state of the world (with Spanish-speakers) to the outcome for that same individual in another state of the world (without Spanish-speakers). However, it is impossible to observe both potential outcomes at once. One of the potential outcomes is always missing, since a unit cannot travel back in time, and experience both treatments. This dilemma is the Fundamental Problem of Causal Inference.\nIn most circumstances, we are interested in comparing two experimental manipulations, one generally termed “treatment” and the other “control.” The difference between the potential outcome under treatment and the potential outcome under control is a “causal effect” or a “treatment effect.” According to the RCM, the causal effect of being on the platform with Spanish-speakers is the difference between what your attitude would have been under “treatment” (with Spanish-speakers) and under “control” (no Spanish-speakers).\nThe commuter survey consisted of three questions, each measuring agreement on a 1 to 5 integer scale, with 1 being liberal and 5 being conservative. For each person, the three answers were summed, generating an overall measure of attitude toward immigration which ranged from 3 (very liberal) to 15 (very conservative). If your attitude towards immigrants would have been a 13 after being exposed to Spanish-speakers and a 9 with no such exposure, then the causal effect of being on a platform with Spanish-speakers is a 4-point increase in your score.\nWe will use the symbol \\(Y\\) to represent potential outcomes, the variable we are interested in understanding and modeling. \\(Y\\) is called the response or outcome variable. It is the variable we want to “explain.” In our case this would be the attitude score. If we are trying to understand a causal effect, we need two symbols so that the value with treatment and the value with control can be represented separately: \\(Y_t\\) and \\(Y_c\\).\n\n3.2.2 Potential outcomes\nSuppose that Yao is one of the commuters surveyed in this experiment. If we were omniscient, we would know the outcomes for Yao under both treatment (with Spanish-speakers) and control (no Spanish-speakers), and we would be able to ignore the Fundamental Problem of Causal Inference. We can show this using a Preceptor Table. Calculating the number we are interested in is trivial because none of the data is missing.\n\n\n\n\n\n\n\n\n\n\n\n\nPreceptor Table\n\n\nID\n\nOutcomes\n\n\n\nAttitude if Treated\nAttitude if Control\n\n\n\nYao\n13\n9\n\n\n\n\n\nRegardless of what the causal effect is for other subjects, the causal effect for Yao of being on the train platform with Spanish-speakers is a shift towards a more conservative attitude.\nUsing the response variable — the actual symbol rather than a written description — makes for a more concise Preceptor Table.\n\n\n\n\n\n\n\n\n\n\n\n\nPreceptor Table\n\n\nID\n\nOutcomes\n\n\n\n\\[Y_t\\]\n\\[Y_c\\]\n\n\n\nYao\n13\n9\n\n\n\n\n\nThe “causal effect” is the difference between Yao’s potential outcome under treatment and his potential outcome under control.\n\n\n\n\n\n\n\n\n\n\n\n\n\nPreceptor Table\n\n\nID\n\nOutcomes\n\n\nCausal Effect\n\n\n\n\\[Y_t\\]\n\\[Y_c\\]\n\\[Y_t - Y_c\\]\n\n\n\nYao\n13\n9\n+4\n\n\n\n\n\nRemember that, in the real world, we will have a bunch of missing data! We can not use simple arithmetic to calculate the causal effect on Yao’s attitude toward immigration. Instead, we will be required to estimate it. An estimand is some unknown variable in the real world that we are trying to measure. In this case, it is \\(Y_{t}-Y_{c}\\), not \\(+4\\). An estimand is not the value you calculated, but is rather the unknown variable you want to estimate.\n\n\n\n\nDon Rubin was a Professor of Statistics at Harvard.\n\n\n\n\n3.2.3 Causal and predictive models\nCausal inference is often compared with prediction. In prediction, we want to know an outcome, \\(Y\\). In causal inference, we want to know a function of potential outcomes, such as the treatment effect: \\(Y_t - Y_c\\).\nThese are both missing data problems. Prediction involves estimating an outcome variable that we don’t have, and thus is missing, whether because it is in the future or because it is from data that we are unable to collect. Thus, prediction is the term for using statistical inference to fill in missing data for individual outcomes. Causal inference, however, involves filling in missing data for more than one potential outcome. This is unlike prediction, where only one outcome can ever be observed, even in principle.\nKey point: In a predictive model, there is only one \\(Y\\) value for each unit. This is very different to the RCM where there are (at least) two potential outcomes (treatment and control). There is only one outcome column in a predictive model, whereas there are two or more in a causal model.\nWith a predictive model, we cannot infer what would happen to the outcome \\(Y\\) if we changed \\(X\\) for a given unit. We can only compare two units, one with one value of \\(X\\) and another with a different value of \\(X\\).\nIn a sense, all models are predictive. However, only a subset of models are causal, meaning that, for a given individual, you can change the value \\(X\\) and observe a change in outcome, \\(Y\\), and from that calculate a causal effect.\n\n3.2.4 No causation without manipulation\n\nIn order for a potential outcome to make sense, it must be possible, at least a priori, for a unit to receive both the treatment and the control. If a unit can not receive the treatment, for example, then \\(Y_t\\) is not defined and, therefore, the causal effect for that unit is not defined.\nThe causal effect of exposure to Spanish-speakers is the difference between two potential outcomes. In this case, we (or something else) can manipulate the world, at least conceptually, so that it is possible that one thing (exposure to Spanish speakers) or another (no exposure) might happen.\nThis definition of causal effects becomes much more problematic if there is no way for one of the potential outcomes to occur, ever. For example, what is the causal effect of Yao’s height on his weight? It might seem we would just need to compare two potential outcomes: Yao’s weight under the treatment (where treatment is defined as being 3 inches taller) and Yao’s weight under the control (where control is defined as his current height).\nA moment’s reflection highlights the problem: we can’t increase Yao’s height. There is no way to observe, even conceptually, what Yao’s weight would be if he were taller because there is no way to make him taller. We can’t manipulate Yao’s height, so it makes no sense to investigate the causal effect of height on weight. Hence the slogan: No causation without manipulation.\nThis then raises the question of what can and cannot be manipulated. If something cannot be manipulated, we should not consider it causal. Can race ever be considered causal? What about sex? A genetic condition like color blindness? Can we manipulate these characteristics? In the modern world these questions are not simple.\nConsider color blindness. Say we are interested in how color blindness impacts ability to complete a jig-saw puzzle. Because color blindness is genetic some might argue it cannot be manipulated. But advances in technology like gene therapy might allow us to actually change someone’s genes. Could we then claim the ability to manipulate color blindness? If yes, we could then measure the causal effect of color blindness on ability to complete jig-saw puzzles.\nThe slogan of “No causation without manipulation” may at first seem straight-forward, but it is not so simple. Questions about race, sex, and genetics are very complex and should be considered with care.\n\n3.2.5 Multiple units\n\nGenerally, a study has many individuals (or, more broadly, “units”) who each have their own potential outcomes. More notation is needed to allow us to differentiate between different units.\nIn other words, there needs to be a distinction between \\(Y_t\\) for Yao, and \\(Y_t\\) for Emma. We use the variable \\(u\\) (\\(u\\) for “unit”) to indicate that the outcome under control and the outcome under treatment can differ for each individual unit (person).\nInstead of \\(Y_t\\), we will use \\(Y_t(u)\\) to represent “Attitude if Treated.” If you want to talk about only Emma, you could say “Emma’s Attitude if Treated” or “\\(Y_t(u = Emma)\\)” or “the \\(Y_t(u)\\) for Emma”, but not just \\(Y_t\\). That notation is too ambiguous when there is more than one subject.\nLet’s look at a Preceptor Table with more subjects using our new notation:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPreceptor Table\n\n\nID\n\nOutcomes\n\n\nCausal Effect\n\n\n\n\\[Y_t(u)\\]\n\\[Y_c(u)\\]\n\\[Y_t(u) - Y_c(u)\\]\n\n\n\n\nYao\n13\n9\n+4\n\n\nEmma\n14\n11\n+3\n\n\nCassidy\n11\n6\n+5\n\n\nTahmid\n9\n12\n-3\n\n\nDiego\n3\n4\n-1\n\n\n\n\n\n\nFrom this Preceptor Table, there are many possible estimands we might be interested in. Consider some examples, along with their true values:\n\n\n\nA potential outcome for one person, e.g., Yao’s potential outcome under treatment: \\(13\\).\nA causal effect for one person, such as for Emma. This is the difference between the potential outcomes: \\(14 - 11 = +3\\).\nThe most positive causal effect: \\(+5\\), for Cassidy.\nThe most negative causal effect: \\(-3\\), for Tahmid.\nThe median causal effect: \\(+3\\).\nThe median percentage change: \\(+27.2\\%\\). To see this, calculate the percentage change for each person. You’ll get 5 percentages: \\(+44.4\\%\\), \\(+27.2\\%\\), \\(+83.3\\%\\), \\(-25.0\\%\\), and \\(-25.0\\%\\).\n\n\nSimilar concepts can also be applied to the Population Table:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPopulation Table\n\n\nSource\nYear\nID\n\nOutcomes\n\n\nCausal Effect\n\n\n\n\\[Y_t(u)\\]\n\\[Y_c(u)\\]\n\\[Y_t(u) - Y_c(u)\\]\n\n\n\n\n…\n2010\n?\n?\n?\n?\n\n\n…\n…\n…\n…\n…\n…\n\n\nData\n2012\nYao\n5\n3\n+2\n\n\nData\n2012\nCassidy\n3\n6\n-3\n\n\n…\n…\n…\n…\n…\n…\n\n\n…\n2018\n?\n?\n?\n?\n\n\n…\n…\n…\n…\n…\n…\n\n\nPreceptor Table\n2025\nYao\n13\n9\n+4\n\n\n…\n…\n…\n…\n…\n…\n\n\n…\n2030\n?\n?\n?\n?\n\n\n\n\n\n\nAgain, all the rows in the Preceptor Table are, by definition, included in the Population Table. The later includes two sorts of extra rows. The first corresponds to our data set. The second corresponds to other unit/time combinations in the population which are neither part of the Preceptor Table nor part of the data.\nAll of the variables calculated in the Preceptor and Population Tables are examples of estimands we might be interested in. One estimand is important enough that it has its own name: the average treatment effect, often abbreviated as ATE. The average treatment effect is the mean of all the individual causal effects. Here, the mean is \\(+1.6\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Rubin Causal Model</span>"
    ]
  },
  {
    "objectID": "03-rubin-causal-model.html#assumptions-1",
    "href": "03-rubin-causal-model.html#assumptions-1",
    "title": "3  Rubin Causal Model",
    "section": "\n3.3 Assumptions",
    "text": "3.3 Assumptions\nIn this section, we will explore the four key assumptions underlying any data science project: validity, stability, representativeness and unconfoundedness.\nRecall that the Population Table is constructed from three sources: the Preceptor Table, our data, and the greater population from which both are drawn. Consider a different Population Table:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPopulation Table\n\n\nSource\n\nOutcomes\n\nYear\n\nCovariates\n\n\nCausal Effect\n\n\n\n\\[Y_t(u)\\]\n\\[Y_c(u)\\]\nSex\n\\[Y_t(u) - Y_c(u)\\]\n\n\n\n\n...\n?\n?\n2010\n?\n?\n\n\n...\n?\n?\n2010\n?\n?\n\n\n...\n...\n...\n...\n...\n...\n\n\nData\n13\n?\n2012\nMale\n?\n\n\nData\n?\n9\n2012\nFemale\n?\n\n\n...\n...\n...\n...\n...\n...\n\n\nPreceptor Table\n?\n?\n2025\nFemale\n?\n\n\nPreceptor Table\n?\n?\n2025\nFemale\n?\n\n\n...\n...\n...\n...\n...\n...\n\n\n...\n?\n?\n2025\n?\n?\n\n\n...\n?\n?\n2025\n?\n?\n\n\n\n\n\n\nThe rows from our data have covariates and two potential outcomes. (By definition, we only know the value for one, at most, of the potential outcomes for each unit.) The rows from the Preceptor Table include covariates, but not outcomes. (We have no outcome data for 2025, but we will, in theory, be given the sex of any units about whom we need to make inferences.) The rows from our greater population include no data, as we know nothing about these units.\n\n\n\n3.3.1 Validity\n\nValidity is the consistency, or lack there of, in the columns of your data set and the corresponding columns in your Preceptor Table. In order to consider the two data sets to be drawn from the same population, the columns from one must have a valid correspondence with the columns in the other. Validity, if true (or at least reasonable), allows us to construct the Population Table.\nConsider the potential outcomes: attitude toward immigration under treatment and control. How is this measured? In this case, we use answers to survey questions. Are we going to use the exact same questions in 2025 as were used in our data from 2012? Probably not. Even if we used the same wording, will the meaning of the words be constant? Almost certainly not! The word “undocumented” could mean something very different in the America before the election of Donald Trump in 2016. Consider the treatment: exposure to Spanish speakers on a train platform. The speakers used in 2012 will not be available in 2025 if we were to redo the experiment.\nValidity is an assumption which allows us to “stack” the data and the Preceptor Table on top of one another, into a single Population Table. The variables from each source, while not identical in meaning, are similar enough that we can treat them as if they are the same thing.\n\n3.3.2 Stability\nStability means that the relationship between the columns in the Population Table is the same for three categories of rows: the data, the Preceptor Table, and the larger population from which both are drawn.\nWe will be constructing models, mathematical relationships between different variables. For example, perhaps the treatment has a greater effect on women than on men. However, we can only directly observe that relationship in the data which we have, from 2012. We must assume stability in order to use the model created from the data to make inferences about other rows in the Population Table, specifically the rows corresponding to the Preceptor Table.\nMust the world be stable? No! In fact, the world is always changing! The stability assumption, in this case, is a claim that the world has not changed that much between 2012 and 2025. Stability applies, not just to the data and the Preceptor Table rows, but to the entire Population Table. Stability allows us to ignore the passage of time.\n\n3.3.3 Representativeness\n\n\nRepresentativeness, or the lack thereof, concerns two relationships among the rows in the Population Table. The first is between the Preceptor Table and the other rows. The second is between our data and the other rows. Ideally, we would like both the Preceptor Table and our data to be random samples from the population. Sadly, this is almost never the case.\nDoes the train experiment allow us to calculate a causal effect for people who commute by cars? Can we calculate the causal effect for people in New York City? Before we generalize to a broader population we have to consider if our experimental estimates are applicable beyond our experiment. Maybe we think that commuters in Boston and New York are similar enough to generalize our findings. We could also conclude that people who commute by car are fundamentally different than people who commute by train. If that was true, then we could not say our estimate is true for all commuters because our sample does not accurately represent the broader group to which we want to generalize.\n\n\n\n\n\n3.3.4 Unconfoundedness\nA fourth assumption we use when working with causal models — but not with predictive models — is “unconfoundedness.” If whether or not a unit received treatment or control is random, we write that treatment assignment is not “confounded.” If, however, treatment assignment depends on the value of a potential outcome, then treatment assignment is confounded. Our lives are easiest if we can (reasonably!) assume unconfoundedness. In that case, we can estimate the average treatment effect by subtracting the average outcome for control units from the average outcome for treated units.\nConsider the “Perfect Doctor” as an example of the problems caused by confounded treatment assignments. Imagine we have an omniscient doctor who knows how any patient will respond to a certain drug. She has perfect knowledge of the entire Preceptor Table. Using this information, she always assign each patient the treatment with the best outcome, whether that is treatment or control. In this case, lower blood pressue is better. Consider:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHoly Grail of Information\n\n\nID\n\nBlood Pressure Outcomes\n\n\nCausal Effect\n\n\n\n\\[Y_t(u)\\]\n\\[Y_c(u)\\]\n\\[Y_t(u) - Y_c(u)\\]\n\n\n\n\nYao\n130\n105\n+25\n\n\nEmma\n120\n140\n-20\n\n\nCassidy\n100\n170\n-70\n\n\nTahmid\n115\n125\n-10\n\n\nDiego\n135\n100\n35\n\n\nMEAN\n120\n128\n-8\n\n\n\n\n\n\nThe Perfect Doctor would assign the treatment to Emma, Cassidy and Tahmid. She would assign control to Yao and Diego. And that is good! This is what the doctor should do. This is the best treatment assignment for the patients. But it is not a good assignment mechanism for estimating the average causal effect because treatment assignment is confounded by the values of the potential outcomes.\nWe, the non-Perfect Doctors, do not have access to the entire Preceptor Table. We can only see this:\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkewed Holy Grail of Information\n\n\nID\n\nBlood Pressure Outcomes\n\n\nCausal Effect\n\n\n\n\\[Y_t(u)\\]\n\\[Y_c(u)\\]\n\\[Y_t(u) - Y_c(u)\\]\n\n\n\n\nYao\n?\n105\n?\n\n\nEmma\n120\n?\n?\n\n\nCassidy\n100\n?\n?\n\n\nTahmid\n115\n?\n?\n\n\nDiego\n?\n100\n?\n\n\nMEAN\n111.66\n102.5\n9.16\n\n\n\n\n\n\nThe true causal effect of the treatment, as we can see in the first table, is -8. In other words, the treatment lowers blood pressure on average. But, using just the data we have access to if the Perfect Doctor performs the treatment assignment, we would estimate — if we mistakenly assume unconfoundedness — that the causal effect is positive, that treatment increases blood pressure.\nThe best way to ensure unconfoundedness is to randomize the treatment across units. Don’t let the doctor decide who gets the treatment and who gets the control. Randomize assignment. As long as you use randomization as your assignment mechanism, you are good. There is the possibility that you can’t use pure randomization due to ethical or practical reasons, so we are forced to use a non-random assignment mechanisms. Many statistical methods have been developed for causal inference when there is a non-random assignment mechanism. Those methods, however, are largely beyond the scope of this book.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Rubin Causal Model</span>"
    ]
  },
  {
    "objectID": "03-rubin-causal-model.html#simple-models",
    "href": "03-rubin-causal-model.html#simple-models",
    "title": "3  Rubin Causal Model",
    "section": "\n3.4 Simple models",
    "text": "3.4 Simple models\nHow can we fill in the question marks? Because of the Fundamental Problem of Causal Inference, we can never know the missing values. Because we can never know the missing values, we must make assumptions. “Assumption” just means that we need a “model,” and all models have parameters.\n\n3.4.1 A single value for tau\nOne model might be that the causal effect is the same for everyone. There is a single parameter, \\(\\tau\\), which we then estimate. (\\(\\tau\\) is a Greek letter, written as “tau” and rhyming with “cow.”) Once we have an estimate, we can fill in the Preceptor Table because, knowing it, we can estimate what the unobserved potential outcome is for each person. We use our assumption about \\(\\tau\\) to estimate the counterfactual outcome for each unit.\nRemember what our Preceptor Table looks like with all of the missing data:\n\n\n\n\n\n\n\n\n\n\n\n\n\nPreceptor Table\n\n\nID\n\nOutcomes\n\n\nCausal Effect\n\n\n\n\\[Y_t(u)\\]\n\\[Y_c(u)\\]\n\\[Y_t(u) - Y_c(u)\\]\n\n\n\n\nYao\n13\n?\n?\n\n\nEmma\n14\n?\n?\n\n\nCassidy\n?\n6\n?\n\n\nTahmid\n?\n12\n?\n\n\nDiego\n3\n?\n?\n\n\n\n\n\n\nIf we assume \\(\\tau\\) is the treatment effect for everyone, how do we fill in the table? We are using \\(\\tau\\) as an estimate for the causal effect. By definition: \\(Y_t(u) - Y_c(u) = \\tau\\). Using simple algebra, it is then clear that \\(Y_t(u) = Y_c(u) + \\tau\\) and \\(Y_c(u) = Y_t(u) - \\tau\\). In other words, you could add it to the observed outcome for every observation in the control group (or subtract it from the observed outcome for every observation in the treatment group), and thus fill in all the missing values.\nAssuming there is a constant treatment effect, \\(\\tau\\), for everyone, filling in the missing values would look like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\nPreceptor Table\n\n\nID\n\nOutcomes\n\n\nCausal Effect\n\n\n\n\\[Y_t(u)\\]\n\\[Y_c(u)\\]\n\\[Y_t(u) - Y_c(u)\\]\n\n\n\n\nYao\n13\n\\[13 - \\tau\\]\n\\[\\tau\\]\n\n\nEmma\n14\n\\[14 - \\tau\\]\n\\[\\tau\\]\n\n\nCassidy\n\\[6 + \\tau\\]\n6\n\\[\\tau\\]\n\n\nTahmid\n\\[12 + \\tau\\]\n12\n\\[\\tau\\]\n\n\nDiego\n3\n\\[3 - \\tau\\]\n\\[\\tau\\]\n\n\n\n\n\n\nNow we need to find an estimate for \\(\\tau\\) in order to fill in the missing values. One approach is to subtract the average of the observed outcomes from units assigned to control from the average of the observed outcomes from units assigned to treatment: \\[((13 + 14 + 3) / 3) - ((6 + 12) /  2)\\] \\[10 - 9 = +1\\]\nOr, in other words, we use this formula:\n\\[\\frac{\\Sigma Y_t(u)}{n_t} + \\frac{\\Sigma Y_c(u)}{n_c} = \\widehat{ATE}\\]\n\\(\\Sigma\\) represents the sum of the outcomes under treatment or control, with \\(n_t\\) and \\(n_c\\) being the number of units assigned to treatment and control, respectively. This is the formula for \\(\\widehat{ATE}\\), our estimate of the average causal (or treatment) effect.\nContinuing with the example, calculating the ATE or the causal effect, gives us an estimate of \\(+1\\) for \\(\\tau\\). Let’s fill in our missing values by adding \\(\\tau\\) to the observed values under control and by subtracting \\(\\tau\\) from the observed value under treatment like so:\n\n\n\n\n\n\n\n\n\n\n\n\n\nPreceptor Table\n\n\nID\n\nOutcomes\n\n\nCausal Effect\n\n\n\n\\[Y_t(u)\\]\n\\[Y_c(u)\\]\n\\[Y_t(u) - Y_c(u)\\]\n\n\n\n\nYao\n13\n\\[13 - (+1)\\]\n+1\n\n\nEmma\n14\n\\[14 - (+1)\\]\n+1\n\n\nCassidy\n\\[6 + (+1)\\]\n6\n+1\n\n\nTahmid\n\\[12 + (+1)\\]\n12\n+1\n\n\nDiego\n3\n\\[3 - (+1)\\]\n+1\n\n\n\n\n\n\nWhich gives us:\n\n\n\n\n\n\n\n\n\n\n\n\n\nPreceptor Table\n\n\nID\n\nOutcomes\n\n\nCausal Effect\n\n\n\n\\[Y_t(u)\\]\n\\[Y_c(u)\\]\n\\[Y_t(u) - Y_c(u)\\]\n\n\n\n\nYao\n13\n12\n+1\n\n\nEmma\n14\n13\n+1\n\n\nCassidy\n7\n6\n+1\n\n\nTahmid\n13\n12\n+1\n\n\nDiego\n3\n2\n+1\n\n\n\n\n\n\nIf we make the assumption that there is a single value for \\(\\tau\\) and that \\(1\\) is a good estimate of that value, then we can determine the missing potential outcomes. The Preceptor Table no longer has any missing values, so we can use it to easily answer (almost) any conceivable question.\n\n3.4.2 Two values for tau\nA second model might assume that the causal effect is different between levels of a category but the same within those levels. For example, perhaps there is a \\(\\tau_F\\) for females and \\(\\tau_M\\) for males where \\(\\tau_F might not equal \\tau_M\\). We are making this assumption to give us a different model with which we can fill in the missing values in our Preceptor Table. We can’t make any progress unless we make some assumptions. That is an inescapable consequence of the Fundamental Problem of Causal Inference.\nA covariate is any variable outside of the (potential) outcomes. In this case, possible covariates include, but are not limited to, sex, age, political party and almost everything else which might be associated with attitudes toward immigration. Consider a model in which causal effects differ based on sex. (Don’t forget that “causal effect” means, by convention, the same thing as average causal effect when we are discussing the results of a data science project.)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPreceptor Table\n\n\nID\n\nOutcomes\n\n\nCovariate\n\n\nCausal Effect\n\n\n\n\\[Y_t(u)\\]\n\\[Y_c(u)\\]\nSex\n\\[Y_t(u) - Y_c(u)\\]\n\n\n\n\nYao\n13\n\\[13 - \\tau_M\\]\nMale\n\\[\\tau_M\\]\n\n\nEmma\n14\n\\[14 - \\tau_F\\]\nFemale\n\\[\\tau_F\\]\n\n\nCassidy\n\\[6 + \\tau_F\\]\n6\nFemale\n\\[\\tau_F\\]\n\n\nTahmid\n\\[12 + \\tau_M\\]\n12\nMale\n\\[\\tau_M\\]\n\n\nDiego\n3\n\\[3 - \\tau_M\\]\nMale\n\\[\\tau_M\\]\n\n\n\n\n\n\nOur estimate for \\(\\tau_M\\) would be \\[(13+3)/2 - 12 = -4\\] while that for \\(\\tau_F\\) would be \\[14-6 = +8\\].\nUsing those values, we would fill out our new table like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\nPreceptor Table\n\n\nID\n\nOutcomes\n\n\nCausal Effect\n\n\n\n\\[Y_t(u)\\]\n\\[Y_c(u)\\]\n\\[Y_t(u) - Y_c(u)\\]\n\n\n\n\nYao\n13\n\\[13 - (-4)\\]\n-4\n\n\nEmma\n14\n\\[14 - (+8)\\]\n+8\n\n\nCassidy\n\\[6 + (+8)\\]\n6\n+8\n\n\nTahmid\n\\[12 + (-4)\\]\n12\n-4\n\n\nDiego\n3\n\\[3 - (-4)\\]\n-4\n\n\n\n\n\n\nWhich gives us:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPreceptor Table\n\n\nID\n\nOutcomes\n\n\nCausal Effect\n\n\n\n\\[Y_t(u)\\]\n\\[Y_c(u)\\]\n\\[Y_t(u) - Y_c(u)\\]\n\n\n\n\nYao\n13\n17\n-4\n\n\nEmma\n14\n6\n+8\n\n\nCassidy\n14\n6\n+8\n\n\nTahmid\n8\n12\n-4\n\n\nDiego\n3\n7\n-4\n\n\n\n\n\n\nWe now have two different estimates for Emma (and for everyone else in the table). When we estimate \\(Y_c(Emma)\\) using an assumption of constant treatment effect (a single value for \\(\\tau\\)), we get \\(Y_c(Emma) = 13\\). When we estimate assuming treatment effect is constant for each sex, we calculate that \\(Y_c(Emma) = 8\\). This difference between our estimates for Emma highlights the difficulties of inference. Models drive inference. Different models will produce different inferences.\n\n3.4.3 Heterogenous treatment effects\nIs the assumption of a constant treatment effect, \\(\\tau\\), usually true? No! It is never true. People vary. The effect of a pill on you will always be different from the effect of a pill on your friend, at least if we measure outcomes accurately enough. Treatment effects are always heterogeneous, meaning that they vary across individuals.\nReality looks like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\nPreceptor Table\n\n\nID\n\nOutcomes\n\n\nCausal Effect\n\n\n\n\\[Y_t(u)\\]\n\\[Y_c(u)\\]\n\\[Y_t(u) - Y_c(u)\\]\n\n\n\n\nYao\n13\n\\[13 - \\tau_{yao}\\]\n\\[\\tau_{yao}\\]\n\n\nEmma\n14\n\\[14 - \\tau_{emma}\\]\n\\[\\tau_{emma}\\]\n\n\nCassidy\n\\[6 + \\tau_{cassidy}\\]\n6\n\\[\\tau_{cassidy}\\]\n\n\nTahmid\n\\[12 + \\tau_{tahmid}\\]\n12\n\\[\\tau_{tahmid}\\]\n\n\nDiego\n3\n\\[3 - \\tau_{diego}\\]\n\\[\\tau_{diego}\\]\n\n\n\n\n\n\nCan we solve for \\(\\tau_{yao}\\)? No! That is the Fundamental Problem of Causal Inference. Instead, we usually focus on the average causal effect for the entire population.\n\n\n3.4.4 Average treatment effect\nThe average treatment effect (ATE), as discussed above, is the average difference in potential outcomes between the treated group and the control groups. Because averaging is a linear operator, the average difference is the same as the difference between the averages. The distinction between this estimand and estimands like \\(\\tau\\), \\(\\tau_M\\) and \\(\\tau_F\\), is that, in this case, we do not care about using the average treatment effect to fill in missing values in each row. The average treatment effect is useful because we don’t have to assume anything about each individuals’ \\(\\tau\\), like \\(\\tau_{yao}\\), but can still understand something about the average causal effect across the whole population.\nAs we did before, the simplest way to estimate the ATE is to take the mean of the treated group (\\(10\\)) and the mean of the control group (\\(9\\)) and then take the difference in those means (\\(1\\)). If we use this method to an estimate of the ATE, we’ll call it \\(\\widehat{ATE}\\), pronounced “ATE-hat.”\nAs we noted before, this is a popular estimand. Why?\n\nThere’s an obvious estimator for this estimand: the mean difference of the observed outcomes between the treated group and the control group: \\(\\overline{Y_t(u)} - \\overline{Y_c(u)}\\).\nIf treatment is randomly assigned, the estimator is unbiased: you can be fairly confident in the estimate if you have a large enough treatment and control groups.\nIf you are willing to assume that the causal effect is the same for everyone (a big assumption!), you can use your estimate of the ATE, \\(\\widehat{ATE}\\), to fill in the missing individual values in your Preceptor Table.\n\nJust because the ATE is often a useful estimand doesn’t mean that it always is.\nConsider point #3. For example, let’s say the treatment effect does vary dependent on sex. For males there is a small negative effect (-4), but for females there is a larger positive effect (+8). However, the average treatment effect for the whole sample, even if you estimate it correctly, will be a single positive number (+1) – since the positive effect for females is larger than the negative effect for males.\n\nEstimating the average treatment effect, by calculating \\(\\widehat{ATE}\\), is easy. But is our \\(\\widehat{ATE}\\) a good estimate of the actual ATE? After all, if we knew all the missing values in the Preceptor Table, we could calculate the ATE perfectly. But those missing values may be wildly different from the observed values. Consider this Preceptor Table:\n\n\n\n\n\n\n\n\n\n\n\n\n\nPreceptor Table\n\n\nID\n\nOutcomes\n\n\nCausal Effect\n\n\n\n\\[Y_t(u)\\]\n\\[Y_c(u)\\]\n\\[Y_t(u) - Y_c(u)\\]\n\n\n\n\nYao\n13\n10\n+3\n\n\nEmma\n14\n11\n+3\n\n\nCassidy\n9\n6\n+3\n\n\nTahmid\n15\n12\n+3\n\n\nDiego\n3\n0\n+3\n\n\n\n\n\n\nIn this example, there is indeed a constant treatment effect for everyone: \\(+3\\). Note that the observed values are all the same, but the unobserved values were such that our estimated ATE, \\(+1\\), is pretty far from the actual ATE, \\(+3\\). If we think we have a reasonable estimate of ATE, using that value as a constant for \\(\\tau\\) might be our best guess.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Rubin Causal Model</span>"
    ]
  },
  {
    "objectID": "03-rubin-causal-model.html#summary",
    "href": "03-rubin-causal-model.html#summary",
    "title": "3  Rubin Causal Model",
    "section": "\n3.5 Summary",
    "text": "3.5 Summary\n\n\n\n\n\n\nTipGlossary\n\n\n\n\nA Preceptor Table is the smallest possible table that includes all rows and columns such that, if no data is missing, it is easy to calculate our quantity of interest. Unfortunately, there is always data missing.\nA Population Table has three sources of data: the Preceptor Table, the data set, and the greater population from which both are drawn.\nA Potential Outcome is the outcome for an individual depending on if they receive the treatment or not.\nA Causal Effect is the difference between two potential outcomes. The term treatment effect is used interchangeably with causal effect.\nATE, the average treatment (or causal) effect, is the average of the causal (treatment) effect for the units of interest. We often drop the word “average” since it is implied. By convention, causal effect, treatment effect and average treatment effect all refer to the same (unknown) number.\nThe Fundamental Problem of Causal Inference is that it is impossible to observe the causal effect on a single unit because we can never observe both potential outcomes for that unit. We must make assumptions in order to estimate causal effects.\nWhen confronting a causal question, first identify the units, treatments and outcomes.\n\n\n\nThe fundamental components of every problem in causal inference are units, treatments and outcomes. The units are the rows in the table. The treatments are (some of) the columns. The outcomes are the values under the treatment columns. (There are also covariate columns and the values within them.) Whenever you confront a problem in causal inference, start by identifying the units, treatments and outcomes.\nA causal effect is the difference between one potential outcome and another. How different would your life be if you missed the train?\n\n\nA Preceptor Table includes all rows and columns such that, if no data is missing, it is easy to calculate our quantity of interest. Unfortunately, data is always missing in causal models because, at most, we can only observe one potential outcome for each unit. The causal effect of a treatment on a single unit at a point in time is the difference between the value of the outcome variable with the treatment and without the treatment. The Fundamental Problem of Causal Inference is that it is impossible to observe the causal effect on a single unit. We must make assumptions — i.e, we must make models — in order to estimate causal effects.\nThe Population Table has three sources of data: the Preceptor Table, the data set, and the greater population from which both are drawn.\nThe assumption of validity, if met, allows us to create the Population Table because it ensures that the columns of data from the different sources can be put into a single table. If the relationships among the data are the same (or at least same’ish), over time, then we can assume stability. A model estimated on our data will also apply to our Preceptor Table. Representativeness examines the rows we have relative to the rows in the Population Table which we might have had. Unconfoundedness, which only matters in causal settings, means that either the treatment was randomly assigned or that we can act as if it was.\nRandom assignment of treatments to units is the best experimental set up for estimating causal effects. Other assignment mechanisms are subject to confounding. If the treatment assigned is correlated with the potential outcomes, it is very hard to estimate the true treatment effect. (As always, we use the terms “causal effects” and “treatment effects” interchangeably.) With random assignment, we can, mostly safely, estimate the average treatment effect (ATE) by looking at the difference between the average outcomes of the treated and control units.\nBe wary of claims made in situations without random assignment: Here be dragons!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Rubin Causal Model</span>"
    ]
  },
  {
    "objectID": "03-rubin-causal-model.html#footnotes",
    "href": "03-rubin-causal-model.html#footnotes",
    "title": "3  Rubin Causal Model",
    "section": "",
    "text": "The term Preceptor Table is, perhaps unsurprisingly, unique to this textbook. We hope you find it useful.↩︎\nThe term Population Table is, perhaps unsurprisingly, unique to this textbook. We hope you find it useful.↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Rubin Causal Model</span>"
    ]
  },
  {
    "objectID": "04-cardinal-virtues.html",
    "href": "04-cardinal-virtues.html",
    "title": "\n4  Cardinal Virtues\n",
    "section": "",
    "text": "4.1 Introduction\nThe four Cardinal Virtues are Wisdom, Justice, Courage, and Temperance. Because data science is, ultimately, a moral act, we use these virtues to guide our work. Every data science project begins with a question.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cardinal Virtues</span>"
    ]
  },
  {
    "objectID": "04-cardinal-virtues.html#introduction",
    "href": "04-cardinal-virtues.html#introduction",
    "title": "\n4  Cardinal Virtues\n",
    "section": "",
    "text": "Wisdom starts by creating the Preceptor Table. What data, if we had it, would allow us to answer our question easily? If the Preceptor Table has one outcome, then the model is predictive. If it has more than one (potential) outcome, then the model is causal. We then explore the data we have. You can never look too closely at your data. Key question: Are the data we have close enough to the data we want (i.e., the Preceptor Table) that we can consider both as coming from the same population? If not, we can’t proceed further. Key in making that decision is the assumption of validity. Do the columns in the Preceptor Table match the columns in the data?\nJustice starts with the Population Table – the data we want to have (i.e., the Preceptor Table), the data which we actually have, and all the other data from that same population. Each row of the Population Table is defined by a unique Unit/Time combination. We explore three key issues about the Population Table. First, does the relationship among the variables demonstrate stability, meaning is the model stable across different time periods? Second, are the rows associated with the data and, separately, the rows associated with the Preceptor Table representative of all the units from the population? Third, for causal models only, we consider unconfoundedness.\nCourage allows us to explore different models. Justice gave us the Population Table. Courage creates the data generating mechanism. We begin with the basic mathematical structure of the model. With that structure in mind, we decide which variables to include. We estimate the values of the unknown parameters. We avoid hypothesis tests. We check our models for consistency with the data we have. We select one model.\nTemperance guides us in the use of the model we have created to answer the questions with which we began. We create posteriors of quantities of interest. We should be modest in the claims we make. Humility is important. The posteriors we create are never the “truth.” The assumptions we made to create the model are never perfect. Yet decisions made with flawed posteriors are almost always better than decisions made without them.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cardinal Virtues</span>"
    ]
  },
  {
    "objectID": "04-cardinal-virtues.html#wisdom",
    "href": "04-cardinal-virtues.html#wisdom",
    "title": "\n4  Cardinal Virtues\n",
    "section": "\n4.2 Wisdom",
    "text": "4.2 Wisdom\n\n\n\n\nWisdom.\n\n\n\n\nWisdom requires the creation of a Preceptor Table, some exploratory data analysis, and a determination, using the concept of validity, as to whether or not we can (reasonably!) assume that the two come from the same population.\n\nWisdom helps us decide if we can even hope to answer our question with the data that we have.\n\nA Preceptor Table is smallest possible table with rows and columns such that, if there is no missing data, our question is easy to answer.\n\nOne key aspect of this Preceptor Table is whether or not we need more than one potential outcome in order to calculate our estimand. For example, if we want to know the causal effect of exposure to Spanish-speakers on attitude toward immigration then we need a causal model, one which estimates that attitude for each person under both treatment and control. The Preceptor Table would require two columns for the outcome. If, on the other hand, we only want to predict someone’s attitude, or compare one person’s attitude to another person’s, then we would only need a Preceptor Table with one column for the outcome.\nEvery model is predictive, in the sense that, if we give you new data — and it is drawn from the same population — then you can create a predictive forecast. But only a subset of those models are causal, meaning that, for a given individual, you can change the value of one input and figure out what the new output would be and then, from that, calculate the causal effect by looking at the difference between two potential outcomes.\nWith prediction, all we care about is forecasting \\(Y\\) given \\(X\\) on some as-yet-unseen data. But there is no notion of “manipulation” in such models. We don’t pretend that, for Joe, we could turn variable \\(X\\) from a value of \\(5\\) to a value of \\(6\\) by just turning some knob and, by doing so, cause Joe’s value of \\(Y\\) to change from \\(17\\) to \\(23\\). We can compare two people (or two groups of people), one with \\(X\\) equal to \\(5\\) and one with \\(X\\) equal to \\(6\\), and see how they differ in \\(Y\\). The basic assumption of predictive models is that there is only one possible \\(Y\\) for Joe. There are not, by assumption, two possible values for \\(Y\\) for Joe, one if \\(X\\) equals \\(5\\) and another if \\(X\\) equals \\(6\\). The Preceptor Table has a single column under \\(Y\\) if that is all we need to answer the question.\nWith causal inference, however, we can consider the case of Joe with \\(X = 5\\) and Joe with \\(X = 6\\). The same mathematical model can be used. And both models can be used for prediction, for estimating what the value of \\(Y\\) will be for a yet-unseen observation with a specified value for \\(X\\). But, in this case, instead of only a single column in the Preceptor Table for \\(Y\\), we have at least two (and possibly many) such columns, one for each of the potential outcomes under consideration.\nThe difference between predictive models and causal models is that the former have one column for the outcome variable and the latter have more than one column.\nSecond, we look at the data we have and perform an exploratory data analysis, an EDA. You can never look at your data too much. The most important variable is the one we most want to understand/explain/predict. In the models we create in later chapters, this variable will go on the left-hand side of our mathematical equations. Some academic fields refer to this as the “dependent variable.” Others use terms like “response” or “outcome.” Whatever the terminology, we need to explore the distribution of this variable, its min/max/range, its mean and median, its standard deviation, and so on.@roas write:\n\nMost important is that the data you are analyzing should map to the research question you are trying to answer. This sounds obvious but is often overlooked or ignored because it can be inconvenient. Optimally, this means that the outcome measure should accurately reflect the phenomenon of interest, the model should include all relevant predictors, and the model should generalize to the cases to which it will be applied.\n\n\nFor example, with regard to the outcome variable, a model of incomes will not necessarily tell you about patterns of total assets. A model of test scores will not necessarily tell you about child intelligence or cognitive development. …\n\nWe care about other variables as well, especially those that are most correlated/connected with the outcome variable. The more time that we spend looking at these variables, the more likely we are to create a useful model.\nThird, the (almost always imaginary) population is key. We need the data we want — the Preceptor Table — and the data we have to be similar enough that we can consider them as all having come from the same statistical population. From Wikipedia:\n\nIn statistics, a population is a set of similar items or events which is of interest for some question or experiment. A statistical population can be a group of existing objects (e.g. the set of all stars within the Milky Way galaxy) or a hypothetical and potentially infinite group of objects conceived as a generalization from experience (e.g. the set of all opening hands in all the poker games in Las Vegas tomorrow).\n\nMechanically, assuming that the Preceptor Table and the data are drawn from the same population is the same thing as “stacking” the two on top of each other. For that to make sense, the variables must mean the same thing — at least mostly — in both cases. This is the assumption of validity.\nIf we assume that the data we have is drawn from the same population as the data in the Preceptor Table is drawn from, then we can use information about the former to make inferences about the latter. We can combine the Preceptor Table and the data into a single Population Table. If we can’t do that, if we can’t assume that the two sources come from the same population, then we can’t use our data to answer our questions. The heart of Wisdom is knowing when to walk away. As John Tukey noted:\n\nThe combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cardinal Virtues</span>"
    ]
  },
  {
    "objectID": "04-cardinal-virtues.html#justice",
    "href": "04-cardinal-virtues.html#justice",
    "title": "\n4  Cardinal Virtues\n",
    "section": "\n4.3 Justice",
    "text": "4.3 Justice\n\n\n\n\nJustice.\n\n\n\n\nJustice concerns four topics: the Population Table, stability, representativeness, and unconfoundedness.\n\nThe Population Table includes a row for each unit/time combination in the underlying population from which both the Preceptor Table and the data are drawn. It can be constructed if the validity assumption is (mostly) true. It includes all the rows from the Preceptor Table. It also includes the rows from the data set. It usually has other rows as well, rows which represent unit/time combinations from other parts of the population.\nThere are three key issues to explore in any Population Table: stability, representativeness and unconfoundedness.\nStability means that the relationship between the columns in the Population Table is the same for three categories of rows: the data, the Preceptor Table, and the larger population from which both are drawn.\nNever forget the temporal nature of almost all real data science problems. Our Preceptor Table will focus on rows for today or for the near future. The data we have will always be from before now. We must almost always assume that the future will be like the past in order to use data from the past to make predictions about the future.\nRepresentativeness, or the lack thereof, concerns two relationship, among the rows in the Population Table. The first is between the Preceptor Table and the other rows. The second is between our data and the other rows. Ideally, we would like both the Preceptor Table and our data to be random samples from the population. Sadly, this is almost never the case.\nValidity is about the columns in our Population Table. Stability and representativeness are about the rows.\nUnconfoundedness means that the treatment assignment is independent of the potential outcomes, when we condition on pre-treatment covariates. This assumption is only relevant for causal models. We describe a model as “confounded” if this is not true. The easiest way to ensure unconfoundedness is to assign treatment randomly.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cardinal Virtues</span>"
    ]
  },
  {
    "objectID": "04-cardinal-virtues.html#courage",
    "href": "04-cardinal-virtues.html#courage",
    "title": "\n4  Cardinal Virtues\n",
    "section": "\n4.4 Courage",
    "text": "4.4 Courage\n\n\n\n\nCourage.\n\n\n\n\nCourage begins with the exploration and testing of different models. It concludes with the creation of the Data Generating Mechanism.\n\nCourage begins by a discussion of the functional form we will be using. This is usually straight-forward because it follows directly from the type of the outcome variable: continuous implies a linear model, binary implies logistic, and more than two categories suggests multinomial logistic. We provide the mathematical formula for this model, using y and x as variables. The rest of the discussion is broken up into three sections: “Models,” “Tests,” and “Data Generating Mechanism.”\nCourage requires math.\nThe three languages of data science are words, math and code, and the most important of these is code.\nWe need to explain the structure of our model using all three languages, but we need Courage to implement the model in code.\nCourage requires us to take the general mathematical formula and then make it specific. Which variables should we include in the model and which do we exclude? Every data science project involves the creation of several models, each with one or more unknown parameters.\nCode allows us to “fit” a model by estimating the values of the unknown parameters. Sadly, we can never know the true values of these parameters. But, like all good statisticians, we can express our uncertain knowledge in the form of posterior probability distributions. With those distributions, we can compare the actual values of the outcome variable with the “fitted” or “predicted” results of the model. We can examine the “residuals,” the difference between the fitted and actual values.\nA parameter is something which does not exist in the real world. (If it did, or could, then it would be data.) Instead, a parameter is a mental abstraction, a building block which we will use to to help us accomplish our true goal: To replace at least some of the questions marks in the actual Preceptor Table. Since parameters are mental abstractions, we will always be uncertain as to their value, however much data we might collect.\nRandomness is intrinsic to this fallen world.\nNull hypothesis testing is a mistake. There is only the data, the models and the summaries therefrom.\nThe final step of Courage is to select the final model, the Data Generating Mechanism.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cardinal Virtues</span>"
    ]
  },
  {
    "objectID": "04-cardinal-virtues.html#temperance",
    "href": "04-cardinal-virtues.html#temperance",
    "title": "\n4  Cardinal Virtues\n",
    "section": "\n4.5 Temperance",
    "text": "4.5 Temperance\n\n\n\n\nTemperance.\n\n\n\n\nTemperance uses the Data Generating Mechanism to answer the specific question with which we began. Humility reminds us that this answer is always a lie. We can also explore the general question by using the DGM to calculate many similar quantities of interest, displaying the results graphically.\n\nThere are few more important concepts in statistics and data science than the Data Generating Mechanism. Our data — the data that we collect and see — has been generated by the complexity and confusion of the world. God’s own mechanism has brought His data to us. Our job is to build a model of that process, to create, on the computer, a mechanism which generates fake data consistent with the data which we see. With that DGM, we can answer any question which we might have. In particular, with the DGM, we provide predictions of data we have not seen and estimates of the uncertainty associated with those predictions. We can fill in the missing values in the Preceptor Table and then, easily, calculate all Quantities of Interest.\nJustice gave us the Population Table. Courage created the DGM, the fitted model. Temperance will guide us in its use.\nHaving created (and checked) a model, we now use the model to answer questions. Models are made for use, not for beauty. The world confronts us. Make decisions we must. Our decisions will be better ones if we use high quality models to help make them.\nSadly, our models are never as good as we would like them to be. First, the world is intrinsically uncertain.\n\n\n\n\nDonald Rumsfeld.\n\n\n\n\nThere are known knowns. There are things we know we know. We also know there are known unknowns. That is to say, we know there are some things we do not know. But there are also unknown unknowns, the ones we do not know we do not know. – Donald Rumsfeld\n\nWhat we really care about is data we haven’t seen yet, mostly data from tomorrow. But what if the world changes, as it always does? If it doesn’t change much, maybe we are OK. If it changes a lot, then what good will our model be? In general, the world changes some. That means that our forecasts are more uncertain that a naive use of our model might suggest.\nIn Temperance, the key distinction is between the true posterior distribution — what we will call “Preceptor’s Posterior” — and the estimated posterior distribution. Recall our discussion from Section 1.1. Imagine that every assumption we made in Wisdom and Justice were correct, that we correctly understand every aspect of how the world works. We still would not know the unknown value we are trying to estimate — recall the Fundamental Problem of Causal Inference — but the posterior we created would be perfect. That is Preceptor’s Posterior. Sadly, even if our estimated posterior is, very close to Preceptor’s Posterior, we can never be sure of that fact, because we can never know the truth, never be certain that all the assumptions we made are correct.\nEven worse, we must always worry that our estimated posterior, despite all the work we put into creating it, is far from the truth. We, therefore, must be cautious in our use of that posterior, humble in our claims about its accuracy. Using our posterior, despite its faults, is better than not using it. Yet it is, at best, a distorted map of reality, a glass through which we must look darkly. Use your posteriors with humility.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cardinal Virtues</span>"
    ]
  },
  {
    "objectID": "07-election-1992.html",
    "href": "07-election-1992.html",
    "title": "\n5  1992 Election\n",
    "section": "",
    "text": "5.1 Introduction\nPackages:\nlibrary(tidyverse)\nlibrary(primer.data)\nlibrary(tidymodels)\nlibrary(broom)\nlibrary(marginaleffects)\nlibrary(easystats)\nThe primer.data package includes nes, a tibble with the data from the American National Election Studies.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>1992 Election</span>"
    ]
  },
  {
    "objectID": "07-election-1992.html#introduction",
    "href": "07-election-1992.html#introduction",
    "title": "\n5  1992 Election\n",
    "section": "",
    "text": "5.1.1 Scenarios\n\nWhat was the relationship between sex and voting in the 1992 US Presidential election among supporters of the three leading candidates: Clinton, Bush and Perot?\n\nThis is the first scenario. The 1992 U.S. Presidential election provides a rich opportunity to explore how demographic characteristics, like sex, influenced voter behavior. By examining the relationship between sex and support for the three major candidates—Bill Clinton, George H. W. Bush, and Ross Perot—we can ask whether men and women showed different patterns of candidate preference. Did one gender lean more Democratic or Republican? Was Perot’s independent appeal stronger among men than women? Understanding these differences helps us assess how social identity may shape political choice.\n\nDid a voter’s sex cause them to be more likely to vote for Clinton, Bush, or Perot in the 1992 U.S. Presidential election?\n\nThis question focuses on whether voting preferences differed systematically between men and women. Were male voters more drawn to one candidate than female voters, or vice versa? By looking at patterns in the data, we can begin to understand how gender may have shaped political preferences in this particular election.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>1992 Election</span>"
    ]
  },
  {
    "objectID": "07-election-1992.html#wisdom",
    "href": "07-election-1992.html#wisdom",
    "title": "\n5  1992 Election\n",
    "section": "\n5.2 Wisdom",
    "text": "5.2 Wisdom\n\n\n\n\n\n\n\n\nWisdom begins with a question and then moves on to the creation of a Preceptor Table and an examination of our data.\n\n5.2.1 EDA\nAs (almost) always, we start by loading the tidyverse package and the primer.data package.\n\nlibrary(tidyverse)\nlibrary(primer.data)\n\nOur data comes from American National Election Studies. The primer.data package includes a version of the main data set with a selection of variables. The full ANES data is much richer than this relatively simple tibble.\n\nnes\n\n# A tibble: 52,359 × 19\n    year state sex    income    age education    race  ideology ideology_numeric\n   &lt;int&gt; &lt;chr&gt; &lt;chr&gt;  &lt;fct&gt;   &lt;int&gt; &lt;fct&gt;        &lt;chr&gt; &lt;fct&gt;               &lt;dbl&gt;\n 1  1952 NY    Female 68 - 95    25 Highschool   White Weak Re…                6\n 2  1952 NY    Female 68 - 95    33 Elementary   White Indepen…                5\n 3  1952 NY    Female 34 - 67    26 Highschool   White Indepen…                4\n 4  1952 NY    Male   34 - 67    63 Some Highsc… White Strong …                7\n 5  1952 OH    Female 0 - 16     66 Highschool + White Strong …                7\n 6  1952 OH    Female 68 - 95    48 Some Highsc… White Indepen…                3\n 7  1952 ID    Female 0 - 16     70 Elementary   White Indepen…                4\n 8  1952 MI    Male   17 - 33    25 Highschool + Black Weak De…                2\n 9  1952 GA    Female 0 - 16     30 Some Highsc… White &lt;NA&gt;                   NA\n10  1952 OH    Female 68 - 95    35 Elementary   White Weak Re…                6\n# ℹ 52,349 more rows\n# ℹ 10 more variables: voted &lt;chr&gt;, region &lt;fct&gt;, pres_appr &lt;chr&gt;,\n#   influence &lt;chr&gt;, equality &lt;chr&gt;, religion &lt;chr&gt;, better_alone &lt;chr&gt;,\n#   therm_black &lt;int&gt;, therm_white &lt;int&gt;, pres_vote &lt;chr&gt;\n\n\nSee the ?nes for details. We only need a small subset of this data:\n\nnes_92 &lt;- nes |&gt; \n  filter(year == 1992) |&gt; \n  select(sex, pres_vote, race, education) |&gt; \n  drop_na() |&gt; \n  mutate(pres_vote = case_when(\n    pres_vote == \"Democrat\" ~ \"Clinton\",\n    pres_vote == \"Republican\" ~ \"Bush\",\n    pres_vote == \"Third Party\" ~ \"Perot\",\n  )) |&gt; \n    mutate(pres_vote = as.factor(pres_vote))\n\n\nnes_92\n\n# A tibble: 1,612 × 4\n   sex    pres_vote race  education      \n   &lt;chr&gt;  &lt;fct&gt;     &lt;chr&gt; &lt;fct&gt;          \n 1 Female Bush      White Highschool     \n 2 Female Bush      White Some College   \n 3 Female Clinton   Black Some Highschool\n 4 Male   Bush      White College        \n 5 Female Clinton   White Highschool     \n 6 Female Clinton   White Some College   \n 7 Female Perot     White Some College   \n 8 Male   Bush      White Some College   \n 9 Female Bush      White Adv. Degree    \n10 Male   Perot     White Highschool     \n# ℹ 1,602 more rows\n\n\nIt is worth thinking about the quality of this data. How did NES, how do we, know for whom a surveyed individual voted? The answer is that we don’t. NES does not check the voting records and, even if it did, it could not determine a person’s vote.\nAs always, visualization is a useful tool. Plot your data! The outcome variable usually goes on the y-axis.\n\nnes_92 |&gt; \n  ggplot(aes(x = pres_vote, fill = sex)) +\n    geom_bar(position = \"dodge\") +\n    labs(title = \"Survey of 1992 Presidential Election Votes\",\n         subtitle = \"Men were much more likely to support Ross Perot\",\n         x = NULL,\n         y = \"Count\",\n         caption = \"Source: American National Election Survey\")\n\n\n\n\n\n\n\n\n5.2.2 Preceptor Table\nA Preceptor Table is the smallest possible table of data with rows and columns such that, if there is no missing data, we can easily calculate the quantity of interest.\n\n5.2.2.1 Scenario 1\nUnits: The rows of the Preceptor Table refer to individual US voters. The question suggests that we are not interested in people who did not vote, although one might explore if men were more or less likely to vote in the first place.\nOutcome: We want Presidential voting behavior in 1992. Such explorations are often restricted to just the two “major party” candidates, the nominees of the Democratic and the Republican parties, Bill Clinton and George HW Bush. But, in 1992, Ross Perot was a very successful “third party” candidate, winning almost 19% of the vote. Should he be included in the analysis? What about the 4th place finisher, Libertarian Andre Marrou? Again, the question does not specify. However, the outcome variable is certainly the candidate for whom an individual voted.\nCausal or predictive model: Predictive.\nTreatment: There are no treatments since this model is predictive.\nCovariates: The only covariate we will consider is sex, with two values: “male” and “female”. Whatever your feelings about the contemporary limits of the sex binary, this is the only data collected in 1992s. In a full analysis, we would make use of more covariates, but the primary purpose of this chapter is to explore a categorical model with three possible outcomes.\nMoment in Time: The Preceptor Table refers to the election season in the fall of 1992. We make no claims about other US presidential elections, not least because few feature a third party candidate as popular as Perot. Our purpose here is historical. We want to better understand what happened in this election, not make claims about other time periods. The moment in time is, however, longer than just Election Day itself since mail-in votes were cast in the weeks proceeding the election.\n\n\n\n\n\n\n\n\n\n\n\n\nPreceptor Table\n\n\nID\n\nOutcome\n\n\nCovariate\n\n\n\nVote\nSex\n\n\n\n\n1\nDemocrat\nM\n\n\n2\nThird Party\nF\n\n\n…\n…\n…\n\n\n10\nRepublican\nF\n\n\n11\nDemocrat\nF\n\n\n…\n…\n…\n\n\n103,754,865\nRepublican\nM\n\n\n\n\n\n\n\n5.2.2.2 Scenario 2\nUnits: Individual US voters.\nOutcome: The presidential voting result of each individual voter.\nCausal or predictive model: Causal.\nTreatment: sex. Since we are studying how sex influences different voting behaviors, we must manipulate sex to see the outcomes.\nCovariates:\nMoment in Time: The election season in the fall of 1992",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>1992 Election</span>"
    ]
  },
  {
    "objectID": "07-election-1992.html#justice",
    "href": "07-election-1992.html#justice",
    "title": "\n5  1992 Election\n",
    "section": "\n5.3 Justice",
    "text": "5.3 Justice\n\n\n\n\n\n\n\n\nJustice concerns the Population Table and the four key assumptions which underlie it: validity, stability, representativeness, and unconfoundedness.\n\n5.3.1 Validity\nValidity is the consistency, or lack thereof, in the columns of the data set and the corresponding columns in the Preceptor Table. In order to consider the two data sets to be drawn from the same population, the columns from one must have a valid correspondence with the columns in the other. Validity, if true (or at least reasonable), allows us to construct the Population Table, which is the central result of Justice.\nOur Preceptor Table includes the 103,754,865 people who voted for one of the three leading candidates in the 1992 presidential election. Our data includes a (tiny) subset of those people. This is a standard aspect of a “historical” data science project, when the data we want (the Preceptor Table) and the data we have come from the same moment in time. This makes the assumptions of validity and stability much easier to maintain. Of course, there can always be problems. A response to the nice ANES surveyer about one’s vote (or sex) is not the same thing as one’s actual vote (or sex). Indeed, a standard artifact of political surveys is that more people claim to have voted for the winning candidate than actually did. In this case, however, we will assume that validity holds and that we can “stack” the columns from the Preceptor Table and the data on top of each other. In fact, validity allows us to assume that the rows in the data is actually a subset of the rows in the Preceptor Table. To summarize:\n\nUsing data from the National Election Studies survey of US citizens, we seek to understand the relationship between voter preference and sex in the 1992 Presidential election.\n\n\n5.3.2 Population Table\nThe Population Table includes a row for each unit/time combination in the underlying population from which both the Preceptor Table and the data are drawn.\n\n5.3.2.1 Scenario 1\nBecause this project is historical, the Population Table has the same number of rows as the Preceptor Table.\n\n\n\n\n\n\n\n\n\n\n\n\n\nPopulation Table\n\n\nSource\nID\n\nOutcome\n\n\nCovariate\n\n\n\nVote\nSex\n\n\n\n\nPT/Data\n1\nDemocrat\nM\n\n\nPT/Data\n2\nThird Party\nF\n\n\nPT\n3\nRepublican\nM\n\n\nPT\n4\nDemocrat\nF\n\n\nPT\n5\nDemocrat\nF\n\n\nPT\n6\nDemocrat\nM\n\n\n…\n…\n…\n…\n\n\nPT/Data\n10\nRepublican\nF\n\n\nPT/Data\n11\nDemocrat\nF\n\n\nPT\n12\nDemocrat\n…\n\n\nPT\n13\nRepublican\nF\n\n\n…\n…\n…\n…\n\n\nPT/Data\n103,754,865\nRepublican\nM\n\n\n\n\n\n\nWe are not interested in these voters outside of the 1992 election. So, this Population Table, unlike most, does not require a Time column. The Preceptor Table and the data come from the same moment in time. Of course, this is not literally true. Recall: The Time column is always a lie. Some voters cast their ballots weeks before Election Day. Some NES participants were surveyed right after the election. Some were survey later. We sweep all these complications under the mythical moment in time which we assert is the same for both the data and the Preceptor Table.\n\n5.3.2.2 Scenario 2\n\n\n5.3.3 Stability\nStability means that the relationship between the columns in the Population Table is the same for three categories of rows: the data, the Preceptor Table, and the larger population from which both are drawn.\nIf the assumption of stability holds, then the relationship between the columns in the Population Table is the same for three categories of rows: the data, the Preceptor Table, and the larger population from which both are drawn. In this case, there is no larger population. Or, rather, the Preceptor Table is the larger population. And, since the data and the Preceptor Table come from the same moment in time, stability holds by definition.\n\n5.3.4 Representativeness\nRepresentativeness, or the lack thereof, concerns two relationships among the rows in the Population Table. The first is between the data and the other rows. The second is between the other rows and the Preceptor Table. Ideally, we would like both the Preceptor Table and our data to be random samples from the population. Sadly, this is almost never the case.\nThe NES is a highly professional organization so their survey does a good of capturing a random sample of all voters. But no real world survey is perfect! There are always problems. In particular, there are (somewhat) unobserved differences between the sort of people who respond to surveys and the sort of people who don’t. NES is more likely to capture the votes of cooperative people than the votes of misanthropes. If, among misanthropes, the relationship between sex and Presidential candidate is different than the relationship among cooperative voters, we might have problems. The technical term for the problem of people who don’t respond to surveys is “non-response.”\n\n5.3.5 Unconfoundedness\nUnconfoundedness means that the treatment assignment is independent of the potential outcomes, when we condition on pre-treatment covariates. This assumption is only relevant for causal models. We describe a model as “confounded” if this is not true. The easiest way to ensure unconfoundedness is to assign treatment randomly.\nSince Scenario 1 is a predictive model, unconfoundedness can only apply to scenario 2.\n\n5.3.6 Mathematics\nThe link function, which defines the mathematical form of a regression model, is largely determined by the type of outcome variable. For a binary outcome, we typically use a binomial logistic model. But when our outcome variable has three or more unordered categories, we turn to the multinomial logistic regression model — sometimes also called the categorical model.\nWe start with a simple three-outcome version of the model:\n\\[\n\\begin{aligned}\n\\rho_{A} &= \\frac{e^{\\beta_{0, A} + \\beta_{1, A} \\cdot X}}{1 + e^{\\beta_{0, A} + \\beta_{1, A} \\cdot X} + e^{\\beta_{0, B} + \\beta_{1, B} \\cdot X}} \\\\\\\\\n\\rho_{B} &= \\frac{e^{\\beta_{0, B} + \\beta_{1, B} \\cdot X}}{1 + e^{\\beta_{0, A} + \\beta_{1, A} \\cdot X} + e^{\\beta_{0, B} + \\beta_{1, B} \\cdot X}} \\\\\\\\\n\\rho_{C} &= 1 - \\rho_{A} - \\rho_{B}\n\\end{aligned}\n\\]\nEach category (A, B, or C) gets its own linear predictor (a combination of coefficients and covariates), and the probabilities are constructed so that they all fall between 0 and 1 and sum to 1.\nThis structure generalizes naturally to more than three outcomes and more than one covariate. The general form of the multinomial logistic regression model is:\n\\[\nP(Y = k) = \\frac{e^{\\beta_{k0} + \\beta_{k1} X_1 + \\beta_{k2} X_2 + \\cdots + \\beta_{kn} X_n}}{\\sum_{j=1}^{K} e^{\\beta_{j0} + \\beta_{j1} X_1 + \\beta_{j2} X_2 + \\cdots + \\beta_{jn} X_n}}\n\\]\nwith\n\\[\nY \\sim \\text{Multinomial}(\\boldsymbol{\\rho}) \\quad \\text{where} \\quad \\boldsymbol{\\rho} = (\\rho_1, \\rho_2, \\ldots, \\rho_K)\n\\]\nand each \\(\\rho_k\\) corresponds to the predicted probability for category \\(k\\).\nEach outcome class \\(k\\) has its own set of regression parameters \\(\\beta_{k0}, \\beta_{k1}, \\ldots, \\beta_{kn}\\), allowing the model to capture distinct patterns of association between the covariates and each outcome.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>1992 Election</span>"
    ]
  },
  {
    "objectID": "07-election-1992.html#courage",
    "href": "07-election-1992.html#courage",
    "title": "\n5  1992 Election\n",
    "section": "\n5.4 Courage",
    "text": "5.4 Courage\n\n\n\n\n\n\n\n\nCourage creates the data generating mechanism.\n\n5.4.1 Models\nBecause our outcome variable has more than two unordered categories, we use a multinomial logistic regression model, using multinom_reg(engine = \"nnet\").\nMultinomial logistic regression estimates the probability of each outcome category relative to a reference. In our case, the outcome pres_vote has three categories: Clinton, Bush, and Perot. The model uses one of these as a baseline (e.g., Bush), and then estimates how predictors such as sex influence the odds of choosing one of the other candidates relative to that baseline.\nWe fit the model using the tidymodels framework, using only sex as a predictor:\n\nfit_sex &lt;- multinom_reg(engine = \"nnet\") |&gt;\n  fit(pres_vote ~ sex, data = nes_92)\n\nCategorical predictors like sex are automatically converted into 0/1 dummy variables. If sex includes levels “Male” and “Female”, then the model creates a dummy variable like sexMale, with “Male” as the reference category. The model estimates one set of coefficients for each non-reference outcome, so we get two equations: one comparing Clinton vs. Bush and one comparing Perot vs. Bush.\nAfter fitting the model, we summarize it with\n\ntidy(fit_sex, conf.int = TRUE)\n\n# A tibble: 4 × 8\n  y.level term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 Clinton (Intercept)    0.477    0.0757      6.30 2.90e-10    0.329    0.625 \n2 Clinton sexMale       -0.267    0.113      -2.37 1.77e- 2   -0.488   -0.0465\n3 Perot   (Intercept)   -0.858    0.109      -7.88 3.39e-15   -1.07    -0.644 \n4 Perot   sexMale        0.429    0.147       2.92 3.48e- 3    0.141    0.717 \n\n\nThis returns estimates and 95% confidence intervals for the model’s parameters. These coefficients are on the log-odds scale and tell us how being male (vs. female) affects the odds of voting for Clinton or Perot, relative to Bush.\n\n\nOutcome\nTerm\nEstimate\n2.5%\n97.5%\n\n\n\nClinton\n(Intercept)\n0.45\n0.21\n0.69\n\n\nClinton\nsexMale\n-0.25\n-0.49\n-0.01\n\n\nPerot\n(Intercept)\n-0.85\n-1.10\n-0.60\n\n\nPerot\nsexMale\n0.42\n0.17\n0.67\n\n\n\nEach coefficient represents a log-odds difference. For example: - The intercept of 0.45 for Clinton indicates that, among female voters (since sexMale = 0), the log-odds of voting for Clinton rather than Bush is 0.45. - The sexMale coefficient of -0.25 for Clinton indicates that males have lower log-odds of voting for Clinton relative to Bush compared to females, by 0.25 log-odds units. - For Perot, the intercept of -0.85 suggests that females had a lower baseline likelihood of voting for Perot than Bush. - The sexMale coefficient of 0.42 for Perot implies that males were more likely than females to vote for Perot rather than Bush.\nWe then study race individually:\n\nfit_race &lt;- multinom_reg(engine = \"nnet\") |&gt;\n  fit(pres_vote ~ race, data = nes_92)\n\nIn this model, we include only race as a predictor of vote choice. Race is a categorical variable (e.g., White, Black, Other), so the model creates dummy variables for each level, using one group—such as Black—as the reference category. The model estimates how racial identity influences the likelihood of voting for Clinton or Perot relative to Bush.\nBecause sex is not included in this model, the estimates reflect average differences in vote choice across racial groups, without accounting for potential gender effects. For instance, the model might show that White voters were significantly less likely than Black voters to support Clinton over Bush, or more likely to vote for Perot rather than Bush. This provides insight into how voting behavior in 1992 varied by race, on average, across the electorate.\nWe can then summarize it with tidy, this time on fit_race.\n\ntidy(fit_race, conf.int = TRUE)\n\n# A tibble: 10 × 8\n   y.level term          estimate std.error statistic p.value conf.low conf.high\n   &lt;chr&gt;   &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Clinton (Intercept)     -0.693     0.548    -1.27  2.06e-1   -1.77     0.380 \n 2 Clinton raceBlack        3.61      0.646     5.59  2.32e-8    2.34     4.87  \n 3 Clinton raceHispanic     1.29      0.588     2.20  2.77e-2    0.142    2.45  \n 4 Clinton raceNative A…    1.20      0.913     1.32  1.87e-1   -0.585    2.99  \n 5 Clinton raceWhite        0.794     0.551     1.44  1.50e-1   -0.286    1.87  \n 6 Perot   (Intercept)     -1.61      0.775    -2.08  3.77e-2   -3.13    -0.0914\n 7 Perot   raceBlack        0.799     0.980     0.815 4.15e-1   -1.12     2.72  \n 8 Perot   raceHispanic     0.386     0.854     0.452 6.51e-1   -1.29     2.06  \n 9 Perot   raceNative A…    1.20      1.20      1.01  3.15e-1   -1.14     3.55  \n10 Perot   raceWhite        1.03      0.778     1.32  1.88e-1   -0.500    2.55  \n\n\nThis command returns the estimated coefficients and 95% confidence intervals for each term in the multinomial logistic regression model that includes race as a predictor. The output shows how belonging to a particular racial group, compared to the reference group (e.g., Black voters), affects the log-odds of voting for Clinton or Perot relative to Bush.\nEach row in the output corresponds to one term in one of the two comparisons: Clinton vs. Bush and Perot vs. Bush. The intercept represents the baseline log-odds of voting for each candidate among the reference group (e.g., Black voters), while the race terms indicate how those log-odds shift for White and Other racial groups. For example, a negative coefficient for raceWhite in the Clinton row would suggest that White voters had lower odds of voting for Clinton (vs. Bush) than Black voters.\nWe finally add sex to race as a final predictor:\n\nfit_sex_race &lt;- multinom_reg(engine = \"nnet\") |&gt;\n  fit(pres_vote ~ sex + race, data = nes_92)\n\nIn this model, we include both sex and race as predictors of vote choice. Race is a categorical variable (e.g., White, Black, Other), so the model creates dummy variables for each level, with one group as the reference (e.g., Black). The model now tells us how sex affects vote choice within each racial group, along with how race affects vote choice, controlling for sex.\nBy including both variables, we can see whether racial identity influenced voting patterns independently of gender. For example, this model might show that White voters were more likely to vote for Perot relative to Bush than Black voters, even after accounting for sex.\nWe can now summarize it again with tidy, this time on fit_sex_race.\n\ntidy(fit_sex_race, conf.int = TRUE)\n\n# A tibble: 12 × 8\n   y.level term          estimate std.error statistic p.value conf.low conf.high\n   &lt;chr&gt;   &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Clinton (Intercept)     -0.528     0.555    -0.951 3.41e-1  -1.62      0.560 \n 2 Clinton sexMale         -0.220     0.118    -1.86  6.23e-2  -0.451     0.0113\n 3 Clinton raceBlack        3.53      0.647     5.46  4.75e-8   2.27      4.80  \n 4 Clinton raceHispanic     1.21      0.590     2.06  3.94e-2   0.0589    2.37  \n 5 Clinton raceNative A…    1.08      0.916     1.18  2.39e-1  -0.716     2.87  \n 6 Clinton raceWhite        0.729     0.553     1.32  1.87e-1  -0.354     1.81  \n 7 Perot   (Intercept)     -1.96      0.785    -2.50  1.24e-2  -3.50     -0.424 \n 8 Perot   sexMale          0.439     0.148     2.97  2.96e-3   0.149     0.728 \n 9 Perot   raceBlack        0.933     0.982     0.950 3.42e-1  -0.993     2.86  \n10 Perot   raceHispanic     0.532     0.857     0.621 5.34e-1  -1.15      2.21  \n11 Perot   raceNative A…    1.45      1.20      1.21  2.27e-1  -0.905     3.81  \n12 Perot   raceWhite        1.14      0.780     1.47  1.43e-1  -0.386     2.67  \n\n\nThis returns estimates and 95% confidence intervals for the model’s parameters. These coefficients are on the log-odds scale and tell us how being male (vs. female) and being white (vs. Black) affect the odds of voting for Clinton or Perot, relative to Bush.\n\n\nOutcome\nTerm\nEstimate\n2.5%\n97.5%\n\n\n\nClinton\n(Intercept)\n0.60\n0.35\n0.86\n\n\nClinton\nsexMale\n-0.22\n-0.46\n0.02\n\n\nClinton\nraceWhite\n-0.50\n-0.77\n-0.24\n\n\nPerot\n(Intercept)\n-0.70\n-0.94\n-0.46\n\n\nPerot\nsexMale\n0.39\n0.15\n0.63\n\n\nPerot\nraceWhite\n0.28\n0.02\n0.53\n\n\n\nEach coefficient represents a log-odds difference, holding the other predictor constant. For Clinton, the intercept of 0.60 tells us that among Black female voters (the reference levels for both variables), the log-odds of voting for Clinton rather than Bush is 0.60. The sexMale coefficient of -0.22 means that male voters have slightly lower log-odds of voting for Clinton over Bush compared to females, controlling for race. The raceWhite coefficient of -0.50 indicates that white voters were less likely than Black voters to vote for Clinton over Bush, adjusting for sex. For Perot, the sexMale coefficient of 0.39 shows that men were more likely than women to vote for Perot rather than Bush. The raceWhite coefficient of 0.28 suggests that white voters were also more likely than Black voters to vote for Perot rather than Bush.\n\n5.4.2 Posterior Predictive Checks\nBecause our model is not Bayesian, we do not have full posterior distributions. But we can still ask: how well does our model reproduce the patterns in the data?\nTo answer this, we compare our model’s predicted probabilities to the actual proportions in the data. If the predicted values are close to the observed ones, that’s a good sign. If not, the model may be missing something important.\nFor instance, we might calculate the average predicted probability of voting for Clinton, Bush, or Perot within each sex group. We then compare those predicted probabilities to the actual proportions of each vote choice among males and females. These checks help confirm whether our fitted model provides a reasonable approximation of reality.\nThis approach is more qualitative than quantitative, but it’s a crucial step in understanding what our model is doing—and whether we believe it.\n\n5.4.3 Testing\nTo interpret the coefficients from the model, we begin with the intercepts: these represent the log-odds of voting for Clinton or Perot (relative to Bush) for the reference group (females). The coefficients for sexMale tell us how those log-odds change for males. A negative coefficient means males are less likely than females to vote for that candidate (compared to Bush), while a positive coefficient means they are more likely.\nImportantly, each coefficient corresponds to a separate binary comparison. For example, the model estimates how sex influences the odds of voting for Clinton vs. Bush and separately how it influences the odds of voting for Perot vs. Bush. There is no single parameter that describes how sex affects the odds of voting for Bush directly — rather, Bush acts as the comparison point.\nAfter fitting and tidying the model, we examine the magnitude, direction, and statistical uncertainty (via confidence intervals) of each coefficient. This helps us assess whether sex had a statistically and practically significant effect on vote choice in 1992.\nTo interpret these coefficients, remember that each one corresponds to a specific comparison relative to the reference category (Bush).\n\n\nClinton vs. Bush: The negative coefficient for sexMale (–0.25) means that males were less likely than females to vote for Clinton over Bush.\n\nPerot vs. Bush: The positive coefficient for sexMale (+0.42) means that males were more likely than females to vote for Perot over Bush.\n\nThe size of these coefficients helps us understand the magnitude of the difference: - A coefficient of –0.25 for Clinton translates into an odds ratio of approximately exp(–0.25) ≈ 0.78, meaning males had 22% lower odds than females of voting for Clinton (vs. Bush). - A coefficient of +0.42 for Perot gives an odds ratio of exp(0.42) ≈ 1.52, meaning males had 52% higher odds than females of voting for Perot (vs. Bush).\nThese results support the idea that sex had a measurable and statistically significant impact on vote choice in the 1992 election. But this model is still simple—real voting behavior is likely influenced by many other variables. This model serves as a clear and interpretable first step.\n\n5.4.4 Data Generating Mechanism\nPutting the mathematics together with the parameter estimates gives us the data generating mechanism\n\\[ vote_i  \\sim Categorical(\\rho_{bush}, \\rho_{clinton}, \\rho_{perot}) \\]\n\\[\n\\begin{aligned}\n\\rho_{bush}  &=& 1 - \\rho_{clinton} - \\rho_{perot}\\\\\n\\rho_{clinton} &=& \\frac{e^{0.45 - 0.25 male}}{1 + e^{0.45 - 0.25 male}}\\\\\n\\rho_{perot} &=& \\frac{e^{-0.86 + 0.42 male}}{1 + e^{-0.86 + 0.42 male}}\\\\\n\\end{aligned}\n\\] This is the last time we are going to go to the trouble of combining the mathematical formula of the DGM with the specific estimated values. First, these formulas are misleading! The value of \\(\\beta_{0, clinton}\\), for example, is not exactly 0.45. In fact, we don’t know what the value is! Being Bayesians, we calculated a posterior probability distribution for \\(\\beta_{0, clinton}\\). We draw from that distribution when we calculate quantities of interest, as we did above with add_epred_draws(). Second, you probably did not even look that closely at these complex formulas. And we don’t blame you! Being a good race car driver means focussing on how to drive better, not on the physics of carburetors. Similar, being a good data scientist means focusing on the Cardinal Virtues as a method for using data to answer questions. Leave the math to the computer.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>1992 Election</span>"
    ]
  },
  {
    "objectID": "07-election-1992.html#temperance",
    "href": "07-election-1992.html#temperance",
    "title": "\n5  1992 Election\n",
    "section": "\n5.5 Temperance",
    "text": "5.5 Temperance\n\n\n\n\n\n\n\n\nTemperance uses the data generating mechanism to answer the question with which we began. Humility reminds us that this answer is always a lie. We can also use the DGM to calculate many similar quantities of interest, displaying the results graphically.\n\n5.5.1 Questions and Answers\nRecall the questions with which we began:\n\nWhat was the relationship between sex and voting in the 1992 US Presidential election among supporters of the three leading candidates: Clinton, Bush and Perot?\n\n\nDid a voter’s sex cause them to be more likely to vote for Clinton, Bush, or Perot in the 1992 U.S. Presidential election?\n\nWe answer these questions by using plot_predictions:\n\nplot_predictions(fit_sex_race,\n                 by = \"sex\", \n                 type = \"prob\",\n                 draw = FALSE)\n\n    group    sex  estimate  std.error statistic       p.value  s.value\n1    Bush Female 0.3294549 0.01559268  21.12881  4.322640e-99 326.7590\n2    Bush   Male 0.3466157 0.01696772  20.42795  9.438735e-93 305.7007\n3 Clinton Female 0.5308494 0.01613187  32.90687 1.752638e-237 786.4874\n4 Clinton   Male 0.4276229 0.01703860  25.09729 5.323373e-139 459.3357\n5   Perot Female 0.1396957 0.01169067  11.94933  6.544824e-33 106.9133\n6   Perot   Male 0.2257615 0.01501235  15.03838  4.114672e-51 167.3776\n   conf.low conf.high  df\n1 0.2988938 0.3600160 Inf\n2 0.3133595 0.3798718 Inf\n3 0.4992315 0.5624673 Inf\n4 0.3942278 0.4610179 Inf\n5 0.1167824 0.1626090 Inf\n6 0.1963378 0.2551851 Inf\n\n\nThis code returns the estimated probabilities of voting for each candidate, for male and female.\nThe intercept coefficient from the fitted model (0.4553900) represents the log-odds, whereas the predicted probabilities from plot_predictions() represent the chances of a voter choosing Clinton based on sex, transformed from those log-odds.\nOften, the best answers to broad questions are graphics. Consider:\n\nplot_predictions(fit_sex_race, \n                          by = \"sex\", \n                          type = \"prob\", \n                          draw = FALSE) |&gt; \n    ggplot(aes(x = group, y = estimate, color = sex)) +\n      geom_point(size = 3, position = position_dodge(width = 0.5)) +\n      geom_errorbar(aes(ymin = conf.low, \n                        ymax = conf.high), \n                    width = 0.2, \n                    position = position_dodge(width = 0.5)) +\n      labs(title = \"Voting Preferences by Candidate and Sex\",\n           x = NULL,\n           y = \"Estimated Proportion\",\n           color = \"Sex\") +\n      theme_minimal()\n\n\n\n\n\n\n\n\n5.5.2 Humility\nThe world is always more uncertain than our models would have us believe.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>1992 Election</span>"
    ]
  },
  {
    "objectID": "set-up.html",
    "href": "set-up.html",
    "title": "Set Up for Working on PPBDS Projects",
    "section": "",
    "text": "Computer Set Up\nThis document provides a guide in setting up R/Positron/Git/GitHub to work on The Primer, both the book itself, PPDBS/primer, and the associated tutorials (PPDBS/primer.tutorials) and data (PPDBS/primer.data) packages. The same procedures apply for working on other packages associated with the PPBDS organization: PPDBS/tutorial.helpers, PPDBS/positron.tutorials, PPDBS/r4ds.tutorials, PPDBS/tidycensus.tutorials, PPDBS/tidymodels.tutorials and others. There are three steps:\nThe test which ensures that you have successfully completed this set up is to submit a PR for the TODO.txt file in the PPBDS package (which you are working on) which adds your name at the very bottom of the file. The TODO.txt file is always located at the top of the directory structure. The PR should only change that file. If you are working on a tutorial package, you should also read about how to write tutorials, especially tutorials based on books.\nI left out the end of the output.\nIf the first part — Git config — seems messed up, execute (with your information):\nuse_git_config(user.name = \"davidkane9\", user.email = \"dave.kane@gmail.com\")\nIf the second part seems messed up, try:\nusethis::create_github_token()\nand read about Github credentials. After you do, restart R and then run git_sitrep() again to make sure that things look like mine, more or less.\nIt is not critical to understand all the details of how renv works. The big picture is that it creates a set of libraries which will be used just for this project and whose versions are kept in sync between you and me.\nAgain, with luck, you will only have to do these steps once.",
    "crumbs": [
      "Set Up for Working on PPBDS Projects"
    ]
  },
  {
    "objectID": "set-up.html#computer-set-up",
    "href": "set-up.html#computer-set-up",
    "title": "Set Up for Working on PPBDS Projects",
    "section": "",
    "text": "Read the Getting Started chapter from The Primer. Complete at least the first four tutorials in the positron.tutorials package. The best reference for R/Git/Github issues is always Happy Git and GitHub for the useR.\nMake sure that your Git/Github connections are good. If you have gone through the key chapters in Happy Git with R — as you should have — then these may already be OK. If not (or, even if you have), then you need to run usethis::git_sitrep().\n\n&gt; library(usethis)   \n&gt; git_sitrep()    \nGit config (global)   \n● Name: 'David Kane'   \n● Email: 'dave.kane@gmail.com'   \n● Vaccinated: FALSE   \nℹ See `?git_vaccinate` to learn more   \nℹ Defaulting to https Git protocol   \n● Default Git protocol: 'https'   \nGitHub   \n● Default GitHub host: 'https://github.com'   \n● Personal access token for 'https://github.com': '&lt;discovered&gt;'   \n● GitHub user: 'davidkane9'   \n● Token scopes: 'delete_repo, gist, notifications, repo, user, workflow'   \n● Email(s): 'dave.kane@gmail.com (primary)', 'dkane@fas.harvard.edu'   \n...   \n\n\n\n\n\n\n\nInstall the renv package. Read about the renv package here. Most of the PPBDS projects use renv. The most important exception is primer, which uses a DESCRIPTION file to keep track of the required R packages.\n\n\n\nAt this point, you should have all the tools you need to contribute. If you have never done a pull request, however, you will need to learn more. Start by reading the help page. Read the whole thing! Don’t just skim it. These are important concepts for professional-level workflow. The usethis package is mostly providing wrappers around the underlying git commands. If you want to understand what is happening at a lower level, read this, but doing so is optional.\n\n\n\nProve to yourself (and to me) that your set up is working by submitting a pull request to me which simply adds your name to the top of the TODO.txt file in whichever PPBDS project you are working. (See below for how to do this.) Email me to set up our next meeting after you do this.",
    "crumbs": [
      "Set Up for Working on PPBDS Projects"
    ]
  },
  {
    "objectID": "set-up.html#project-set-up",
    "href": "set-up.html#project-set-up",
    "title": "Set Up for Working on PPBDS Projects",
    "section": "Project Set Up",
    "text": "Project Set Up\nYou will need to do the below steps at least one time. It is more likely, however, that you will do them dozens of times. If things are working, great! If they start not working, you can try to diagnose the problem. But, if you can’t, then you are in a nuke it from orbit scenario, which means that you start by deleting the current version of the package from two places: your computer and your Github account. To delete a project from your computer, put the R Studio project directory in the Trash. Make sure to also close out of the R Studio session after you delete it. If for some reason you cannot completely remove it, consider using the command $sudo rm -r dirname where you replace dirname with the path to the project on your computer. sudo and rm can be extremely dangerous when used together, so make sure to double check the command and/or do additional research. After you successfully remove the R project from your computer, go to your Github account and then go to Settings to delete the repo.\nKey steps:\n\nFork/download the target repo:\n\n\nlibrary(usethis)  \ncreate_from_github(repo_spec = \"PPBDS/INSERT-NAME-OF-PROJECT\",   \n                   fork = TRUE,   \n                   destdir = \"/Users/davidkane/Desktop/projects/\",   \n                   protocol = \"https\")  \n\nObviously, you need to modify the string “INSERT-NAME-OF-PROJECT” to correspond to one of our projects. Most common options are r4ds.tutorials, primer, primer.data, and primer.tutorials. If, for some reason, you are working on a project which does not reside in the PPBDS organization, you should modify the first part of the repo_spec as well.\nYou must change destdir to be a location on your computer. Indeed, professionals will generally have several different Positron windows open, each working on a different project, each of which is connected to its own Github repo.\nFor your education, it is worth reading the help page for create_from_github(). The fork and protocal arguments may not really be necessary and, obviously, you should place the project in the location on your computer in which your other projects live. The command first forks a copy of PPBDS/primer to your Github account and then clone/downloads that fork to your computer.\nThis may seem like overkill, but, as Pro Git explains, it is how (essentially) all large projects are organized. With luck, you only have to issue this command once. After that, you are always connected, both to your fork and to the true repos, which live at github/com/PPBDS. Also, note that, if something ever gets totally messed up on your computer, you can just delete the project folder on your computer and the repo on your Github account and then start again. (If you have made changes that you don’t want to lose, just save the files with those changes to one side and then move them back after you have recreated the project.)\nNote that this command should automatically put you in a new Positron session within the primer (or primer.tutorials or primer.data) project which now resides on your computer.\n\n\nThe next step is to get renv setup so that you are running the same package versions as everyone else. Run this once:\n\n\nlibrary(renv)\nrenv::restore()\n\nThis will install all the packages you need in the directory of this project. If this command fails, most commonly with a report about problems with compiling a specific package, notify me right away. If I update the renv.lock file by using renv::update(), the problem will often go away. See the Common Problems section below for further discussion.\nrenv::restore() has no effect on your main library of R packages. Restart your R session. Again, this means that you now have two separate installations of, for example, ggplot2. One is in the default place which your R sessions is by default pointed to. (In a different project without a renv directory, you can run .libPaths() to see where that is.) The second place that ggplot2 is installed is in the renv directory associated with this project.\nNote that, for the most part, you won’t do anything with renv after this initial use. If you use error = TRUE in any code chunk, you will also need renv.ignore = TRUE in that code chunk, or you will get an annoying warning because renv can’t parse the code in that chunk.\nHowever, there are three other renv commands you might issue:\nrenv::status() just reports if anything is messed up. It won’t hurt anything.\nrenv::restore() looks at the renv.lock file and installs the packages it specifies. You will need to do this when I make a change to renv.lock, e.g., if I upgrade our version of ggplot2 or add a new package.\nrenv::snapshot() should only be issued if you know what you are doing. This changes the renv.lock file, which is something that, usually, only I do. Most common case for use would be if you need to add a new package to the project.\nWarning: The renv package commands are very sensitive to your current working directory. Ensure that you are in the main directory of your project — something like /Users/dkane/Desktop/projects/primer.tutorials — before you run any of them.\n\nCreate a branch to work from:\n\n\npr_init(branch = \"chapter-9\")\n\nMake sure the branch name is sensible. Again, this is a command that you only need to issue once, at least for our current workflow. You should always be “on” this branch, never on the default (master) branch. You can check this in the upper right corner of the git panel on R Studio.\nIn more professional settings, you will often work on several different branches at once. So, if you are comfortable, you should feel free to create more than one branch, use it, delete it and so on. Never work on the default branch, however. And, if you use multiple branches, be careful where you are and what you are doing.",
    "crumbs": [
      "Set Up for Working on PPBDS Projects"
    ]
  },
  {
    "objectID": "set-up.html#daily-work",
    "href": "set-up.html#daily-work",
    "title": "Set Up for Working on PPBDS Projects",
    "section": "Daily Work",
    "text": "Daily Work\n\nAlways use the “Source” view — not the “Visual” view — in Positron to edit. The Visual view will often reformat your code, without you asking, in unhelpful ways. You don’t want Positron to make changes in your code.\nPull regularly:\n\n\npr_merge_main()\n\nIssue this command all the time. This is how you make sure that your repo and your computer is updated with the latest changes that have been made in the book. The word “upstream” is associated with the repos at PPBDS. The word “origin” is associated with the fork at your Github account. But, in general, you don’t need to worry about this. Just pull every time you sit down. (Just clicking the pull button is not enough. That only pulls from your repo, to which no changes have been made. It does not pull from PPBDS/primer, et al.) You issue this command multiple times a day.\n\nMake changes in the file you are editing. Knit to make sure the changes work. Commit with a message. Push to the repo on your Github account. And so on.\n\nAt some point, you will be ready to push to the PPBDS organization. However, you can’t do this directly. Instead, you must submit a pull request (PR). Because you are part of a larger project, these commands are slightly different than what you have done before.\n\nIssue pull requests every few days, depending on how much work you have done and/or whether other people are waiting for something you have done.\n\n\npr_push()\n\nThis command bundles up a bunch of git commands (which you could do by hand) into one handy step. This command does everything needed to create a “pull request” — a request from you to me that I accept the changes you are proposing into the repo at PPBDS/primer — and then opens up the web page to show you. But you are not done! You must PRESS the green button on that web page, sometimes twice. Until then, the PR has not actually been created. pr_push() just does everything before that. The “pr” in pr_push() stands for pull request.\n\nI will leave aside for now issues associated with the back-and-forth discussions we might have around your pull request. I will probably just accept it. Your changes will go into the repos at PPBDS and then be distributed to everyone else when they run pr_merge_main().\nYou can now continue on. There is no need to wait for me to deal with your pull request. There is no need to fork/clone/download again. You don’t need to create a new branch, although many people do, with a branch name which describes what they are working on now. You just keep editing your files, knitting, and committing then pushing to your forked repo. When you feel you have completed another chunk of work, just run pr_push() again.\nRead the usethis setup help page at least once, perhaps after a week or two of working within this framework. It has lots of good stuff!",
    "crumbs": [
      "Set Up for Working on PPBDS Projects"
    ]
  },
  {
    "objectID": "set-up.html#common-problems",
    "href": "set-up.html#common-problems",
    "title": "Set Up for Working on PPBDS Projects",
    "section": "Common Problems",
    "text": "Common Problems\n\n\n\nIn the immediate aftermath of this creation process, the blue/green arrows (in the Git panel) for pulling/pushing may be grayed out. This is a sign that the connection between your computer and your forked repo has not “settled in.” (I am not sure of the cause or even if this is the right terminology.) I think that just issuing your first pr_merge_main() fixes it. If not, it always goes away. Until it does, however, you can’t pull/push to your repo. That doesn’t really matter, however, since the key commands you need are pr_merge_main() and pr_push(), both of which always work immediately.\nAfter running pr_merge_main(), you will often see a bunch of files, in your Source Control pane, marked with an M (for Modified), including files which you know you did not edit. These are the files that have been updated on the “truth” — on PPBDS/primer, for example — since your last pr_merge_main(). Since you pulled them directly from the PPBDS/primer repo, your forked repo sees all the changes other people have made and thinks that you made them*. This is easily fixed, however — just commit all the changes to your forked repo. (Strangely, this seems to not always happen. If you don’t see this effect, don’t worry.)\nAlways run pr_merge_main() before committing a file. Otherwise, you may create lots of merge conflicts. If this happens, save a copy of the file(s) you personally were editing off to the side. Then, nuke it from orbit, following the instructions above. Repeat the Project Set Up process. Then move in your file(s) by hand into the new repo, and commit/push them as normal.\nWhen you submit a pull request to merge your work with the PPBDS repo, it won’t always be smiles and sunshine — every once in a while, you’ll run into merge conflicts. When these arise, it is because two parties work on a file separately and submit conflicting changes. This makes it hard for GitHub to “merge” your version with the other version. When this happens, find multiple adjacent “&gt;”, “&lt;”, and “=” signs in your document — these will show you where the conflicts occur. For more background on merge conflicts, read this.\n\nIf you see the above-mentioned conflicts in your document, do not submit a pull request. This will mess things up. Instead, first, go through your document, and make sure all the weird conflict indicators (&lt;, &gt;, and =) are removed. Second, decide what goes in that space. It might be the stuff you wrote. It might be the other stuff. It might be some combination of the two which you decide on. Whatever happens, you are making an affirmative choice about what should appear in the file at that location. Once all the merge conflicts are fixed, run pr_push() again.\n\n\npr_push() can be tricky. First, note that, if I have not accepted a (p)ull (r)equest which you have submitted, then your PR is still open. You can see it on Github. In fact, you can see all the closed/completed pull requests as well. If, while one PR is still open, you submit another pr_push(), then this will just be added to your current PR. And that is OK! We don’t need it to be separate.\n\nBut even if there is not an open PR, pr_push() can be tricky. The key thing to remember is that you must press a green button on Github for a new PR to be created. Normally, this is easy. Running pr_push() automatically (or perhaps after you run pr_view()) puts you in a browser and brings you to the correct Github page. Press the button and – presto! – you have created a PR. But, sometimes, the web page is different. It actually sends you back to an old pull request. When this happens, you need to click on the “Pull Request” tab above. This will take you to a new page, with a green button labeled “Compare & Pull Request”. Press that button.\n\nIf you end up needing to install a new package — which should be rare — just install it with renv::install()and then type renv::status() to confirm than renv is aware of the change. Then, type renv::snapshot(). This will update the renv.lock file to include the new package. You just commit/push the new version of renv.lock, and that shares the information with everyone else on the project. Never commit/push a modified renv.lock unless you know why it has changed. But, for the most part, leave changes in renv.lock to me.\nBe careful of committing garbage files like “.DS_Store”, which is a file created sometimes. Only commit changes which you understand. In the vast majority of cases your PRs will only involve one or two files.\nIf using Windows, make sure you have RTools installed.\nSometimes, your renv library becomes messed up. Note that nuking from orbit may not fix this because renv installs your packages in some private common area which is not impacted when you reinstall the PPBDS package you are working with. In this case, running renv::rebuild() may help. If not, run renv::diagnostics(), find the path to the renv cache directory and delete everything there by hand. Then, run renv::repair()'.\nWhen using renv::restore(), you will occasionally have a problem in which one package, say aprob, fails to install, thereby aborting the entire run. This can be tricky. The most common reason for this problem is that the renv.lock file specifies an older version of aprob than the current version available on CRAN. Since CRAN does not (?) keep binaries for non-current versions, your computer is forced to compile from source. This often works OK, especially if you are using a Mac. But it often doesn’t work. The easiest approach is to update the renv.lock file to use the latest version of aprob.\n\nComplications can also arise if aprob was, in the last day or two, updated on CRAN. If you go to the page for aprob you can see that the source file is version 1.2.7, or whatever, while the binary versions are still 1.2.6. It takes a day or two for new binaries to be created on CRAN, especially for Windows. Again, situations like this are tricky enough that you should reach out to me for guidance.",
    "crumbs": [
      "Set Up for Working on PPBDS Projects"
    ]
  },
  {
    "objectID": "set-up.html#style-guide",
    "href": "set-up.html#style-guide",
    "title": "Set Up for Working on PPBDS Projects",
    "section": "Style Guide",
    "text": "Style Guide\n\nSection headings (other than Chapter titles) are in sentence case (with only the first word capitalized, unless it is something always capitalized) rather than title case (in which all words except small words like “the” and “of” are capitalized). Chapter titles are in title case. Headings do not end with a period.\nNever hard code stuff like “A tibble with 336,776 rows and 19 columns.” What happens when you update the data? Instead, calculate all numbers on the fly, with “r scales::comma(x)” whenever x is a number in the thousands or greater. Example: “A tibble with ‘r scales::comma(nrow(x))’ rows and ‘r ncol(x)’ columns.”\n“We” are writing this book.\nPackage names are in bold: ggplot2 is a package for doing graphics. In general, we reserve bolding for package names. Use italics for emphasis in other contexts.\nR code, anything you might type in the console, is always within backticks. Example: mtcars is a built-in data set.\nFunction names always include the parentheses: we write pivot_wider(), not pivot_wider.\nAdd lots of memes and videos and cartoons.\nMake ample use of comments, placed with the handy Command/Ctrl + Shift + / shortcut. These are notes for everyone else working on the chapter, and for future you.\nAll tables should be created with the gt package.\nAll images and gifs are loaded with knitr::include_graphics().\nInterim data sets should be called x or something sensible to the situation, like ch7 for a data set you are working with in Chapter 7. Do not use names like data and df, both of which are R commands.\nStudents are sometimes tentative. Don’t be! Edit aggressively. If you don’t like what is there, delete it. (If I disagree with your decision, I can always get the text back from Github.) Move things around. Make the chapter yours, while keeping to the style of the other chapters. Note that 90% of the prose here was not written by me. Cut anything you don’t like.\nIf you make an mp4, you can convert it to .gif using https://convertio.co/mp4-gif.\nEverything is Bayesian. The confidence interval for a regression means that there is a 95% chance that the true value lies within that interval. Use Rubin Causal Model and potential outcomes to define precisely what “true value” you are talking about. And so on.",
    "crumbs": [
      "Set Up for Working on PPBDS Projects"
    ]
  }
]